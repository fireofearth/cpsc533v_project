{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a622b81",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Baseline agent algorithm with experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f7b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "        \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c62b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5ad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.common.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.05,\n",
    "    n_episodes=10_000,\n",
    "    batch_size=16,\n",
    "    update_target_interval=32,\n",
    "    report_interval=64,\n",
    "    clip_grad_norm=2.,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    common=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "        n_rnn_layers=2,\n",
    "        apply_bn=False,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        for idx in range(config.adversary.n_agents):\n",
    "            self.__message_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.message_size*(config.adversary.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"adversary_{idx}\"]  = torch.zeros(\n",
    "                (config.adversary.n_rnn_layers, 1, self.config.adversary.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        for idx in range(config.agent.n_agents):\n",
    "            self.__message_d[f\"agent_{idx}\"] = torch.zeros(\n",
    "                self.config.agent.message_size*(config.agent.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"agent_{idx}\"]  = torch.zeros(\n",
    "                (config.agent.n_rnn_layers, 1, self.config.agent.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        return self.__message_d[agent_name]\n",
    "\n",
    "    def get_hidden(self, agent_name):\n",
    "        return self.__hidden_d[agent_name]\n",
    "\n",
    "    def update_message(self, agent_name, message):\n",
    "        \"\"\"Update message cache.\n",
    "        \n",
    "        Messages of multiple agents are concatenated together.\n",
    "        For example, if agent 2 receives messages from agents 0, 1, and 3 then\n",
    "        the message is a vector of the form: [ 0's message, 1's message, 3's message ]\n",
    "        \"\"\"\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        message_size = self.config[agent_type].message_size\n",
    "        for jdx in range(config[agent_type].n_agents):\n",
    "            if jdx < agent_idx:\n",
    "                start_idx = message_size * (agent_idx - 1)\n",
    "            elif jdx == agent_idx:\n",
    "                # do not update message to oneself\n",
    "                continue\n",
    "            else:\n",
    "                # agent_idx < jdx\n",
    "                start_idx = message_size * agent_idx\n",
    "            end_idx   = start_idx + self.config[agent_type].message_size\n",
    "            # print(jdx, agent_idx, self.__message_d[f\"{agent_type}_{jdx}\"].shape, start_idx, end_idx)\n",
    "            messages = self.__message_d[f\"{agent_type}_{jdx}\"]\n",
    "            self.__message_d[f\"{agent_type}_{jdx}\"] = \\\n",
    "                    torch.hstack((messages[:start_idx], message, messages[end_idx:]))\n",
    "\n",
    "    def update_hidden(self, agent_name, hidden):\n",
    "        self.__hidden_d[agent_name] = hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.device      = config.device\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.n_agents    = config[agent_type].n_agents\n",
    "        self.n_actions   = config[agent_type].n_actions\n",
    "        self.apply_bn    = config[agent_type].apply_bn\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.n_output    = self.n_actions\n",
    "        \n",
    "        self.agent_lookup    = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.observation_mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"linear\", torch.nn.Linear(self.observation_size, self.hidden_size)),\n",
    "            (\"relu\", torch.nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.output_mlp = torch.nn.Sequential()\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.output_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.n_output))\n",
    "    \n",
    "    def forward(self, agent_idx, observation):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        agent_idx : int\n",
    "            Index of agent\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        \"\"\"\n",
    "        agent_idx   = torch.tensor(agent_idx, dtype=torch.int, device=self.device)\n",
    "        observation = torch.tensor(observation, dtype=torch.float, device=self.device)\n",
    "        z_a = self.agent_lookup(agent_idx)\n",
    "        z_o = self.observation_mlp(observation)\n",
    "        # z has shape (N=1, L=1, H_in=128)\n",
    "        z = z_a + z_o\n",
    "        Q = self.output_mlp(z)\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f86814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def choose_action(config, agent_type, Q, is_val=False):\n",
    "    if not is_val and random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "def run_episode(config, adversary_net, agent_net, should_render=False, is_val=False):\n",
    "    \"\"\"Run one episodes.\n",
    "    \n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    AttrDict\n",
    "        Contains episode metrics:\n",
    "        - steps : number of steps. All agents take an action at each step.\n",
    "        - reward : episodic rewards indexed by ('adversary', 'agent').\n",
    "        - step_records : list of quantities produced indiced by step, ('adversary', 'agent'), agent index.\n",
    "          Each step record has:\n",
    "            + observation\n",
    "            + Q\n",
    "            + reward\n",
    "            + done\n",
    "        - loss : contains episodic losses indexed by ('adversary', 'agent'). To be updated by train_agents()\n",
    "    \"\"\"\n",
    "    episode = AttrDict(\n",
    "        steps=0,\n",
    "        reward=AttrDict(adversary=0, agent=0),\n",
    "        step_records=[],\n",
    "        loss=AttrDict(adversary=0, agent=0)\n",
    "    )\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    \n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "            \n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "\n",
    "        if agent_type == \"adversary\":\n",
    "            Q_curr = adversary_net(agent_idx, obs_curr)\n",
    "        else:\n",
    "            # agent type is synonymous with good agent\n",
    "            Q_curr = agent_net(agent_idx, obs_curr)\n",
    "\n",
    "        action = choose_action(config, agent_type, Q_curr, is_val=is_val)\n",
    "        env.step(action)\n",
    "        step_record[agent_type][agent_idx] = AttrDict(\n",
    "            # inputs to network\n",
    "            observation=obs_curr,\n",
    "            # outputs of network / inputs to environment\n",
    "            action=action,\n",
    "            Q=Q_curr,\n",
    "            # output of environment\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "        )\n",
    "        episode.reward[agent_type] += reward\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def train_agents(config, batch, adversary_net, agent_net,\n",
    "                 adversary_target_net, agent_target_net,\n",
    "                 adversary_optimizer, agent_optimizer):\n",
    "    \"\"\"Compute loss of episode and update agent weights.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float, device=device)\n",
    "    adversary_loss = torch.tensor(0., device=device)\n",
    "    agent_loss = torch.tensor(0., device=device)\n",
    "    for episode in batch:\n",
    "        for step_idx in range(episode.steps):\n",
    "            for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "                curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "                if curr_record.done:\n",
    "                    # agent is done at this step\n",
    "                    continue\n",
    "                next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "                r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "                y = None\n",
    "                if next_record.done:\n",
    "                    # agent terminates at next step\n",
    "                    y = r\n",
    "                else:\n",
    "                    next_o = next_record.observation\n",
    "                    with torch.no_grad():\n",
    "                        target_Q = adversary_target_net(agent_idx, next_o)\n",
    "                        max_target_Q = torch.max(target_Q)\n",
    "                        y = r + discount*max_target_Q\n",
    "                u = curr_record.action\n",
    "                Q_u = curr_record.Q[u]\n",
    "                adversary_loss += torch.pow(y - Q_u, 2.)\n",
    "\n",
    "            for agent_idx in episode.step_records[step_idx].agent.keys():\n",
    "                curr_record = episode.step_records[step_idx].agent[agent_idx]\n",
    "                if curr_record.done:\n",
    "                    # agent is done at this step\n",
    "                    continue\n",
    "                next_record = episode.step_records[step_idx + 1].agent[agent_idx]\n",
    "                r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "                y = None\n",
    "                if next_record.done:\n",
    "                    # agent terminates at next step\n",
    "                    y = r\n",
    "                else:\n",
    "                    next_o = next_record.observation\n",
    "                    with torch.no_grad():\n",
    "                        target_Q = agent_target_net(agent_idx, next_o)\n",
    "                        max_target_Q = torch.max(target_Q)\n",
    "                        y = r + discount*max_target_Q\n",
    "                u = curr_record.action\n",
    "                Q_u = curr_record.Q[u]\n",
    "                agent_loss += torch.pow(y - Q_u, 2.)\n",
    "    \n",
    "    n_adversary = torch.tensor(config.adversary.n_agents, dtype=torch.float, device=device)\n",
    "    n_agent = torch.tensor(config.agent.n_agents, dtype=torch.float, device=device)\n",
    "    bs = torch.tensor(config.batch_size, dtype=torch.float, device=device)\n",
    "    adversary_loss = adversary_loss / (bs + n_adversary)\n",
    "    agent_loss     = agent_loss / (bs + n_agent)\n",
    "    adversary_optimizer.zero_grad()\n",
    "    agent_optimizer.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(adversary_net.parameters(), config.clip_grad_norm)\n",
    "    torch.nn.utils.clip_grad_norm_(agent_net.parameters(), config.clip_grad_norm)\n",
    "    adversary_loss.backward()\n",
    "    agent_loss.backward()\n",
    "    adversary_optimizer.step()\n",
    "    agent_optimizer.step()\n",
    "    episode.loss = AttrDict(adversary=adversary_loss.item(), agent=agent_loss.item())\n",
    "    \n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    print(\"Training the agents...\")\n",
    "    os.makedirs(\"models/batched-baseline\", exist_ok=True)\n",
    "    t0 = time.time()\n",
    "    device = config.device\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    agent_net = SimpleTagNet(config, \"agent\").to(device)\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    agent_target_net = SimpleTagNet(config, \"agent\").to(device)\n",
    "    adversary_target_net.eval()\n",
    "    agent_target_net.eval()\n",
    "    print(\"Created the agent nets.\")\n",
    "    adversary_optimizer = torch.optim.RMSprop(adversary_net.parameters())\n",
    "    agent_optimizer = torch.optim.RMSprop(agent_net.parameters())\n",
    "    logger = AttrDict(\n",
    "        episodic_losses=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_rewards=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_message_maxsum=AttrDict(adversary=[], agent=[])\n",
    "    )\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "        agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "    print(\"Initial update of target nets\")\n",
    "    update_targets()\n",
    "    \n",
    "    batch = []\n",
    "    print(\"Beginning the episodes...\")\n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        # Run an episode\n",
    "        episode = run_episode(config, adversary_net, agent_net,\n",
    "                              should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n",
    "        batch.append(episode)\n",
    "        \n",
    "        # Train on the episode\n",
    "        if episode_idx % config.batch_size == 0 and episode_idx > 0:\n",
    "            train_agents(config, batch, adversary_net, agent_net,\n",
    "                         adversary_target_net, agent_target_net,\n",
    "                         adversary_optimizer, agent_optimizer)\n",
    "            batch = []\n",
    "        \n",
    "        # Logging the reward and los\n",
    "        logger.episodic_losses.adversary.append(episode.loss.adversary)\n",
    "        logger.episodic_losses.agent.append(episode.loss.agent)\n",
    "        logger.episodic_rewards.adversary.append(episode.reward.adversary)\n",
    "        logger.episodic_rewards.agent.append(episode.reward.agent)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            # Update double network\n",
    "            update_targets()\n",
    "        \n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            # Logging\n",
    "            t1 = time.time()\n",
    "            tdelta = TimeDelta(round(t1 - t0, 0))\n",
    "            print(f\"on episode {episode_idx} (time taken so far: {tdelta})\")\n",
    "            mean_loss_adversary = statistics.fmean(logger.episodic_losses.adversary[-config.report_interval:])\n",
    "            mean_loss_agent = statistics.fmean(logger.episodic_losses.agent[-config.report_interval:])\n",
    "            mean_reward_adversary = statistics.fmean(logger.episodic_rewards.adversary[-config.report_interval:])\n",
    "            mean_reward_agent = statistics.fmean(logger.episodic_rewards.agent[-config.report_interval:])\n",
    "            print(f\"     mean loss: adversary {mean_loss_adversary}, agent {mean_loss_agent}\")\n",
    "            print(f\"     mean reward: adversary {mean_reward_adversary}, agent {mean_reward_agent}\")\n",
    "            \n",
    "            torch.save(adversary_net.state_dict(), f\"models/batched-baseline/adversary-net-{episode_idx}.pth\")\n",
    "            torch.save(agent_net.state_dict(), f\"models/batched-baseline/agent-net-{episode_idx}.pth\")\n",
    "    \n",
    "    return adversary_net, agent_net, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0210fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agents...\n",
      "Created the agent nets.\n",
      "Initial update of target nets\n",
      "Beginning the episodes...\n",
      "on episode 64 (time taken so far: 0-00:01:35.0)\n",
      "     mean loss: adversary 45578.02366542816, agent 4035.12215423584\n",
      "     mean reward: adversary 54.375, agent -3847.972675087753\n",
      "on episode 128 (time taken so far: 0-00:03:7.0)\n",
      "     mean loss: adversary 42.7421612739563, agent 112.09345436096191\n",
      "     mean reward: adversary 62.8125, agent -699.7811212829506\n",
      "on episode 192 (time taken so far: 0-00:04:40.0)\n",
      "     mean loss: adversary 46.712759017944336, agent 153.68387985229492\n",
      "     mean reward: adversary 68.4375, agent -853.5938138342701\n",
      "on episode 256 (time taken so far: 0-00:06:16.0)\n",
      "     mean loss: adversary 1861.3388833999634, agent 172.19524765014648\n",
      "     mean reward: adversary 43.125, agent -956.9395410755142\n",
      "on episode 320 (time taken so far: 0-00:07:48.0)\n",
      "     mean loss: adversary 233.45912170410156, agent 116.75400161743164\n",
      "     mean reward: adversary 39.84375, agent -736.5218913048581\n",
      "on episode 384 (time taken so far: 0-00:09:21.0)\n",
      "     mean loss: adversary 251.4985694885254, agent 140.2194118499756\n",
      "     mean reward: adversary 60.46875, agent -821.62742113357\n",
      "on episode 448 (time taken so far: 0-00:10:54.0)\n",
      "     mean loss: adversary 317.0207290649414, agent 157.615966796875\n",
      "     mean reward: adversary 52.96875, agent -864.6205606462648\n",
      "on episode 512 (time taken so far: 0-00:12:26.0)\n",
      "     mean loss: adversary 586.2492370605469, agent 84.70089817047119\n",
      "     mean reward: adversary 48.75, agent -625.0816247804686\n",
      "on episode 576 (time taken so far: 0-00:13:58.0)\n",
      "     mean loss: adversary 551.30126953125, agent 92.3322582244873\n",
      "     mean reward: adversary 59.53125, agent -653.8636167053814\n",
      "on episode 640 (time taken so far: 0-00:15:29.0)\n",
      "     mean loss: adversary 592.9075164794922, agent 98.88846015930176\n",
      "     mean reward: adversary 47.8125, agent -633.2987713581437\n",
      "on episode 704 (time taken so far: 0-00:17:1.0)\n",
      "     mean loss: adversary 619.4233245849609, agent 85.26277732849121\n",
      "     mean reward: adversary 58.125, agent -612.5676432291499\n",
      "on episode 768 (time taken so far: 0-00:18:34.0)\n",
      "     mean loss: adversary 677.9061279296875, agent 88.3301362991333\n",
      "     mean reward: adversary 64.6875, agent -549.2358489170916\n",
      "on episode 832 (time taken so far: 0-00:20:6.0)\n",
      "     mean loss: adversary 758.5879058837891, agent 59.57845401763916\n",
      "     mean reward: adversary 69.84375, agent -467.3053084899212\n",
      "on episode 896 (time taken so far: 0-00:21:38.0)\n",
      "     mean loss: adversary 777.0863952636719, agent 71.40925216674805\n",
      "     mean reward: adversary 66.09375, agent -575.1536386691242\n",
      "on episode 960 (time taken so far: 0-00:23:11.0)\n",
      "     mean loss: adversary 998.1934509277344, agent 69.85775375366211\n",
      "     mean reward: adversary 85.78125, agent -538.2595911803297\n",
      "on episode 1024 (time taken so far: 0-00:24:42.0)\n",
      "     mean loss: adversary 1019.0597839355469, agent 63.30888557434082\n",
      "     mean reward: adversary 77.34375, agent -483.7394154987488\n",
      "on episode 1088 (time taken so far: 0-00:26:16.0)\n",
      "     mean loss: adversary 1061.8368530273438, agent 71.02103614807129\n",
      "     mean reward: adversary 69.375, agent -511.51315105036565\n",
      "on episode 1152 (time taken so far: 0-00:27:49.0)\n",
      "     mean loss: adversary 1221.9815216064453, agent 81.0821762084961\n",
      "     mean reward: adversary 60.0, agent -551.3542868839514\n",
      "on episode 1216 (time taken so far: 0-00:29:22.0)\n",
      "     mean loss: adversary 1416.6920776367188, agent 68.54995918273926\n",
      "     mean reward: adversary 68.4375, agent -549.6355179563986\n",
      "on episode 1280 (time taken so far: 0-00:30:54.0)\n",
      "     mean loss: adversary 1567.8717651367188, agent 54.139615058898926\n",
      "     mean reward: adversary 79.6875, agent -415.88164523988564\n",
      "on episode 1344 (time taken so far: 0-00:32:25.0)\n",
      "     mean loss: adversary 1985.0546875, agent 69.44755172729492\n",
      "     mean reward: adversary 76.875, agent -513.2620982105999\n",
      "on episode 1408 (time taken so far: 0-00:33:57.0)\n",
      "     mean loss: adversary 2265.6206665039062, agent 57.11547374725342\n",
      "     mean reward: adversary 65.625, agent -471.0991357945413\n",
      "on episode 1472 (time taken so far: 0-00:35:28.0)\n",
      "     mean loss: adversary 3309.7664184570312, agent 61.01097011566162\n",
      "     mean reward: adversary 72.65625, agent -456.63345734348974\n",
      "on episode 1536 (time taken so far: 0-00:36:59.0)\n",
      "     mean loss: adversary 4814.097412109375, agent 54.718356132507324\n",
      "     mean reward: adversary 50.15625, agent -476.90881221007993\n",
      "on episode 1600 (time taken so far: 0-00:38:31.0)\n",
      "     mean loss: adversary 5925.5897216796875, agent 63.51782512664795\n",
      "     mean reward: adversary 67.5, agent -514.5779264684446\n",
      "on episode 1664 (time taken so far: 0-00:40:2.0)\n",
      "     mean loss: adversary 7514.180908203125, agent 60.63705539703369\n",
      "     mean reward: adversary 73.125, agent -442.48301214562173\n",
      "on episode 1728 (time taken so far: 0-00:41:34.0)\n",
      "     mean loss: adversary 10611.61767578125, agent 48.83799457550049\n",
      "     mean reward: adversary 74.0625, agent -380.90505194144424\n",
      "on episode 1792 (time taken so far: 0-00:43:5.0)\n",
      "     mean loss: adversary 14805.458984375, agent 56.92672824859619\n",
      "     mean reward: adversary 88.59375, agent -433.323046767027\n",
      "on episode 1856 (time taken so far: 0-00:44:36.0)\n",
      "     mean loss: adversary 19421.0224609375, agent 70.37906455993652\n",
      "     mean reward: adversary 75.9375, agent -479.7645501841495\n",
      "on episode 1920 (time taken so far: 0-00:46:8.0)\n",
      "     mean loss: adversary 28030.57763671875, agent 53.034223556518555\n",
      "     mean reward: adversary 86.25, agent -429.29403369075357\n",
      "on episode 1984 (time taken so far: 0-00:47:39.0)\n",
      "     mean loss: adversary 57710.4306640625, agent 60.85236930847168\n",
      "     mean reward: adversary 98.90625, agent -475.9867392538769\n",
      "on episode 2048 (time taken so far: 0-00:49:9.0)\n",
      "     mean loss: adversary 307796.208984375, agent 62.92949104309082\n",
      "     mean reward: adversary 56.25, agent -517.9476955074695\n",
      "on episode 2112 (time taken so far: 0-00:50:41.0)\n",
      "     mean loss: adversary 28703455.25, agent 82.08685302734375\n",
      "     mean reward: adversary 30.46875, agent -649.5706494502367\n",
      "on episode 2176 (time taken so far: 0-00:52:13.0)\n",
      "     mean loss: adversary 14959025.0625, agent 69.93908786773682\n",
      "     mean reward: adversary 25.78125, agent -557.5848617103749\n",
      "on episode 2240 (time taken so far: 0-00:53:44.0)\n",
      "     mean loss: adversary 14603858.125, agent 59.84800910949707\n",
      "     mean reward: adversary 33.28125, agent -499.240466982381\n",
      "on episode 2304 (time taken so far: 0-00:55:16.0)\n",
      "     mean loss: adversary 12000999.4375, agent 63.201881408691406\n",
      "     mean reward: adversary 21.09375, agent -523.2212148602736\n",
      "on episode 2368 (time taken so far: 0-00:56:47.0)\n",
      "     mean loss: adversary 16598765.5, agent 57.85684680938721\n",
      "     mean reward: adversary 39.375, agent -510.9183412832954\n",
      "on episode 2432 (time taken so far: 0-00:58:18.0)\n",
      "     mean loss: adversary 19170135.0, agent 96.23915767669678\n",
      "     mean reward: adversary 52.03125, agent -547.2837746857456\n",
      "on episode 2496 (time taken so far: 0-00:59:50.0)\n",
      "     mean loss: adversary 14543451.625, agent 46.64935302734375\n",
      "     mean reward: adversary 31.875, agent -391.89286154647715\n",
      "on episode 2560 (time taken so far: 0-01:01:21.0)\n",
      "     mean loss: adversary 20702189.0, agent 55.40335655212402\n",
      "     mean reward: adversary 45.46875, agent -412.82974366005266\n",
      "on episode 2624 (time taken so far: 0-01:02:53.0)\n",
      "     mean loss: adversary 10311670.9375, agent 53.49057197570801\n",
      "     mean reward: adversary 31.875, agent -424.84400431711305\n",
      "on episode 2688 (time taken so far: 0-01:04:24.0)\n",
      "     mean loss: adversary 12039494.625, agent 65.50451564788818\n",
      "     mean reward: adversary 53.4375, agent -483.48160953614195\n",
      "on episode 2752 (time taken so far: 0-01:05:56.0)\n",
      "     mean loss: adversary 15756757.0, agent 59.683133602142334\n",
      "     mean reward: adversary 47.34375, agent -497.79141068340306\n",
      "on episode 2816 (time taken so far: 0-01:07:27.0)\n",
      "     mean loss: adversary 20893963.0, agent 47.047176361083984\n",
      "     mean reward: adversary 29.53125, agent -405.5661853022901\n",
      "on episode 2880 (time taken so far: 0-01:08:58.0)\n",
      "     mean loss: adversary 17379586.5, agent 60.58812618255615\n",
      "     mean reward: adversary 27.65625, agent -436.4752160790086\n",
      "on episode 2944 (time taken so far: 0-01:10:30.0)\n",
      "     mean loss: adversary 9526640.75, agent 71.38541412353516\n",
      "     mean reward: adversary 32.8125, agent -516.3806410868896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 3008 (time taken so far: 0-01:12:1.0)\n",
      "     mean loss: adversary 8893532.3125, agent 62.51217269897461\n",
      "     mean reward: adversary 30.0, agent -486.7960905062996\n",
      "on episode 3072 (time taken so far: 0-01:13:32.0)\n",
      "     mean loss: adversary 11178274.25, agent 54.85436534881592\n",
      "     mean reward: adversary 33.28125, agent -442.6507336273601\n",
      "on episode 3136 (time taken so far: 0-01:15:4.0)\n",
      "     mean loss: adversary 9074483.875, agent 47.50102615356445\n",
      "     mean reward: adversary 36.09375, agent -380.72672289607647\n",
      "on episode 3200 (time taken so far: 0-01:16:36.0)\n",
      "     mean loss: adversary 13330616.5, agent 45.38941192626953\n",
      "     mean reward: adversary 25.3125, agent -395.7210927350198\n",
      "on episode 3264 (time taken so far: 0-01:18:7.0)\n",
      "     mean loss: adversary 6449438.25, agent 60.31577491760254\n",
      "     mean reward: adversary 44.0625, agent -439.8082307987484\n",
      "on episode 3328 (time taken so far: 0-01:19:38.0)\n",
      "     mean loss: adversary 7270848.625, agent 44.01920032501221\n",
      "     mean reward: adversary 35.625, agent -386.2771133514282\n",
      "on episode 3392 (time taken so far: 0-01:21:9.0)\n",
      "     mean loss: adversary 9897338.375, agent 57.266221046447754\n",
      "     mean reward: adversary 32.8125, agent -396.77103649773096\n",
      "on episode 3456 (time taken so far: 0-01:22:41.0)\n",
      "     mean loss: adversary 14070823.75, agent 55.39663314819336\n",
      "     mean reward: adversary 43.59375, agent -438.8632062527272\n",
      "on episode 3520 (time taken so far: 0-01:24:12.0)\n",
      "     mean loss: adversary 13989119.25, agent 75.92537212371826\n",
      "     mean reward: adversary 21.09375, agent -458.0638953484226\n",
      "on episode 3584 (time taken so far: 0-01:25:44.0)\n",
      "     mean loss: adversary 13515883.75, agent 53.1448450088501\n",
      "     mean reward: adversary 32.34375, agent -428.47567337496974\n",
      "on episode 3648 (time taken so far: 0-01:27:15.0)\n",
      "     mean loss: adversary 19006192.75, agent 56.08368396759033\n",
      "     mean reward: adversary 39.375, agent -398.96910141749356\n",
      "on episode 3712 (time taken so far: 0-01:28:47.0)\n",
      "     mean loss: adversary 21686173.0, agent 52.010756492614746\n",
      "     mean reward: adversary 28.59375, agent -358.20642325378924\n",
      "on episode 3776 (time taken so far: 0-01:30:18.0)\n",
      "     mean loss: adversary 23939955.0, agent 49.052302837371826\n",
      "     mean reward: adversary 28.59375, agent -388.17361635906576\n",
      "on episode 3840 (time taken so far: 0-01:31:50.0)\n",
      "     mean loss: adversary 25404852.5, agent 58.48274898529053\n",
      "     mean reward: adversary 29.53125, agent -440.2839534122186\n",
      "on episode 3904 (time taken so far: 0-01:33:21.0)\n",
      "     mean loss: adversary 27410159.5, agent 44.65686321258545\n",
      "     mean reward: adversary 23.90625, agent -419.7689366645995\n",
      "on episode 3968 (time taken so far: 0-01:34:53.0)\n",
      "     mean loss: adversary 29338470.0, agent 56.50837802886963\n",
      "     mean reward: adversary 28.125, agent -512.8862383689208\n",
      "on episode 4032 (time taken so far: 0-01:36:24.0)\n",
      "     mean loss: adversary 34096466.0, agent 50.72568702697754\n",
      "     mean reward: adversary 27.65625, agent -458.02190757483686\n",
      "on episode 4096 (time taken so far: 0-01:37:56.0)\n",
      "     mean loss: adversary 34341613.5, agent 45.43708610534668\n",
      "     mean reward: adversary 29.0625, agent -391.8993407540851\n",
      "on episode 4160 (time taken so far: 0-01:39:28.0)\n",
      "     mean loss: adversary 38055737.0, agent 51.81768751144409\n",
      "     mean reward: adversary 30.9375, agent -392.36985669261924\n",
      "on episode 4224 (time taken so far: 0-01:40:59.0)\n",
      "     mean loss: adversary 42710235.0, agent 50.31758260726929\n",
      "     mean reward: adversary 49.6875, agent -374.31051340787116\n",
      "on episode 4288 (time taken so far: 0-01:42:30.0)\n",
      "     mean loss: adversary 44111651.0, agent 48.17812538146973\n",
      "     mean reward: adversary 43.59375, agent -392.6909884841273\n",
      "on episode 4352 (time taken so far: 0-01:44:1.0)\n",
      "     mean loss: adversary 45091946.0, agent 64.97741317749023\n",
      "     mean reward: adversary 34.21875, agent -444.6507793605186\n",
      "on episode 4416 (time taken so far: 0-01:45:33.0)\n",
      "     mean loss: adversary 44452645.0, agent 48.382540702819824\n",
      "     mean reward: adversary 36.09375, agent -437.7446269929407\n",
      "on episode 4480 (time taken so far: 0-01:47:5.0)\n",
      "     mean loss: adversary 47537756.0, agent 42.484314918518066\n",
      "     mean reward: adversary 29.53125, agent -346.8422581130675\n",
      "on episode 4544 (time taken so far: 0-01:48:36.0)\n",
      "     mean loss: adversary 48777931.0, agent 35.72216987609863\n",
      "     mean reward: adversary 37.03125, agent -336.6948130322638\n",
      "on episode 4608 (time taken so far: 0-01:50:7.0)\n",
      "     mean loss: adversary 47418385.0, agent 55.11093711853027\n",
      "     mean reward: adversary 28.125, agent -421.7002809690248\n",
      "on episode 4672 (time taken so far: 0-01:51:39.0)\n",
      "     mean loss: adversary 50509724.0, agent 35.13345694541931\n",
      "     mean reward: adversary 26.25, agent -348.61168367889843\n",
      "on episode 4736 (time taken so far: 0-01:53:11.0)\n",
      "     mean loss: adversary 51245560.0, agent 40.12162399291992\n",
      "     mean reward: adversary 33.28125, agent -358.6324693629033\n",
      "on episode 4800 (time taken so far: 0-01:54:42.0)\n",
      "     mean loss: adversary 52414790.0, agent 62.846595764160156\n",
      "     mean reward: adversary 30.0, agent -401.6767118332971\n",
      "on episode 4864 (time taken so far: 0-01:56:13.0)\n",
      "     mean loss: adversary 54046378.0, agent 39.458861351013184\n",
      "     mean reward: adversary 32.8125, agent -358.150113078596\n",
      "on episode 4928 (time taken so far: 0-01:57:44.0)\n",
      "     mean loss: adversary 51662131.0, agent 35.22990036010742\n",
      "     mean reward: adversary 24.375, agent -361.4975905886724\n",
      "on episode 4992 (time taken so far: 0-01:59:16.0)\n",
      "     mean loss: adversary 48182894.0, agent 52.10191345214844\n",
      "     mean reward: adversary 27.65625, agent -426.14115101634366\n",
      "on episode 5056 (time taken so far: 0-02:00:47.0)\n",
      "     mean loss: adversary 55717176.0, agent 46.19286823272705\n",
      "     mean reward: adversary 26.25, agent -415.353357688342\n",
      "on episode 5120 (time taken so far: 0-02:02:19.0)\n",
      "     mean loss: adversary 54004535.0, agent 36.87662601470947\n",
      "     mean reward: adversary 22.96875, agent -348.9573156337365\n",
      "on episode 5184 (time taken so far: 0-02:03:50.0)\n",
      "     mean loss: adversary 59157835.0, agent 63.19987392425537\n",
      "     mean reward: adversary 32.8125, agent -487.0607796694912\n",
      "on episode 5248 (time taken so far: 0-02:05:22.0)\n",
      "     mean loss: adversary 56390847.0, agent 58.194908142089844\n",
      "     mean reward: adversary 31.40625, agent -437.3873200463699\n",
      "on episode 5312 (time taken so far: 0-02:06:53.0)\n",
      "     mean loss: adversary 54777491.0, agent 57.09314775466919\n",
      "     mean reward: adversary 26.71875, agent -462.31816714843217\n",
      "on episode 5376 (time taken so far: 0-02:08:25.0)\n",
      "     mean loss: adversary 56128024.0, agent 57.7236909866333\n",
      "     mean reward: adversary 36.09375, agent -413.7409548210124\n",
      "on episode 5440 (time taken so far: 0-02:09:56.0)\n",
      "     mean loss: adversary 49065272.0, agent 57.94945812225342\n",
      "     mean reward: adversary 35.15625, agent -432.2160363720569\n",
      "on episode 5504 (time taken so far: 0-02:11:28.0)\n",
      "     mean loss: adversary 53429396.0, agent 45.49365043640137\n",
      "     mean reward: adversary 30.46875, agent -385.6475022143065\n",
      "on episode 5568 (time taken so far: 0-02:12:59.0)\n",
      "     mean loss: adversary 52521345.0, agent 60.483848571777344\n",
      "     mean reward: adversary 23.90625, agent -451.1423065476612\n",
      "on episode 5632 (time taken so far: 0-02:14:30.0)\n",
      "     mean loss: adversary 45288224.0, agent 60.51304817199707\n",
      "     mean reward: adversary 32.8125, agent -454.37631337517564\n",
      "on episode 5696 (time taken so far: 0-02:16:2.0)\n",
      "     mean loss: adversary 50034410.0, agent 51.325151443481445\n",
      "     mean reward: adversary 24.84375, agent -441.0946775857069\n",
      "on episode 5760 (time taken so far: 0-02:17:33.0)\n",
      "     mean loss: adversary 46890997.0, agent 44.34557104110718\n",
      "     mean reward: adversary 25.3125, agent -362.93449409785893\n",
      "on episode 5824 (time taken so far: 0-02:19:5.0)\n",
      "     mean loss: adversary 44822098.0, agent 48.2203311920166\n",
      "     mean reward: adversary 21.09375, agent -394.23646707581065\n",
      "on episode 5888 (time taken so far: 0-02:20:36.0)\n",
      "     mean loss: adversary 40939779.0, agent 68.75866317749023\n",
      "     mean reward: adversary 27.1875, agent -477.4072066385455\n",
      "on episode 5952 (time taken so far: 0-02:22:7.0)\n",
      "     mean loss: adversary 40433369.0, agent 64.43881750106812\n",
      "     mean reward: adversary 45.9375, agent -442.40091341735484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 6016 (time taken so far: 0-02:23:39.0)\n",
      "     mean loss: adversary 42967188.0, agent 46.295684814453125\n",
      "     mean reward: adversary 40.3125, agent -369.71354966260157\n",
      "on episode 6080 (time taken so far: 0-02:25:10.0)\n",
      "     mean loss: adversary 39647933.5, agent 59.57460308074951\n",
      "     mean reward: adversary 34.6875, agent -428.17046059848025\n",
      "on episode 6144 (time taken so far: 0-02:26:41.0)\n",
      "     mean loss: adversary 38534527.0, agent 34.084110260009766\n",
      "     mean reward: adversary 22.5, agent -345.8630825432408\n",
      "on episode 6208 (time taken so far: 0-02:28:13.0)\n",
      "     mean loss: adversary 38224249.0, agent 55.01156568527222\n",
      "     mean reward: adversary 22.03125, agent -450.8226034858394\n",
      "on episode 6272 (time taken so far: 0-02:29:44.0)\n",
      "     mean loss: adversary 38332844.0, agent 49.749505043029785\n",
      "     mean reward: adversary 29.53125, agent -395.26975944709704\n",
      "on episode 6336 (time taken so far: 0-02:31:15.0)\n",
      "     mean loss: adversary 33850198.5, agent 58.963051319122314\n",
      "     mean reward: adversary 26.71875, agent -447.30622693673195\n",
      "on episode 6400 (time taken so far: 0-02:32:47.0)\n",
      "     mean loss: adversary 34600824.0, agent 40.213782787323\n",
      "     mean reward: adversary 29.53125, agent -360.5507748958237\n",
      "on episode 6464 (time taken so far: 0-02:34:18.0)\n",
      "     mean loss: adversary 32973395.5, agent 44.716867446899414\n",
      "     mean reward: adversary 25.78125, agent -408.32154406192575\n",
      "on episode 6528 (time taken so far: 0-02:35:50.0)\n",
      "     mean loss: adversary 32179936.0, agent 55.29509735107422\n",
      "     mean reward: adversary 26.25, agent -441.1406823828714\n",
      "on episode 6592 (time taken so far: 0-02:37:20.0)\n",
      "     mean loss: adversary 30648633.0, agent 58.713218688964844\n",
      "     mean reward: adversary 30.9375, agent -433.2129756563081\n",
      "on episode 6656 (time taken so far: 0-02:38:52.0)\n",
      "     mean loss: adversary 27509333.5, agent 58.887887954711914\n",
      "     mean reward: adversary 23.4375, agent -454.37394953659043\n",
      "on episode 6720 (time taken so far: 0-02:40:24.0)\n",
      "     mean loss: adversary 29184305.0, agent 53.95780849456787\n",
      "     mean reward: adversary 30.0, agent -415.8101402160925\n",
      "on episode 6784 (time taken so far: 0-02:41:56.0)\n",
      "     mean loss: adversary 27842907.0, agent 64.86970806121826\n",
      "     mean reward: adversary 45.0, agent -473.36673715462615\n",
      "on episode 6848 (time taken so far: 0-02:43:27.0)\n",
      "     mean loss: adversary 25014875.0, agent 48.50776386260986\n",
      "     mean reward: adversary 25.3125, agent -434.03528041931673\n",
      "on episode 6912 (time taken so far: 0-02:44:58.0)\n",
      "     mean loss: adversary 24172595.5, agent 57.47874164581299\n",
      "     mean reward: adversary 32.8125, agent -461.9429659086939\n",
      "on episode 6976 (time taken so far: 0-02:46:30.0)\n",
      "     mean loss: adversary 23695614.0, agent 42.29514408111572\n",
      "     mean reward: adversary 29.0625, agent -372.5811955214023\n",
      "on episode 7040 (time taken so far: 0-02:48:1.0)\n",
      "     mean loss: adversary 23558789.5, agent 43.854573249816895\n",
      "     mean reward: adversary 36.5625, agent -373.63546132197416\n",
      "on episode 7104 (time taken so far: 0-02:49:32.0)\n",
      "     mean loss: adversary 23516320.0, agent 60.08328151702881\n",
      "     mean reward: adversary 33.75, agent -451.19242610203673\n",
      "on episode 7168 (time taken so far: 0-02:51:4.0)\n",
      "     mean loss: adversary 20032167.5, agent 47.27910041809082\n",
      "     mean reward: adversary 23.4375, agent -423.6085424542673\n",
      "on episode 7232 (time taken so far: 0-02:52:35.0)\n",
      "     mean loss: adversary 21153209.5, agent 56.249155044555664\n",
      "     mean reward: adversary 40.78125, agent -422.42297133263133\n",
      "on episode 7296 (time taken so far: 0-02:54:7.0)\n",
      "     mean loss: adversary 19041319.5, agent 52.26815462112427\n",
      "     mean reward: adversary 27.1875, agent -451.56133618742723\n",
      "on episode 7360 (time taken so far: 0-02:55:38.0)\n",
      "     mean loss: adversary 19533939.5, agent 52.612961769104004\n",
      "     mean reward: adversary 29.0625, agent -447.2208767664895\n",
      "on episode 7424 (time taken so far: 0-02:57:9.0)\n",
      "     mean loss: adversary 16692983.5, agent 45.32659959793091\n",
      "     mean reward: adversary 33.28125, agent -417.8141862693142\n",
      "on episode 7488 (time taken so far: 0-02:58:41.0)\n",
      "     mean loss: adversary 17814152.5, agent 50.858370780944824\n",
      "     mean reward: adversary 35.15625, agent -384.928657100197\n",
      "on episode 7552 (time taken so far: 0-03:00:12.0)\n",
      "     mean loss: adversary 16185328.5, agent 43.258225440979004\n",
      "     mean reward: adversary 25.78125, agent -367.5503334397154\n",
      "on episode 7616 (time taken so far: 0-03:01:43.0)\n",
      "     mean loss: adversary 15877538.5, agent 48.62505054473877\n",
      "     mean reward: adversary 19.6875, agent -436.8909672812142\n",
      "on episode 7680 (time taken so far: 0-03:03:15.0)\n",
      "     mean loss: adversary 15304293.0, agent 36.40920877456665\n",
      "     mean reward: adversary 31.875, agent -356.6898095855348\n",
      "on episode 7744 (time taken so far: 0-03:04:46.0)\n",
      "     mean loss: adversary 14697021.5, agent 63.96718215942383\n",
      "     mean reward: adversary 27.1875, agent -498.2468178232983\n",
      "on episode 7808 (time taken so far: 0-03:06:18.0)\n",
      "     mean loss: adversary 13814991.5, agent 52.74356985092163\n",
      "     mean reward: adversary 25.78125, agent -424.38869461346934\n",
      "on episode 7872 (time taken so far: 0-03:07:49.0)\n",
      "     mean loss: adversary 12604591.75, agent 32.40879225730896\n",
      "     mean reward: adversary 22.03125, agent -323.5047666138734\n",
      "on episode 7936 (time taken so far: 0-03:09:20.0)\n",
      "     mean loss: adversary 12675157.5, agent 52.44905948638916\n",
      "     mean reward: adversary 28.125, agent -415.27276685089737\n",
      "on episode 8000 (time taken so far: 0-03:10:51.0)\n",
      "     mean loss: adversary 13081482.75, agent 46.26165771484375\n",
      "     mean reward: adversary 26.71875, agent -417.22486251610286\n",
      "on episode 8064 (time taken so far: 0-03:12:23.0)\n",
      "     mean loss: adversary 12216218.5, agent 51.997886657714844\n",
      "     mean reward: adversary 27.1875, agent -438.27933604570376\n",
      "on episode 8128 (time taken so far: 0-03:13:54.0)\n",
      "     mean loss: adversary 11841628.75, agent 47.12489557266235\n",
      "     mean reward: adversary 33.28125, agent -376.1041538206068\n",
      "on episode 8192 (time taken so far: 0-03:15:26.0)\n",
      "     mean loss: adversary 11095623.75, agent 66.45147228240967\n",
      "     mean reward: adversary 30.46875, agent -458.2303528197884\n",
      "on episode 8256 (time taken so far: 0-03:16:58.0)\n",
      "     mean loss: adversary 10900013.25, agent 46.33101272583008\n",
      "     mean reward: adversary 25.3125, agent -411.5329595247836\n",
      "on episode 8320 (time taken so far: 0-03:18:29.0)\n",
      "     mean loss: adversary 10373629.5, agent 59.70482397079468\n",
      "     mean reward: adversary 28.59375, agent -425.8528595037695\n",
      "on episode 8384 (time taken so far: 0-03:20:0.0)\n",
      "     mean loss: adversary 9298263.125, agent 71.7975959777832\n",
      "     mean reward: adversary 37.03125, agent -540.3401401100606\n",
      "on episode 8448 (time taken so far: 0-03:21:31.0)\n",
      "     mean loss: adversary 9404515.0, agent 55.341586112976074\n",
      "     mean reward: adversary 27.65625, agent -439.61274190514365\n",
      "on episode 8512 (time taken so far: 0-03:23:3.0)\n",
      "     mean loss: adversary 8549772.875, agent 47.962753772735596\n",
      "     mean reward: adversary 23.90625, agent -423.13566827495674\n",
      "on episode 8576 (time taken so far: 0-03:24:34.0)\n",
      "     mean loss: adversary 7827099.0, agent 47.18581485748291\n",
      "     mean reward: adversary 31.40625, agent -421.72185181555983\n",
      "on episode 8640 (time taken so far: 0-03:26:5.0)\n",
      "     mean loss: adversary 8356578.5, agent 46.20297050476074\n",
      "     mean reward: adversary 27.65625, agent -399.39341978528086\n",
      "on episode 8704 (time taken so far: 0-03:27:37.0)\n",
      "     mean loss: adversary 7997111.625, agent 54.84462118148804\n",
      "     mean reward: adversary 43.59375, agent -336.97145331335037\n",
      "on episode 8768 (time taken so far: 0-03:29:9.0)\n",
      "     mean loss: adversary 8518587.625, agent 60.987220764160156\n",
      "     mean reward: adversary 29.0625, agent -465.54999376815493\n",
      "on episode 8832 (time taken so far: 0-03:30:40.0)\n",
      "     mean loss: adversary 8055935.75, agent 37.67492628097534\n",
      "     mean reward: adversary 29.0625, agent -389.2987458961276\n",
      "on episode 8896 (time taken so far: 0-03:32:11.0)\n",
      "     mean loss: adversary 8170647.375, agent 71.78240203857422\n",
      "     mean reward: adversary 45.9375, agent -458.4600363674293\n",
      "on episode 8960 (time taken so far: 0-03:33:42.0)\n",
      "     mean loss: adversary 8171509.75, agent 55.25792360305786\n",
      "     mean reward: adversary 35.15625, agent -416.33276417295804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 9024 (time taken so far: 0-03:35:14.0)\n",
      "     mean loss: adversary 8605281.5, agent 57.26376152038574\n",
      "     mean reward: adversary 36.09375, agent -469.66242334660865\n",
      "on episode 9088 (time taken so far: 0-03:36:45.0)\n",
      "     mean loss: adversary 8291688.0, agent 50.0185661315918\n",
      "     mean reward: adversary 31.40625, agent -393.6620737289095\n",
      "on episode 9152 (time taken so far: 0-03:38:16.0)\n",
      "     mean loss: adversary 7914985.0, agent 40.704402923583984\n",
      "     mean reward: adversary 21.09375, agent -364.59971933891524\n",
      "on episode 9216 (time taken so far: 0-03:39:48.0)\n",
      "     mean loss: adversary 8049917.75, agent 39.17176580429077\n",
      "     mean reward: adversary 31.875, agent -378.0134584107463\n",
      "on episode 9280 (time taken so far: 0-03:41:20.0)\n",
      "     mean loss: adversary 8125262.125, agent 59.5505576133728\n",
      "     mean reward: adversary 39.375, agent -476.4342478821185\n",
      "on episode 9344 (time taken so far: 0-03:42:51.0)\n",
      "     mean loss: adversary 7605453.25, agent 45.483975410461426\n",
      "     mean reward: adversary 12.1875, agent -397.2414381375493\n",
      "on episode 9408 (time taken so far: 0-03:44:22.0)\n",
      "     mean loss: adversary 7955554.5, agent 49.41394329071045\n",
      "     mean reward: adversary 32.34375, agent -407.1510949866759\n",
      "on episode 9472 (time taken so far: 0-03:45:53.0)\n",
      "     mean loss: adversary 8105369.25, agent 47.897626876831055\n",
      "     mean reward: adversary 24.84375, agent -429.8535626172277\n",
      "on episode 9536 (time taken so far: 0-03:47:25.0)\n",
      "     mean loss: adversary 8202903.125, agent 42.51169395446777\n",
      "     mean reward: adversary 25.3125, agent -372.5799139086141\n",
      "on episode 9600 (time taken so far: 0-03:48:56.0)\n",
      "     mean loss: adversary 8130805.75, agent 50.65620231628418\n",
      "     mean reward: adversary 26.71875, agent -454.3170542442834\n",
      "on episode 9664 (time taken so far: 0-03:50:27.0)\n",
      "     mean loss: adversary 8883303.25, agent 58.727742195129395\n",
      "     mean reward: adversary 31.875, agent -418.08135979693066\n",
      "on episode 9728 (time taken so far: 0-03:51:58.0)\n",
      "     mean loss: adversary 9195223.0, agent 51.245853424072266\n",
      "     mean reward: adversary 26.25, agent -439.35764815224496\n",
      "on episode 9792 (time taken so far: 0-03:53:28.0)\n",
      "     mean loss: adversary 9956204.25, agent 45.341219902038574\n",
      "     mean reward: adversary 23.4375, agent -405.21954874595554\n",
      "on episode 9856 (time taken so far: 0-03:54:58.0)\n",
      "     mean loss: adversary 10765243.25, agent 58.49239540100098\n",
      "     mean reward: adversary 37.03125, agent -442.6303194132621\n",
      "on episode 9920 (time taken so far: 0-03:56:28.0)\n",
      "     mean loss: adversary 11623725.75, agent 52.259361743927\n",
      "     mean reward: adversary 41.71875, agent -402.5697070872516\n",
      "on episode 9984 (time taken so far: 0-03:57:59.0)\n",
      "     mean loss: adversary 14041667.0, agent 64.76477432250977\n",
      "     mean reward: adversary 35.625, agent -444.52802787322713\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "adversary_net, agent_net, logger = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_net = SimpleTagNet(config, \"adversary\").to(config.device)\n",
    "agent_net = SimpleTagNet(config, \"agent\").to(config.device)\n",
    "adversary_net.load_state_dict(torch.load('./models/batched-baseline/adversary-net-9984.pth'))\n",
    "agent_net.load_state_dict(torch.load('./models/batched-baseline/agent-net-9984.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654746eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].plot(logger.episodic_losses.adversary[50:], label=\"adversary\")\n",
    "axes[0].plot(logger.episodic_losses.agent[50:], label=\"good agent\")\n",
    "axes[0].set_title(\"loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(logger.episodic_rewards.adversary[50:], label=\"adversary\")\n",
    "axes[1].plot(logger.episodic_rewards.agent[50:], label=\"good agent\")\n",
    "axes[1].set_title(\"reward\")\n",
    "axes[1].legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f65c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode steps 301\n",
      "episode rewards ('adversary', 0.0) ('agent', 0.0)\n"
     ]
    }
   ],
   "source": [
    "def visualize(config, adversary_net, agent_net):\n",
    "    container = Container(config)\n",
    "    adversary_net.eval()\n",
    "    agent_net.eval()\n",
    "    with torch.no_grad():\n",
    "        return run_episode(config, adversary_net, agent_net, should_render=True, is_val=True)\n",
    "\n",
    "episode = visualize(config, adversary_net, agent_net)\n",
    "print(\"episode steps\", episode.steps)\n",
    "print(\"episode rewards\", *episode.reward.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53717a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
