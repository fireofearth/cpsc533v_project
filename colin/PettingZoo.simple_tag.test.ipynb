{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa011482",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Testing some hardcoded algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cef3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "\n",
    "class Normalizer(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        self.n_allagents = len(env.world.agents)\n",
    "        self.n_good = sum(map(lambda a: not a.adversary, env.world.agents))\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_abs_pos(s):\n",
    "        \"\"\"Clip absolute position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -1.5, 1.5) / 1.5\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_rel_pos(s):\n",
    "        \"\"\"Clip relative position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -3, 3) / 3\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # normalize and clip positions\n",
    "        norm_obs = obs.copy()\n",
    "        # normalize velocity of current entity\n",
    "        norm_obs[:2] = norm_obs[:2] / 1.3\n",
    "        # clip/scale abs. position of current entity\n",
    "        norm_obs[2:4] = self.normalize_abs_pos(norm_obs[2:4])\n",
    "        # clip/scale rel. position of other entities\n",
    "        n_range = self.n_landmarks + self.n_allagents - 1\n",
    "        for i in range(n_range):\n",
    "            norm_obs[4 + (2*i):4 + (2*(i + 1))] = self.normalize_rel_pos(\n",
    "                norm_obs[4 + (2*i):4 + (2*(i + 1))]\n",
    "            )\n",
    "        # normalize velocity of other entities\n",
    "        norm_obs[4 + (2*n_range):] = norm_obs[4 + (2*n_range):] / 1.3\n",
    "        return norm_obs\n",
    "\n",
    "    \n",
    "class RewardsShaper(object):\n",
    "    # rdist - distance between adversary-good agent to start computing rewards.\n",
    "    rdist = 2\n",
    "    # collision_dist - distance between adversary-good agent to count collision.\n",
    "    #    Based on PettingZoo numbers. \n",
    "    collision_dist = 0.075 + 0.05\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        # self.n_allagents = len(env.world.agents)\n",
    "        self.name_to_idx = {agent.name: i for i, agent in enumerate(env.world.agents)}\n",
    "        self.idx_to_name = {i: agent.name for i, agent in enumerate(env.world.agents)}\n",
    "        self.goodagent_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"agent\")\n",
    "        ]\n",
    "        self.adversary_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"adversary\")\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def bound(x):\n",
    "        if x < 0.9:\n",
    "            return 0\n",
    "        if x < 1.0:\n",
    "            return (x - 0.9) * 10\n",
    "        return min(np.exp(2 * x - 2), 10)\n",
    "\n",
    "    @classmethod\n",
    "    def dist_to_reward(cls, d):\n",
    "        # make score inverse-linear to distance\n",
    "        x = np.clip(1 - (d - cls.collision_dist)/(cls.rdist - cls.collision_dist), 0, np.inf)\n",
    "        # make score increase non-linearly\n",
    "        expscale = np.exp(3*x)/np.exp(3) - 1/np.exp(3)\n",
    "        # set max reward without collision\n",
    "        _max = 10\n",
    "        return np.clip(_max*expscale, -np.inf, _max)\n",
    "    \n",
    "    def __call__(self, agent_name, obs):\n",
    "        \"\"\"Compute reshaped rewards from observation for agent given agent name.\n",
    "        Adversary: start gaining small rewards as it nears good agents.\n",
    "        \n",
    "        Good agent: starts gaining small penality as it nears bad agents.\n",
    "        \"\"\"\n",
    "        _obs = obs[4 + (2*self.n_landmarks):]\n",
    "        agent_idx = self.name_to_idx[agent_name]\n",
    "        cum_r = 0.\n",
    "        if agent_name.startswith(\"agent\"):\n",
    "            # penalty across all adversaries\n",
    "            for adversary_idx in self.adversary_indices:\n",
    "                # penalty from distance of adversary; penalty of collision\n",
    "                other_idx = adversary_idx - 1 if agent_idx < adversary_idx else adversary_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                cum_r -= max(self.dist_to_reward(d), cum_r)\n",
    "                \n",
    "            # penalty from boudary based on PettingZoo\n",
    "            pos = obs[2:4]\n",
    "            cum_r -= self.bound(abs(pos[0]))\n",
    "            cum_r -= self.bound(abs(pos[1]))\n",
    "        \n",
    "        elif agent_name.startswith(\"adversary\"):\n",
    "            # reward across all agents\n",
    "            for goodagent_idx in self.goodagent_indices:\n",
    "                # reward from distance to agent; reward of collision\n",
    "                other_idx = goodagent_idx - 1 if agent_idx < goodagent_idx else goodagent_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                cum_r += max(self.dist_to_reward(d), cum_r)\n",
    "        \n",
    "        return cum_r\n",
    "\n",
    "    \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2f6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=50,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "So observation forms a vector:\n",
    "`[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]`\n",
    "\n",
    "It's not clear what order the relative positions and agent velocities are from the documentation.\n",
    "Instead refer to the source code. For example the values of `other_agent_rel_positions` are ordered via `env.world.agents`.\n",
    "\n",
    "```\n",
    "class Scenario(BaseScenario):\n",
    "    def observation(self, agent, world):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in world.landmarks:\n",
    "            if not entity.boundary:\n",
    "                entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "        # communication of all other agents\n",
    "        comm = []\n",
    "        other_pos = []\n",
    "        other_vel = []\n",
    "        for other in world.agents:\n",
    "            if other is agent:\n",
    "                continue\n",
    "            comm.append(other.state.c)\n",
    "            other_pos.append(other.state.p_pos - agent.state.p_pos)\n",
    "            if not other.adversary:\n",
    "                other_vel.append(other.state.p_vel)\n",
    "        return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel)\n",
    "```\n",
    "\n",
    "From the code, adversaries are enumerated first in `env.world.agents` starting from adversary_0, then good agents starting agent_0.\n",
    "\n",
    "```\n",
    "def bound(x):\n",
    "    if x < 0.9:\n",
    "        return 0\n",
    "    if x < 1.0:\n",
    "        return (x - 0.9) * 10\n",
    "    return min(np.exp(2 * x - 2), 10)\n",
    "for p in range(world.dim_p):\n",
    "    x = abs(agent.state.p_pos[p])\n",
    "    rew -= bound(x)\n",
    "\n",
    "return rew\n",
    "```\n",
    "\n",
    "Max velocity for each coordinate is 1.3. Agents can move off the arena [-0.9, 0.9] and if good agents do they get penalized by increasingly by distance away.\n",
    "\n",
    "Max possible distance away moving in one direction is around 40.\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 1\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [-0.75398685 -0.97086858]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [-0.8592117  -0.02952041]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -373.37718276314683\n"
     ]
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b81a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary penalty visualization\n",
    "\n",
    "x = np.linspace(0, 3, 100)\n",
    "y1 = np.clip((x - 0.9) * 10, 0, np.inf)\n",
    "y2 = np.clip(np.exp(2 * x - 2), 10, np.inf)\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.plot(x, np.full(x.shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0c31985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize proximity reward\n",
    "shapereward = RewardsShaper(env)\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = shapereward.dist_to_reward(x)\n",
    "plt.plot(x, y)\n",
    "shapereward.dist_to_reward(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce552cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize proximity reward\n",
    "x = np.linspace(0, 5, 200)\n",
    "y = np.clip(1 - (x - shapereward.collision_dist)/(shapereward.rdist - shapereward.collision_dist), 0, 1)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33232068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0 landmark 1\n",
      "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "reward 0.0 reshaped 19.11\n",
      "reward 0.0 reshaped 20.07\n",
      "reward 0.0 reshaped 21.85\n",
      "reward 0.0 reshaped 24.42\n",
      "reward 0.0 reshaped 27.78\n",
      "reward 0.0 reshaped 31.98\n",
      "reward 0.0 reshaped 36.34\n",
      "reward 0.0 reshaped 39.14\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 10.0 reshaped 40.0\n",
      "reward 10.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 10.0 reshaped 37.73\n",
      "reward 0.0 reshaped 35.68\n",
      "reward 0.0 reshaped 34.94\n",
      "reward 0.0 reshaped 35.1\n",
      "reward 0.0 reshaped 35.16\n",
      "reward 0.0 reshaped 35.46\n",
      "reward 0.0 reshaped 38.98\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 39.62\n",
      "reward 0.0 reshaped 38.33\n",
      "reward 0.0 reshaped 35.96\n",
      "reward 0.0 reshaped 34.21\n",
      "reward 0.0 reshaped 34.33\n",
      "reward 0.0 reshaped 33.5\n",
      "reward 0.0 reshaped 31.61\n",
      "reward 0.0 reshaped 29.1\n",
      "reward 0.0 reshaped 28.43\n",
      "reward 0.0 reshaped 29.03\n",
      "reward 0.0 reshaped 30.5\n",
      "reward 0.0 reshaped 31.53\n",
      "reward 0.0 reshaped 33.14\n",
      "reward 0.0 reshaped 34.82\n",
      "reward 0.0 reshaped 35.83\n",
      "reward 0.0 reshaped 34.04\n",
      "reward 0.0 reshaped 33.9\n",
      "reward 0.0 reshaped 35.06\n",
      "reward 0.0 reshaped 35.7\n",
      "reward 0.0 reshaped 36.01\n",
      "reward 0.0 reshaped 35.02\n",
      "reward 0.0 reshaped 33.01\n",
      "reward 0.0 reshaped 32.56\n",
      "reward 0.0 reshaped 33.03\n",
      "reward 0.0 reshaped 33.81\n",
      "reward 0.0 reshaped 35.59\n",
      "reward 0.0 reshaped 36.79\n",
      "reward 0.0 reshaped 37.53\n",
      "reward 0.0 reshaped 35.92\n",
      "reward 0.0 reshaped 35.0\n",
      "reward 0.0 reshaped 33.93\n",
      "reward 0.0 reshaped 34.57\n",
      "reward 0.0 reshaped 35.63\n",
      "reward 0.0 reshaped 35.78\n",
      "reward 0.0 reshaped 37.3\n",
      "reward 0.0 reshaped 37.15\n",
      "reward 0.0 reshaped 35.3\n",
      "reward 0.0 reshaped 34.32\n",
      "reward 0.0 reshaped 33.55\n",
      "reward 0.0 reshaped 34.05\n",
      "reward 0.0 reshaped 35.83\n",
      "reward 0.0 reshaped 37.02\n",
      "reward 0.0 reshaped 37.18\n",
      "reward 0.0 reshaped 36.89\n",
      "reward 0.0 reshaped 37.46\n",
      "reward 0.0 reshaped 38.81\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 40.0\n",
      "reward 0.0 reshaped 39.47\n",
      "reward 0.0 reshaped 37.5\n",
      "reward 0.0 reshaped 37.44\n",
      "reward 0.0 reshaped 35.73\n",
      "reward 0.0 reshaped 33.56\n",
      "reward 0.0 reshaped 31.91\n",
      "reward 0.0 reshaped 29.6\n",
      "reward 0.0 reshaped 29.01\n",
      "reward 0.0 reshaped 29.53\n",
      "reward 0.0 reshaped 28.74\n",
      "reward 0.0 reshaped 29.17\n",
      "reward 0.0 reshaped 30.67\n",
      "reward 0.0 reshaped 33.05\n",
      "reward 0.0 reshaped 34.88\n",
      "reward 0.0 reshaped 35.06\n",
      "reward 0.0 reshaped 35.18\n",
      "reward 0.0 reshaped 34.14\n",
      "reward 0.0 reshaped 34.39\n",
      "reward 0.0 reshaped 33.49\n",
      "reward 0.0 reshaped 31.58\n",
      "reward 0.0 reshaped 31.41\n",
      "reward 0.0 reshaped 32.51\n",
      "reward 0.0 reshaped 32.1\n",
      "reward 0.0 reshaped 33.01\n",
      "reward 0.0 reshaped 33.68\n",
      "reward 0.0 reshaped 33.01\n",
      "episode ran for 605 steps\n",
      "agent_rewards -174.24406066278465\n",
      "adversary_rewards 90.0\n"
     ]
    }
   ],
   "source": [
    "# Demo environment with hardcoded policies, 3 agents of each class, 2 landmarks\n",
    "eps = 0.3\n",
    "\n",
    "def hardcode_policy_1(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def hardcode_policy_2(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            # get agent_0's\n",
    "            x, y = observation[12:14]\n",
    "            if x < -eps: # go left\n",
    "                return 1\n",
    "            elif x > eps: # go right\n",
    "                return 2\n",
    "            elif y < -eps: # go down\n",
    "                return 3\n",
    "            elif y > eps: # go up\n",
    "                return 4\n",
    "            else:\n",
    "                return random.randint(0, 4)\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 0\n",
    "            # return random.randint(0, 4)\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "agent_rewards = 0\n",
    "reshaped_agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "reshaped_adversary_rewards = 0\n",
    "rewardshaper = RewardsShaper(env)\n",
    "normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    reshaped_reward = rewardshaper(agent_name, observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_2(observation, agent_name)\n",
    "        env.step(action)\n",
    "        \n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "        reshaped_adversary_rewards += reshaped_reward\n",
    "    elif \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "        reshaped_agent_rewards += reshaped_reward\n",
    "\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "#         print(\"obs[agent_0 pos]\", np.round(observation[12:14], 2))\n",
    "#         print(\"obs[agent_0 pos]\", np.round(np.linalg.norm(observation[12:14]), 2))\n",
    "        print(\"reward\", reward, \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1064bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method randint in module random:\n",
      "\n",
      "randint(a, b) method of random.Random instance\n",
      "    Return random integer in range [a, b], including both end points.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a696c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs[ovels] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.4 -0.  -0.  -0.   0.   0. ] [-0.31 -0.   -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.3  0.4 -0.  -0.   0.   0. ] [-0.23  0.31 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.18  0.3  -0.   -0.    0.    0.  ] [ 0.13  0.23 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.13 -0.18 -0.   -0.    0.    0.  ] [ 0.1  -0.13 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.1  -0.13 -0.   -0.    0.    0.  ] [ 0.08 -0.1  -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.07 -0.1  -0.   -0.    0.    0.  ] [ 0.06 -0.08 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.06 -0.07 -0.   -0.    0.    0.  ] [ 0.04 -0.06 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.44 -0.06 -0.   -0.    0.    0.  ] [ 0.34 -0.04 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.07 -0.04 -0.   -0.    0.    0.  ] [-0.05 -0.03 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.05 -0.43 -0.   -0.    0.    0.  ] [-0.04 -0.33 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.44 -0.32 -0.   -0.    0.    0.  ] [-0.34 -0.25 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.33 -0.24 -0.   -0.    0.    0.  ] [-0.25 -0.19 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.65 -0.18 -0.   -0.    0.    0.  ] [-0.5  -0.14 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.49 -0.54 -0.   -0.    0.    0.  ] [-0.37 -0.41 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.36 -0.8  -0.   -0.    0.    0.  ] [-0.28 -0.62 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.48 -0.38 -0.   -0.    0.    0.  ] [-0.37 -0.29 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.5 -0.2 -0.  -0.   0.   0. ] [-0.39 -0.15 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.38  0.25 -0.   -0.    0.    0.  ] [-0.29  0.19 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.12  0.19 -0.   -0.    0.    0.  ] [ 0.09  0.15 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.49  0.14 -0.   -0.    0.    0.  ] [ 0.37  0.11 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.37  0.11 -0.   -0.    0.    0.  ] [ 0.28  0.08 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.27  0.48 -0.   -0.    0.    0.  ] [ 0.21  0.37 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.21  0.76 -0.   -0.    0.    0.  ] [ 0.16  0.58 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.55  0.57 -0.   -0.    0.    0.  ] [ 0.43  0.44 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.82  0.43 -0.   -0.    0.    0.  ] [ 0.63  0.33 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.61  0.32 -0.   -0.    0.    0.  ] [ 0.47  0.25 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.46 -0.16 -0.   -0.    0.    0.  ] [ 0.35 -0.12 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.06 -0.12 -0.   -0.    0.    0.  ] [-0.04 -0.09 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.04 -0.49 -0.   -0.    0.    0.  ] [-0.03 -0.38 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.03 -0.37 -0.   -0.    0.    0.  ] [-0.02 -0.28 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.02 -0.68 -0.   -0.    0.    0.  ] [-0.02 -0.52 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [-0.02 -0.91 -0.   -0.    0.    0.  ] [-0.01 -0.7  -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.73 -0.05 -0.   -0.    0.    0.  ] [ 0.56 -0.04 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.77  0.17 -0.   -0.    0.    0.  ] [ 0.59  0.13 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.58 -0.27 -0.   -0.    0.    0.  ] [ 0.44 -0.21 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.83 -0.2  -0.   -0.    0.    0.  ] [ 0.64 -0.16 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.63  0.25 -0.   -0.    0.    0.  ] [ 0.48  0.19 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.87  0.19 -0.   -0.    0.    0.  ] [ 0.67  0.14 -0.   -0.    0.    0.  ]\n",
      "obs[ovels] [ 0.65 -0.26 -0.   -0.    0.    0.  ] [ 0.5 -0.2 -0.  -0.   0.   0. ]\n",
      "obs[ovels] [ 0.49 -0.2  -0.   -0.    0.   -0.  ] [ 0.38 -0.15 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.03 -0.15 -0.   -0.    0.   -0.  ] [-0.03 -0.11 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.43 -0.11 -0.   -0.    0.   -0.  ] [-0.33 -0.08 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.32 -0.48 -0.   -0.    0.   -0.  ] [-0.25 -0.37 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.24  0.04 -0.   -0.    0.   -0.  ] [-0.18  0.03 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.58  0.03 -0.   -0.    0.   -0.  ] [-0.45  0.02 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [-0.43  0.42 -0.   -0.    0.   -0.  ] [-0.33  0.32 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [ 0.07  0.32 -0.   -0.    0.   -0.  ] [ 0.06  0.24 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [ 0.06  0.24 -0.   -0.    0.   -0.  ] [ 0.04  0.18 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [ 0.04  0.18 -0.   -0.    0.   -0.  ] [ 0.03  0.14 -0.   -0.    0.   -0.  ]\n",
      "obs[ovels] [ 0.43  0.13 -0.   -0.    0.   -0.  ] [ 0.33  0.1  -0.   -0.    0.   -0.  ]\n",
      "episode ran for 305 steps\n",
      "agent_rewards -34.09530111734343\n",
      "adversary_rewards 0.0\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_1(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_1\":\n",
    "            return 1\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 3\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "normalize = Normalizer(env)\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_1(observation, agent_name)\n",
    "        env.step(action)\n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "    if \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "    \n",
    "    observation = np.round(observation, 2)\n",
    "    norm_obs = np.round(norm_obs, 2)\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", observation[8:10], norm_obs[8:10])\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2), np.round(norm_obs[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_1\":\n",
    "#         print(observation.shape)\n",
    "        print(\"obs[ovels]\", observation[18:], norm_obs[18:])\n",
    "        pass\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36739dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDPNormalizer(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        self.n_allagents = len(env.world.agents)\n",
    "        self.n_good = sum(map(lambda a: not a.adversary, env.world.agents))\n",
    "\n",
    "        # Extra stuff for POMDP\n",
    "        # distance mask for relative position\n",
    "        self.d_mask = 0.5\n",
    "        # value to mask observations\n",
    "        self.mask_v = 1.5\n",
    "        self.name_to_idx = {agent.name: i for i, agent in enumerate(env.world.agents)}\n",
    "        self.idx_to_name = {i: agent.name for i, agent in enumerate(env.world.agents)}\n",
    "        self.goodagent_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"agent\")\n",
    "        ]\n",
    "        self.adversary_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"adversary\")\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_abs_pos(s):\n",
    "        \"\"\"Clip absolute position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -1.5, 1.5) / 1.5\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_rel_pos(s):\n",
    "        \"\"\"Clip relative position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -3, 3) / 3\n",
    "    \n",
    "    def normalize_opp_rel_pos(self, s):\n",
    "        \"\"\"Clip relative position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -self.d_mask, self.d_mask) / self.d_mask\n",
    "\n",
    "    def __call__(self, obs, agent_name):\n",
    "        # normalize and clip positions\n",
    "        norm_obs = obs.copy()\n",
    "        # normalize velocity of current entity\n",
    "        norm_obs[:2] = norm_obs[:2] / 1.3\n",
    "        # clip/scale abs. position of current entity\n",
    "        norm_obs[2:4] = self.normalize_abs_pos(norm_obs[2:4])\n",
    "        \n",
    "        # clip/scale rel. position of landmarks\n",
    "        for i in range(self.n_landmarks):\n",
    "            norm_obs[4 + (2*i):4 + (2*(i + 1))] = self.normalize_rel_pos(\n",
    "                norm_obs[4 + (2*i):4 + (2*(i + 1))]\n",
    "            )\n",
    "        # normalize velocity of other entities\n",
    "        n_range = self.n_landmarks + self.n_allagents - 1\n",
    "        norm_obs[4 + (2*n_range):] = norm_obs[4 + (2*n_range):] / 1.3\n",
    "\n",
    "        # Apply POMDP\n",
    "        agent_idx = self.name_to_idx[agent_name]\n",
    "        _obs = obs[4 + 2*self.n_landmarks:]\n",
    "        start_idx = 4 + 2*self.n_landmarks\n",
    "        vel_start_idx = 4 + 2*(self.n_landmarks + self.n_allagents - 1)\n",
    "        if agent_name.startswith(\"agent\"):\n",
    "            # mask observation from far adversary\n",
    "            for adversary_idx in self.adversary_indices:\n",
    "                i    = adversary_idx - 1 if agent_idx < adversary_idx else adversary_idx\n",
    "                x, y = _obs[2*i:2*i + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d > self.d_mask:\n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.mask_v\n",
    "                else:\n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.normalize_opp_rel_pos(\n",
    "                        norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)]\n",
    "                    )\n",
    "            for goodagent_idx in self.goodagent_indices:\n",
    "                if goodagent_idx == agent_idx:\n",
    "                    continue\n",
    "                i    = goodagent_idx - 1 if agent_idx < goodagent_idx else goodagent_idx\n",
    "                norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.normalize_rel_pos(\n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)]\n",
    "                )\n",
    "        else:\n",
    "            # agent_name.startswith(\"adversary\")\n",
    "            # mask observation from far agent\n",
    "            for adversary_idx in self.adversary_indices:\n",
    "                if adversary_idx == agent_idx:\n",
    "                    continue\n",
    "                i    = adversary_idx - 1 if agent_idx < adversary_idx else adversary_idx\n",
    "                norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.normalize_rel_pos(\n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)]\n",
    "                )\n",
    "            for vel_i, goodagent_idx in enumerate(self.goodagent_indices):\n",
    "                i    = goodagent_idx - 1 if agent_idx < goodagent_idx else goodagent_idx\n",
    "                x, y = _obs[2*i:2*i + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d > self.d_mask:\n",
    "                    # \n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.mask_v\n",
    "                    norm_obs[vel_start_idx + 2*vel_i:vel_start_idx + 2*(vel_i + 1)] = self.mask_v\n",
    "                else:\n",
    "                    norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)] = self.normalize_opp_rel_pos(\n",
    "                        norm_obs[start_idx + 2*i:start_idx + 2*(i + 1)]\n",
    "                    )\n",
    "\n",
    "        return norm_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eccd6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0 landmark 1\n",
      "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n"
     ]
    }
   ],
   "source": [
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cad33ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.6       ,\n",
       "       0.8       , 0.        , 0.        , 0.76923077, 0.15384615,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that POMDPNormalizer works for adversary\n",
    "env.reset()\n",
    "normalize = POMDPNormalizer(env)\n",
    "agent_name = \"adversary_0\"\n",
    "obs = np.array([\n",
    "    0,0, # velocity\n",
    "    0.5,0.5, # position\n",
    "    0,0,0,0, # position of landmark\n",
    "    0,0, # adversary_1\n",
    "    0,0, # adversary_2\n",
    "    0,0, # agent_0\n",
    "    0.3,0.4, # agent_1\n",
    "    0,0, # agent_2\n",
    "    1,0.2, # agent_0 vel\n",
    "    0,0, # agent_1 vel\n",
    "    0,0, # agent_2 vel\n",
    "])\n",
    "normalize(obs, agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e513fb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.33333333, 0.33333333, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6       , 0.8       , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that POMDPNormalizer works for agent\n",
    "env.reset()\n",
    "normalize = POMDPNormalizer(env)\n",
    "agent_name = \"agent_0\"\n",
    "obs = np.array([\n",
    "    0,0, # velocity\n",
    "    0.5,0.5, # position\n",
    "    0,0,0,0, # position of landmark\n",
    "    0,0, # adversary_0\n",
    "    0.3,0.4, # adversary_1\n",
    "    0,0, # adversary_2\n",
    "    0,0, # agent_1\n",
    "    0,0, # agent_2\n",
    "    0,0, # agent_1 vel\n",
    "    0,0, # agent_2 vel\n",
    "])\n",
    "normalize(obs, agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd122a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = 0.3, 0.4\n",
    "math.sqrt(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e924ca9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0 landmark 1\n",
      "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "start index of adversary pos 8\n",
      "for adversary: start index of agent pos 12\n",
      "for agent: start index of agent pos 14\n",
      "start index of vel 18\n",
      "obs[ovels] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   0.4  0.  -0.  -0.  -0.4] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   0.7  0.  -0.  -0.   0.1] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.    0.92  0.   -0.4  -0.    0.08] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.    0.29  0.4  -0.3  -0.4   0.06] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.    0.22  0.3  -0.62  0.1   0.04] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   -0.23  0.22 -0.47  0.48  0.03] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   -0.18  0.17 -0.35  0.36 -0.38] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.4  -0.13  0.13  0.14 -0.13 -0.28] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.1  -0.1   0.09  0.1   0.3  -0.21] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.08 -0.07  0.47  0.08  0.63 -0.16] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.46 -0.06  0.35  0.46  0.47  0.28] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.34 -0.44  0.27  0.74  0.75  0.21] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.26 -0.33  0.2   0.16  0.56  0.16] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.59 -0.25 -0.25  0.12  0.42  0.52] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.44 -0.59 -0.19  0.09  0.32 -0.01] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.73 -0.44 -0.14  0.47  0.24 -0.41] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.55 -0.33 -0.11 -0.05  0.18  0.09] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.41  0.15 -0.08  0.36  0.13  0.47] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.31  0.51 -0.06  0.27  0.1  -0.05] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.23 -0.01 -0.04  0.2  -0.32 -0.04] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.17 -0.41 -0.03 -0.25 -0.64 -0.03] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.13 -0.31  0.37 -0.19 -0.08 -0.02] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.5  -0.23  0.28  0.26 -0.06 -0.01] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.77 -0.17 -0.19  0.2   0.35 -0.01] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.58 -0.13 -0.14  0.15  0.27 -0.41] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.04 -0.1  -0.51  0.11 -0.2  -0.31] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.43 -0.07 -0.38  0.08 -0.55 -0.23] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.32 -0.45 -0.28  0.46 -0.41 -0.57] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.24  0.06 -0.21 -0.05 -0.31 -0.43] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.18 -0.36 -0.56 -0.04  0.17 -0.32] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.53 -0.27 -0.42 -0.03  0.13 -0.24] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   -0.2  -0.32 -0.02  0.49 -0.18] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   -0.15 -0.24 -0.02 -0.03 -0.14] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.   -0.11 -0.18 -0.41 -0.02  0.3 ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.4  -0.08 -0.13 -0.71 -0.02 -0.18] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.3  -0.06 -0.5  -0.53 -0.01 -0.53] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.17 -0.05 -0.77 -0.4   0.39 -0.4 ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.27 -0.04 -0.98 -0.3   0.69 -0.3 ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.2  -0.43 -0.74  0.18  0.12 -0.22] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.25 -0.32 -0.55  0.13  0.49 -0.17] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.19 -0.64 -0.41  0.5   0.37 -0.53] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.26 -0.48 -0.31  0.77  0.28  0.01] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.2  -0.36 -0.23  0.98  0.61  0.  ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.15 -0.27 -0.95  0.43  0.85  0.  ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.29 -0.2  -1.11  0.32  0.64 -0.4 ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.18 -0.15 -0.83  0.64  0.48 -0.7 ] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.14 -0.11 -1.02  0.48  0.36 -0.92] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [ 0.1  -0.09 -0.77 -0.04 -0.13 -0.69] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.32 -0.06 -0.58 -0.03  0.3  -0.52] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-0.64 -0.05 -0.03 -0.02  0.23  0.01] [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Try masking values with POMDPNormalizer\n",
    "\n",
    "\n",
    "\n",
    "def hardcode_policy_3(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    # adversary\n",
    "    if agent_name == \"adversary_0\":\n",
    "        return 1\n",
    "        pass\n",
    "    # non-adversary\n",
    "    if agent_name == \"agent_0\":\n",
    "#         return random.randint(0, 4)\n",
    "#         return 2\n",
    "        pass\n",
    "    if \"agent\" in agent_name:\n",
    "        return random.randint(0, 4)\n",
    "    return 0\n",
    "\n",
    "env.close()\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "print(\"start index of adversary pos\", 4 + 2*2)\n",
    "print(\"for adversary: start index of agent pos\", 4 + 2*2 + 2*2)\n",
    "print(\"for agent: start index of agent pos\", 4 + 2*3 + 2*2)\n",
    "print(\"start index of vel\", 4 + 2*(2 + 5))\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "normalize = POMDPNormalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation, agent_name)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_3(observation, agent_name)\n",
    "        env.step(action)\n",
    "\n",
    "    observation = np.round(observation, 2)\n",
    "    norm_obs    = np.round(norm_obs, 2)\n",
    "    if agent_name == \"agent_0\":\n",
    "        \n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs[vel]\", observation[:2], norm_obs[:2])\n",
    "#         print(\"obs[vel]\", observation[:2], norm_obs[:2])\n",
    "        \n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "#         print(\"obs[adversary pos]\", observation[8:14], norm_obs[8:14])\n",
    "#         print(\"obs[agent pos]\", observation[14:18], norm_obs[14:18])\n",
    "#         print(\"obs[ovels]\", observation[18:], norm_obs[18:])\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "        # print(observation.shape)\n",
    "        # print(\"obs[agent_0 pos]\", np.round(observation[12:14], 2))\n",
    "#         print(\"obs[agent_0 pos]\", np.round(np.linalg.norm(observation[12:14]), 2))\n",
    "\n",
    "#         print(\"obs[adversary pos]\", observation[8:12], norm_obs[8:12])\n",
    "        # print(\"obs[agent pos]\", observation[12:18], norm_obs[12:18])\n",
    "        print(\"obs[ovels]\", observation[18:], norm_obs[18:])\n",
    "        pass\n",
    "    \n",
    "    time.sleep(0.005)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3cd7b",
   "metadata": {},
   "source": [
    "### Test one agent of each class, 1 landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20653c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1f74e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=1,\n",
    "    num_adversaries=1,\n",
    "    num_obstacles=1,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21bf611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0\n",
      "adversary_0 agent_0\n",
      "reward 0.0 reshaped 1.59\n",
      "reward 0.0 reshaped 1.57\n",
      "reward 0.0 reshaped 1.51\n",
      "reward 0.0 reshaped 1.5\n",
      "reward 0.0 reshaped 1.53\n",
      "reward 0.0 reshaped 1.61\n",
      "reward 0.0 reshaped 1.7\n",
      "reward 0.0 reshaped 1.82\n",
      "reward 0.0 reshaped 1.96\n",
      "reward 0.0 reshaped 2.2\n",
      "reward 0.0 reshaped 2.51\n",
      "reward 0.0 reshaped 2.82\n",
      "reward 0.0 reshaped 3.24\n",
      "reward 0.0 reshaped 3.78\n",
      "reward 0.0 reshaped 4.45\n",
      "reward 0.0 reshaped 5.27\n",
      "reward 0.0 reshaped 6.21\n",
      "reward 0.0 reshaped 7.22\n",
      "reward 0.0 reshaped 8.2\n",
      "reward 0.0 reshaped 8.76\n",
      "reward 0.0 reshaped 8.9\n",
      "reward 0.0 reshaped 8.95\n",
      "reward 0.0 reshaped 8.65\n",
      "reward 0.0 reshaped 7.95\n",
      "reward 0.0 reshaped 7.34\n",
      "reward 0.0 reshaped 6.88\n",
      "reward 0.0 reshaped 6.38\n",
      "reward 0.0 reshaped 6.29\n",
      "reward 0.0 reshaped 6.47\n",
      "reward 0.0 reshaped 6.82\n",
      "reward 0.0 reshaped 6.85\n",
      "reward 0.0 reshaped 6.86\n",
      "reward 0.0 reshaped 7.07\n",
      "reward 0.0 reshaped 7.2\n",
      "reward 0.0 reshaped 7.27\n",
      "reward 0.0 reshaped 7.42\n",
      "reward 0.0 reshaped 7.86\n",
      "reward 0.0 reshaped 8.6\n",
      "reward 0.0 reshaped 9.16\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 10.0 reshaped 9.98\n",
      "reward 0.0 reshaped 8.92\n",
      "reward 0.0 reshaped 8.18\n",
      "reward 0.0 reshaped 7.92\n",
      "reward 0.0 reshaped 7.71\n",
      "reward 0.0 reshaped 7.21\n",
      "reward 0.0 reshaped 6.97\n",
      "reward 0.0 reshaped 6.81\n",
      "reward 0.0 reshaped 6.97\n",
      "reward 0.0 reshaped 7.41\n",
      "reward 0.0 reshaped 7.62\n",
      "reward 0.0 reshaped 7.67\n",
      "reward 0.0 reshaped 7.39\n",
      "reward 0.0 reshaped 6.86\n",
      "reward 0.0 reshaped 6.69\n",
      "reward 0.0 reshaped 6.35\n",
      "reward 0.0 reshaped 6.1\n",
      "reward 0.0 reshaped 6.16\n",
      "reward 0.0 reshaped 6.42\n",
      "reward 0.0 reshaped 6.4\n",
      "reward 0.0 reshaped 6.6\n",
      "reward 0.0 reshaped 6.48\n",
      "reward 0.0 reshaped 6.38\n",
      "reward 0.0 reshaped 6.58\n",
      "reward 0.0 reshaped 6.91\n",
      "reward 0.0 reshaped 7.16\n",
      "reward 0.0 reshaped 7.33\n",
      "reward 0.0 reshaped 7.82\n",
      "reward 0.0 reshaped 8.61\n",
      "reward 10.0 reshaped 9.66\n",
      "reward 10.0 reshaped 9.68\n",
      "reward 0.0 reshaped 8.97\n",
      "reward 0.0 reshaped 8.61\n",
      "reward 0.0 reshaped 7.9\n",
      "reward 0.0 reshaped 7.38\n",
      "reward 0.0 reshaped 6.66\n",
      "reward 0.0 reshaped 6.48\n",
      "reward 0.0 reshaped 6.69\n",
      "reward 0.0 reshaped 7.21\n",
      "reward 0.0 reshaped 7.62\n",
      "reward 0.0 reshaped 7.55\n",
      "reward 0.0 reshaped 7.12\n",
      "reward 0.0 reshaped 6.81\n",
      "reward 0.0 reshaped 6.91\n",
      "reward 0.0 reshaped 7.33\n",
      "reward 0.0 reshaped 8.03\n",
      "reward 0.0 reshaped 8.34\n",
      "reward 0.0 reshaped 8.73\n",
      "reward 0.0 reshaped 9.31\n",
      "reward 0.0 reshaped 9.34\n",
      "reward 0.0 reshaped 9.31\n",
      "reward 10.0 reshaped 9.73\n",
      "reward 10.0 reshaped 9.63\n",
      "reward 0.0 reshaped 8.77\n",
      "reward 0.0 reshaped 8.04\n",
      "reward 0.0 reshaped 7.83\n",
      "reward 0.0 reshaped 7.64\n",
      "reward 0.0 reshaped 7.24\n",
      "reward 0.0 reshaped 7.14\n",
      "reward 0.0 reshaped 6.82\n",
      "reward 0.0 reshaped 6.86\n",
      "reward 0.0 reshaped 7.17\n",
      "reward 0.0 reshaped 7.65\n",
      "reward 0.0 reshaped 8.33\n",
      "reward 0.0 reshaped 8.57\n",
      "reward 0.0 reshaped 9.04\n",
      "reward 0.0 reshaped 8.93\n",
      "reward 0.0 reshaped 8.82\n",
      "reward 0.0 reshaped 8.46\n",
      "reward 0.0 reshaped 8.01\n",
      "reward 0.0 reshaped 7.94\n",
      "reward 0.0 reshaped 8.05\n",
      "reward 0.0 reshaped 8.02\n",
      "reward 0.0 reshaped 8.33\n",
      "reward 0.0 reshaped 8.54\n",
      "reward 0.0 reshaped 9.14\n",
      "reward 0.0 reshaped 9.14\n",
      "reward 0.0 reshaped 8.68\n",
      "reward 0.0 reshaped 8.33\n",
      "reward 0.0 reshaped 7.63\n",
      "reward 0.0 reshaped 7.13\n",
      "reward 0.0 reshaped 7.12\n",
      "reward 0.0 reshaped 6.77\n",
      "reward 0.0 reshaped 6.84\n",
      "reward 0.0 reshaped 7.22\n",
      "reward 0.0 reshaped 7.51\n",
      "reward 0.0 reshaped 7.52\n",
      "reward 0.0 reshaped 7.47\n",
      "reward 0.0 reshaped 7.68\n",
      "reward 0.0 reshaped 7.84\n",
      "reward 0.0 reshaped 7.65\n",
      "reward 0.0 reshaped 7.5\n",
      "reward 0.0 reshaped 7.39\n",
      "reward 0.0 reshaped 6.98\n",
      "reward 0.0 reshaped 6.36\n",
      "reward 0.0 reshaped 6.22\n",
      "reward 0.0 reshaped 6.41\n",
      "reward 0.0 reshaped 6.88\n",
      "reward 0.0 reshaped 7.05\n",
      "reward 0.0 reshaped 7.36\n",
      "reward 0.0 reshaped 7.91\n",
      "reward 0.0 reshaped 8.01\n",
      "reward 0.0 reshaped 7.78\n",
      "reward 0.0 reshaped 7.79\n",
      "reward 0.0 reshaped 7.75\n",
      "reward 0.0 reshaped 7.32\n",
      "reward 0.0 reshaped 6.65\n",
      "reward 0.0 reshaped 6.49\n",
      "reward 0.0 reshaped 6.71\n",
      "reward 0.0 reshaped 7.24\n",
      "reward 0.0 reshaped 7.6\n",
      "reward 0.0 reshaped 7.47\n",
      "reward 0.0 reshaped 7.22\n",
      "reward 0.0 reshaped 7.0\n",
      "reward 0.0 reshaped 7.01\n",
      "reward 0.0 reshaped 7.17\n",
      "reward 0.0 reshaped 7.37\n",
      "reward 0.0 reshaped 7.82\n",
      "reward 0.0 reshaped 8.54\n",
      "reward 0.0 reshaped 9.06\n",
      "reward 0.0 reshaped 8.97\n",
      "reward 0.0 reshaped 9.04\n",
      "reward 0.0 reshaped 9.1\n",
      "reward 0.0 reshaped 8.88\n",
      "reward 0.0 reshaped 8.76\n",
      "reward 0.0 reshaped 9.03\n",
      "reward 0.0 reshaped 9.19\n",
      "reward 0.0 reshaped 8.89\n",
      "reward 0.0 reshaped 9.01\n",
      "reward 0.0 reshaped 9.24\n",
      "reward 0.0 reshaped 9.31\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.36\n",
      "reward 0.0 reshaped 9.2\n",
      "reward 0.0 reshaped 8.54\n",
      "reward 0.0 reshaped 8.06\n",
      "reward 0.0 reshaped 7.93\n",
      "reward 0.0 reshaped 7.82\n",
      "reward 0.0 reshaped 7.73\n",
      "reward 0.0 reshaped 7.49\n",
      "reward 0.0 reshaped 6.99\n",
      "reward 0.0 reshaped 6.46\n",
      "reward 0.0 reshaped 5.82\n",
      "reward 0.0 reshaped 5.62\n",
      "reward 0.0 reshaped 5.7\n",
      "reward 0.0 reshaped 5.96\n",
      "reward 0.0 reshaped 6.37\n",
      "reward 0.0 reshaped 6.92\n",
      "reward 0.0 reshaped 7.11\n",
      "reward 0.0 reshaped 7.26\n",
      "reward 0.0 reshaped 7.07\n",
      "reward 0.0 reshaped 6.73\n",
      "reward 0.0 reshaped 6.49\n",
      "reward 0.0 reshaped 6.05\n",
      "reward 0.0 reshaped 5.99\n",
      "reward 0.0 reshaped 6.19\n",
      "reward 0.0 reshaped 6.55\n",
      "reward 0.0 reshaped 7.11\n",
      "reward 0.0 reshaped 7.29\n",
      "reward 0.0 reshaped 7.09\n",
      "reward 0.0 reshaped 7.2\n",
      "reward 0.0 reshaped 7.09\n",
      "reward 0.0 reshaped 6.79\n",
      "reward 0.0 reshaped 6.78\n",
      "reward 0.0 reshaped 6.54\n",
      "reward 0.0 reshaped 6.59\n",
      "reward 0.0 reshaped 6.4\n",
      "reward 0.0 reshaped 6.46\n",
      "reward 0.0 reshaped 6.29\n",
      "reward 0.0 reshaped 6.41\n",
      "reward 0.0 reshaped 6.5\n",
      "reward 0.0 reshaped 6.56\n",
      "reward 0.0 reshaped 6.84\n",
      "reward 0.0 reshaped 6.73\n",
      "reward 0.0 reshaped 6.62\n",
      "reward 0.0 reshaped 6.82\n",
      "reward 0.0 reshaped 6.98\n",
      "reward 0.0 reshaped 6.77\n",
      "reward 0.0 reshaped 6.93\n",
      "reward 0.0 reshaped 7.41\n",
      "reward 0.0 reshaped 7.78\n",
      "reward 0.0 reshaped 7.7\n",
      "reward 0.0 reshaped 7.76\n",
      "reward 0.0 reshaped 7.39\n",
      "reward 0.0 reshaped 6.74\n",
      "reward 0.0 reshaped 6.62\n",
      "reward 0.0 reshaped 6.86\n",
      "reward 0.0 reshaped 7.42\n",
      "reward 0.0 reshaped 7.86\n",
      "reward 0.0 reshaped 8.14\n",
      "reward 0.0 reshaped 8.29\n",
      "reward 0.0 reshaped 8.15\n",
      "reward 0.0 reshaped 8.22\n",
      "reward 0.0 reshaped 8.5\n",
      "reward 0.0 reshaped 8.31\n",
      "reward 0.0 reshaped 8.21\n",
      "reward 0.0 reshaped 7.63\n",
      "reward 0.0 reshaped 7.19\n",
      "reward 0.0 reshaped 6.88\n",
      "reward 0.0 reshaped 7.01\n",
      "reward 0.0 reshaped 7.48\n",
      "reward 0.0 reshaped 7.84\n",
      "reward 0.0 reshaped 8.5\n",
      "reward 0.0 reshaped 8.78\n",
      "reward 0.0 reshaped 9.1\n",
      "reward 0.0 reshaped 9.32\n",
      "reward 0.0 reshaped 9.46\n",
      "reward 10.0 reshaped 9.55\n",
      "reward 10.0 reshaped 9.71\n",
      "reward 10.0 reshaped 9.79\n",
      "reward 0.0 reshaped 9.25\n",
      "reward 0.0 reshaped 8.85\n",
      "reward 0.0 reshaped 8.14\n",
      "reward 0.0 reshaped 7.26\n",
      "reward 0.0 reshaped 7.01\n",
      "reward 0.0 reshaped 7.19\n",
      "reward 0.0 reshaped 7.33\n",
      "reward 0.0 reshaped 7.82\n",
      "reward 0.0 reshaped 8.13\n",
      "reward 0.0 reshaped 8.71\n",
      "reward 0.0 reshaped 9.07\n",
      "reward 0.0 reshaped 8.89\n",
      "reward 0.0 reshaped 8.62\n",
      "reward 0.0 reshaped 8.22\n",
      "reward 0.0 reshaped 8.31\n",
      "reward 0.0 reshaped 8.13\n",
      "reward 0.0 reshaped 8.22\n",
      "reward 0.0 reshaped 8.53\n",
      "reward 0.0 reshaped 8.74\n",
      "reward 0.0 reshaped 9.31\n",
      "reward 10.0 reshaped 9.76\n",
      "reward 0.0 reshaped 9.14\n",
      "reward 0.0 reshaped 8.28\n",
      "reward 0.0 reshaped 7.3\n",
      "reward 0.0 reshaped 6.3\n",
      "reward 0.0 reshaped 5.94\n",
      "reward 0.0 reshaped 5.98\n",
      "reward 0.0 reshaped 6.32\n",
      "reward 0.0 reshaped 6.94\n",
      "reward 0.0 reshaped 7.81\n",
      "reward 0.0 reshaped 8.62\n",
      "reward 0.0 reshaped 9.19\n",
      "reward 0.0 reshaped 9.42\n",
      "reward 0.0 reshaped 9.43\n",
      "reward 0.0 reshaped 9.34\n",
      "reward 0.0 reshaped 9.37\n",
      "reward 0.0 reshaped 8.79\n",
      "reward 0.0 reshaped 8.23\n",
      "reward 0.0 reshaped 7.39\n",
      "reward 0.0 reshaped 6.65\n",
      "reward 0.0 reshaped 5.94\n",
      "reward 0.0 reshaped 5.64\n",
      "reward 0.0 reshaped 5.59\n",
      "reward 0.0 reshaped 5.74\n",
      "reward 0.0 reshaped 6.1\n",
      "reward 0.0 reshaped 6.64\n",
      "reward 0.0 reshaped 7.32\n",
      "episode ran for 601 steps\n",
      "agent_rewards -410.1671760111024\n",
      "adversary_rewards 100.0\n"
     ]
    }
   ],
   "source": [
    "# Demo environment with hardcoded policies, 1 agent of each class, no landmarks\n",
    "eps = 0.3\n",
    "def hardcode_policy_2(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            # get agent_0's\n",
    "            x, y = observation[6:8]\n",
    "            if x < -eps: # go left\n",
    "                return 1\n",
    "            elif x > eps: # go right\n",
    "                return 2\n",
    "            elif y < -eps: # go down\n",
    "                return 3\n",
    "            elif y > eps: # go up\n",
    "                return 4\n",
    "            else:\n",
    "                return random.randint(0, 4)\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 0\n",
    "            # return random.randint(0, 4)\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "agent_rewards = 0\n",
    "reshaped_agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "reshaped_adversary_rewards = 0\n",
    "rewardshaper = RewardsShaper(env)\n",
    "normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    reshaped_reward = rewardshaper(agent_name, observation)\n",
    "#     norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_2(observation, agent_name)\n",
    "        env.step(action)\n",
    "        \n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "        reshaped_adversary_rewards += reshaped_reward\n",
    "    elif \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "        reshaped_agent_rewards += reshaped_reward\n",
    "\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "#         print(\"reward\", np.round(reward, 2), \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "#         print(\"obs[agent_0 pos]\", np.round(observation[12:14], 2))\n",
    "#         print(\"obs[agent_0 pos]\", np.round(np.linalg.norm(observation[12:14]), 2))\n",
    "        print(\"reward\", reward, \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        pass\n",
    "    \n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d52e8e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs[ovels] [0. 0.] obs[ovels] [0. 0.]\n",
      "obs[ovels] [ 0.  -0.4] obs[ovels] [ 0.   -0.31]\n",
      "obs[ovels] [ 0.  -0.7] obs[ovels] [ 0.   -0.54]\n",
      "obs[ovels] [ 0.   -0.92] obs[ovels] [ 0.   -0.71]\n",
      "obs[ovels] [ 0.   -1.09] obs[ovels] [ 0.   -0.84]\n",
      "obs[ovels] [ 0.   -1.22] obs[ovels] [ 0.   -0.94]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.07 -1.27] obs[ovels] [ 0.06 -0.98]\n",
      "obs[ovels] [ 0.39 -1.24] obs[ovels] [ 0.3  -0.96]\n",
      "obs[ovels] [ 0.28 -1.27] obs[ovels] [ 0.21 -0.98]\n",
      "obs[ovels] [ 0.2  -1.29] obs[ovels] [ 0.15 -0.99]\n",
      "obs[ovels] [ 0.14 -1.29] obs[ovels] [ 0.11 -0.99]\n",
      "obs[ovels] [ 0.1 -1.3] obs[ovels] [ 0.08 -1.  ]\n",
      "obs[ovels] [ 0.07 -1.3 ] obs[ovels] [ 0.05 -1.  ]\n",
      "obs[ovels] [ 0.05 -1.3 ] obs[ovels] [ 0.04 -1.  ]\n",
      "obs[ovels] [ 0.04 -1.3 ] obs[ovels] [ 0.03 -1.  ]\n",
      "obs[ovels] [ 0.03 -1.3 ] obs[ovels] [ 0.02 -1.  ]\n",
      "obs[ovels] [ 0.02 -1.3 ] obs[ovels] [ 0.01 -1.  ]\n",
      "obs[ovels] [ 0.01 -1.3 ] obs[ovels] [ 0.01 -1.  ]\n",
      "obs[ovels] [ 0.01 -1.3 ] obs[ovels] [ 0.01 -1.  ]\n",
      "obs[ovels] [ 0.01 -1.3 ] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "obs[ovels] [ 0.  -1.3] obs[ovels] [ 0. -1.]\n",
      "episode ran for 601 steps\n",
      "agent_rewards -2822.7052220972537\n",
      "adversary_rewards 20.0\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_3(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            pass\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 3\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "normalize = Normalizer(env)\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    # env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_3(observation, agent_name)\n",
    "        env.step(action)\n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "    if \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "    \n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2), \"obs[vel]\", np.round(norm_obs[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2), \"obs[pos]\", np.round(norm_obs[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[4:6], 2), \"obs[landmark1]\", np.round(norm_obs[4:6], 2))\n",
    "#         print(\"obs[oagents]\", np.round(observation[6:8], 2), \"obs[oagents]\", np.round(norm_obs[6:8], 2))\n",
    "#         print(\"obs[ovels]\", np.round(observation[8:], 2), \"obs[ovels]\", np.round(norm_obs[8:], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "        print(\"obs[ovels]\", np.round(observation[8:], 2), \"obs[ovels]\", np.round(norm_obs[8:], 2))\n",
    "        pass\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef16af8",
   "metadata": {},
   "source": [
    "### Visualize physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c89871",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "919cadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=1,\n",
    "    num_adversaries=1,\n",
    "    num_obstacles=0,\n",
    "    max_cycles=10,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82887822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 1 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 2 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 3 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 4 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 5 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 6 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 7 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 8 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 9 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 10 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 11 obs[pos] [-0.63 -0.9 ] obs[vel] [0.3 0. ]\n",
      "step 12 obs[pos] [-0.57 -0.9 ] obs[vel] [0.52 0.  ]\n",
      "step 13 obs[pos] [-0.5 -0.9] obs[vel] [0.69 0.  ]\n",
      "step 14 obs[pos] [-0.42 -0.9 ] obs[vel] [0.82 0.  ]\n",
      "step 15 obs[pos] [-0.39 -0.9 ] obs[vel] [0.32 0.  ]\n",
      "step 16 obs[pos] [-0.4 -0.9] obs[vel] [-0.06  0.  ]\n",
      "step 17 obs[pos] [-0.43 -0.9 ] obs[vel] [-0.35  0.  ]\n",
      "step 18 obs[pos] [-0.46 -0.9 ] obs[vel] [-0.26  0.  ]\n",
      "step 19 obs[pos] [-0.48 -0.9 ] obs[vel] [-0.2  0. ]\n",
      "step 20 obs[pos] [-0.49 -0.9 ] obs[vel] [-0.15  0.  ]\n",
      "step 21 obs[pos] [-0.5 -0.9] obs[vel] [-0.11  0.  ]\n",
      "step 22 obs[pos] [-0.51 -0.9 ] obs[vel] [-0.08  0.  ]\n",
      "step 23 obs[pos] [-0.52 -0.9 ] obs[vel] [-0.06  0.  ]\n",
      "step 24 obs[pos] [-0.52 -0.9 ] obs[vel] [-0.05  0.  ]\n",
      "step 25 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.03  0.  ]\n",
      "step 26 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.03  0.  ]\n",
      "step 27 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.02  0.  ]\n",
      "step 28 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 29 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 30 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 31 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 32 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 33 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 34 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 35 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 36 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 37 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 38 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 39 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 40 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 41 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 42 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 43 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 44 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 45 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 46 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 47 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 48 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 49 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 50 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_4(step_idx, observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    \n",
    "    if agent_name == \"adversary_0\":\n",
    "        if step_idx == 10 or step_idx == 11 or step_idx == 12 or step_idx == 13:\n",
    "            return 2\n",
    "        elif step_idx == 14 or step_idx == 15 or step_idx == 16:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "env.reset()\n",
    "# normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    step_idx = agent_step_idx // 2\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "#     norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_4(step_idx, observation, agent_name)\n",
    "        env.step(action)\n",
    "    \n",
    "    observation = np.round(observation, 2)\n",
    "    if agent_name == \"agent_0\":\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "        print(\"step\", step_idx, \"obs[pos]\", observation[2:4], \"obs[vel]\", observation[0:2])\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f437e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adversary_0 agent_0\n",
      "step 0 agent adversary_0\n",
      "obs[agent pos] [ 0.79       -0.40000004] obs[adversary pos] [ 0.46 -0.91]\n",
      "step 0 agent agent_0\n",
      "obs[agent pos] [ 0.79 -0.39] obs[adversary pos] [ 0.46 -0.9 ]\n",
      "step 1 agent adversary_0\n",
      "obs[agent pos] [ 0.83000004 -0.40000004] obs[adversary pos] [ 0.43 -0.91]\n",
      "step 1 agent agent_0\n",
      "obs[agent pos] [ 0.83 -0.39] obs[adversary pos] [ 0.42999998 -0.9       ]\n",
      "step 2 agent adversary_0\n",
      "obs[agent pos] [ 0.9        -0.40000004] obs[adversary pos] [ 0.38 -0.91]\n",
      "step 2 agent agent_0\n",
      "obs[agent pos] [ 0.9  -0.39] obs[adversary pos] [ 0.38 -0.9 ]\n",
      "step 3 agent adversary_0\n",
      "obs[agent pos] [ 0.99       -0.40000004] obs[adversary pos] [ 0.31 -0.91]\n",
      "step 3 agent agent_0\n",
      "obs[agent pos] [ 0.99 -0.39] obs[adversary pos] [ 0.31 -0.9 ]\n",
      "step 4 agent adversary_0\n",
      "obs[agent pos] [ 1.1        -0.40000004] obs[adversary pos] [ 0.23 -0.91]\n",
      "step 4 agent agent_0\n",
      "obs[agent pos] [ 1.1  -0.39] obs[adversary pos] [ 0.23000002 -0.9       ]\n",
      "step 5 agent adversary_0\n",
      "obs[agent pos] [ 1.23       -0.40000004] obs[adversary pos] [ 0.14 -0.91]\n",
      "step 5 agent agent_0\n",
      "obs[agent pos] [ 1.23 -0.39] obs[adversary pos] [ 0.13999999 -0.9       ]\n",
      "step 6 agent adversary_0\n",
      "obs[agent pos] [ 1.36       -0.40000004] obs[adversary pos] [ 0.04 -0.91]\n",
      "step 6 agent agent_0\n",
      "obs[agent pos] [ 1.36 -0.39] obs[adversary pos] [ 0.03999996 -0.9       ]\n",
      "step 7 agent adversary_0\n",
      "obs[agent pos] [ 1.49       -0.40000004] obs[adversary pos] [-0.06 -0.91]\n",
      "step 7 agent agent_0\n",
      "obs[agent pos] [ 1.49 -0.39] obs[adversary pos] [-0.05999994 -0.9       ]\n",
      "step 8 agent adversary_0\n",
      "obs[agent pos] [ 1.62       -0.40000004] obs[adversary pos] [-0.16 -0.91]\n",
      "step 8 agent agent_0\n",
      "obs[agent pos] [ 1.62 -0.39] obs[adversary pos] [-0.15999997 -0.9       ]\n",
      "step 9 agent adversary_0\n",
      "obs[agent pos] [ 1.75       -0.40000004] obs[adversary pos] [-0.26 -0.91]\n",
      "step 9 agent agent_0\n",
      "obs[agent pos] [ 1.75 -0.39] obs[adversary pos] [-0.26 -0.9 ]\n",
      "step 10 agent adversary_0\n",
      "obs[agent pos] [ 1.88       -0.40000004] obs[adversary pos] [-0.36 -0.91]\n",
      "step 10 agent agent_0\n",
      "obs[agent pos] [ 1.88 -0.39] obs[adversary pos] [-0.36 -0.9 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def hardcode_policy_5(step_idx, observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    \n",
    "    if agent_name == \"adversary_0\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "env.close()\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    step_idx = agent_step_idx // 2\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_5(step_idx, observation, agent_name)\n",
    "        env.step(action)\n",
    "    \n",
    "    observation = np.round(observation, 2)\n",
    "    print(\"step\", step_idx, \"agent\", agent_name)\n",
    "    if agent_name == \"agent_0\":\n",
    "        print(\"obs[agent pos]\", observation[2:4], \"obs[adversary pos]\", observation[2:4] + observation[4:6])\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "        print(\"obs[agent pos]\", observation[2:4] + observation[4:6], \"obs[adversary pos]\", observation[2:4])\n",
    "        pass\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5476e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
