{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa011482",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Testing some hardcoded algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cef3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "\n",
    "class Normalizer(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        self.n_allagents = len(env.world.agents)\n",
    "        self.n_good = sum(map(lambda a: not a.adversary, env.world.agents))\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_abs_pos(s):\n",
    "        \"\"\"Clip absolute position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -1.5, 1.5) / 1.5\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_rel_pos(s):\n",
    "        \"\"\"Clip relative position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -3, 3) / 3\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # normalize and clip positions\n",
    "        norm_obs = obs.copy()\n",
    "        # normalize velocity of current entity\n",
    "        norm_obs[:2] = norm_obs[:2] / 1.3\n",
    "        # clip/scale abs. position of current entity\n",
    "        norm_obs[2:4] = self.normalize_abs_pos(norm_obs[2:4])\n",
    "        # clip/scale rel. position of other entities\n",
    "        n_range = self.n_landmarks + self.n_allagents - 1\n",
    "        for i in range(n_range):\n",
    "            norm_obs[4 + (2*i):4 + (2*(i + 1))] = self.normalize_rel_pos(\n",
    "                norm_obs[4 + (2*i):4 + (2*(i + 1))]\n",
    "            )\n",
    "        # normalize velocity of other entities\n",
    "        norm_obs[4 + (2*n_range):] = norm_obs[4 + (2*n_range):] / 1.3\n",
    "        return norm_obs\n",
    "\n",
    "    \n",
    "class RewardsShaper(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        # self.n_allagents = len(env.world.agents)\n",
    "        self.name_to_idx = {agent.name: i for i, agent in enumerate(env.world.agents)}\n",
    "        self.idx_to_name = {i: agent.name for i, agent in enumerate(env.world.agents)}\n",
    "        self.goodagent_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"agent\")\n",
    "        ]\n",
    "        self.adversary_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"adversary\")\n",
    "        ]\n",
    "        # rdist - distance between adversary-good agent to start computing rewards.\n",
    "        self.rdist = 2\n",
    "        # collision_dist - distance between adversary-good agent to count collision.\n",
    "        #    Based on PettingZoo numbers. \n",
    "        self.collision_dist = 0.075 + 0.05\n",
    "\n",
    "    @staticmethod\n",
    "    def bound(x):\n",
    "        if x < 0.9:\n",
    "            return 0\n",
    "        if x < 1.0:\n",
    "            return (x - 0.9) * 10\n",
    "        return min(np.exp(2 * x - 2), 10)\n",
    "        \n",
    "    def __call__(self, agent_name, obs):\n",
    "        \"\"\"Compute reshaped rewards from observation for agent given agent name.\n",
    "        Adversary: start gaining small rewards as it nears good agents.\n",
    "        \n",
    "        Good agent: starts gaining small penality as it nears bad agents.\n",
    "        \"\"\"\n",
    "        _obs = obs[4 + (2*self.n_landmarks):]\n",
    "        agent_idx = self.name_to_idx[agent_name]\n",
    "        cum_r = 0.\n",
    "        if agent_name.startswith(\"agent\"):\n",
    "            # penalty across all adversaries\n",
    "            for adversary_idx in self.adversary_indices:\n",
    "                # penalty from distance of adversary; penalty of collision\n",
    "                other_idx = adversary_idx - 1 if agent_idx < adversary_idx else adversary_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d < self.collision_dist:\n",
    "                    cum_r -= 10\n",
    "                else:\n",
    "                    cum_r -= min(max(1 -  (1/self.rdist)*d, 0), 1)\n",
    "                \n",
    "            # penalty from boudary based on PettingZoo\n",
    "            pos = obs[2:4]\n",
    "            cum_r -= self.bound(abs(pos[0]))\n",
    "            cum_r -= self.bound(abs(pos[1]))\n",
    "        \n",
    "        elif agent_name.startswith(\"adversary\"):\n",
    "            # reward across all agents\n",
    "            for goodagent_idx in self.goodagent_indices:\n",
    "                # reward from distance to agent; reward of collision\n",
    "                other_idx = goodagent_idx - 1 if agent_idx < goodagent_idx else goodagent_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d < self.collision_dist:\n",
    "                    cum_r += 10\n",
    "                else:\n",
    "                    cum_r += min(max(1 -  (1/self.rdist)*d, 0), 1)\n",
    "        \n",
    "        return cum_r\n",
    "    \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc2f6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=100,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "So observation forms a vector:\n",
    "`[self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]`\n",
    "\n",
    "It's not clear what order the relative positions and agent velocities are from the documentation.\n",
    "Instead refer to the source code. For example the values of `other_agent_rel_positions` are ordered via `env.world.agents`.\n",
    "\n",
    "```\n",
    "class Scenario(BaseScenario):\n",
    "    def observation(self, agent, world):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in world.landmarks:\n",
    "            if not entity.boundary:\n",
    "                entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "        # communication of all other agents\n",
    "        comm = []\n",
    "        other_pos = []\n",
    "        other_vel = []\n",
    "        for other in world.agents:\n",
    "            if other is agent:\n",
    "                continue\n",
    "            comm.append(other.state.c)\n",
    "            other_pos.append(other.state.p_pos - agent.state.p_pos)\n",
    "            if not other.adversary:\n",
    "                other_vel.append(other.state.p_vel)\n",
    "        return np.concatenate([agent.state.p_vel] + [agent.state.p_pos] + entity_pos + other_pos + other_vel)\n",
    "```\n",
    "\n",
    "From the code, adversaries are enumerated first in `env.world.agents` starting from adversary_0, then good agents starting agent_0.\n",
    "\n",
    "```\n",
    "def bound(x):\n",
    "    if x < 0.9:\n",
    "        return 0\n",
    "    if x < 1.0:\n",
    "        return (x - 0.9) * 10\n",
    "    return min(np.exp(2 * x - 2), 10)\n",
    "for p in range(world.dim_p):\n",
    "    x = abs(agent.state.p_pos[p])\n",
    "    rew -= bound(x)\n",
    "\n",
    "return rew\n",
    "```\n",
    "\n",
    "Max velocity for each coordinate is 1.3. Agents can move off the arena [-0.9, 0.9] and if good agents do they get penalized by increasingly by distance away.\n",
    "\n",
    "Max possible distance away moving in one direction is around 40.\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53e39403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd605a8e6a0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 3, 100)\n",
    "y1 = np.clip((x - 0.9) * 10, 0, np.inf)\n",
    "y2 = np.clip(np.exp(2 * x - 2), 10, np.inf)\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.plot(x, np.full(x.shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 4\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [-0.84429837  0.7186429 ]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.34800373 -0.42134618]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -3542.3399623641044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17711.699811820523"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33232068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0 landmark 1\n",
      "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.01\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 0.99\n",
      "reward 0.0 reshaped 0.97\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.8\n",
      "reward 0.0 reshaped 0.74\n",
      "reward 0.0 reshaped 0.69\n",
      "reward 0.0 reshaped 0.63\n",
      "reward 0.0 reshaped 0.6\n",
      "reward 0.0 reshaped 0.56\n",
      "reward 0.0 reshaped 0.56\n",
      "reward 0.0 reshaped 0.57\n",
      "reward 0.0 reshaped 0.6\n",
      "reward 0.0 reshaped 0.64\n",
      "reward 0.0 reshaped 0.69\n",
      "reward 0.0 reshaped 0.74\n",
      "reward 0.0 reshaped 0.79\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 1.03\n",
      "reward 0.0 reshaped 1.07\n",
      "reward 0.0 reshaped 1.1\n",
      "reward 0.0 reshaped 1.12\n",
      "reward 10.0 reshaped 10.16\n",
      "reward 10.0 reshaped 10.15\n",
      "reward 0.0 reshaped 1.06\n",
      "reward 0.0 reshaped 1.03\n",
      "reward 0.0 reshaped 1.01\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.95\n",
      "reward 0.0 reshaped 0.97\n",
      "reward 0.0 reshaped 0.99\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.05\n",
      "reward 0.0 reshaped 1.08\n",
      "reward 0.0 reshaped 1.11\n",
      "reward 0.0 reshaped 1.09\n",
      "reward 0.0 reshaped 1.08\n",
      "reward 0.0 reshaped 1.06\n",
      "reward 0.0 reshaped 1.05\n",
      "reward 0.0 reshaped 1.05\n",
      "reward 0.0 reshaped 1.04\n",
      "reward 0.0 reshaped 1.08\n",
      "reward 0.0 reshaped 1.11\n",
      "reward 0.0 reshaped 1.09\n",
      "reward 0.0 reshaped 1.04\n",
      "reward 0.0 reshaped 0.99\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.99\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.06\n",
      "reward 0.0 reshaped 1.13\n",
      "reward 10.0 reshaped 10.22\n",
      "reward 0.0 reshaped 1.15\n",
      "reward 0.0 reshaped 1.12\n",
      "reward 0.0 reshaped 1.1\n",
      "reward 0.0 reshaped 1.13\n",
      "reward 0.0 reshaped 1.18\n",
      "reward 0.0 reshaped 1.26\n",
      "reward 0.0 reshaped 1.31\n",
      "reward 10.0 reshaped 10.4\n",
      "reward 10.0 reshaped 10.4\n",
      "reward 10.0 reshaped 10.42\n",
      "reward 0.0 reshaped 1.33\n",
      "reward 0.0 reshaped 1.33\n",
      "reward 0.0 reshaped 1.33\n",
      "reward 0.0 reshaped 1.33\n",
      "reward 0.0 reshaped 1.35\n",
      "reward 0.0 reshaped 1.36\n",
      "reward 0.0 reshaped 1.37\n",
      "reward 0.0 reshaped 1.38\n",
      "reward 0.0 reshaped 1.38\n",
      "reward 0.0 reshaped 1.37\n",
      "reward 0.0 reshaped 1.37\n",
      "reward 0.0 reshaped 1.38\n",
      "reward 0.0 reshaped 1.39\n",
      "reward 0.0 reshaped 1.4\n",
      "reward 0.0 reshaped 1.39\n",
      "reward 0.0 reshaped 1.38\n",
      "reward 0.0 reshaped 1.37\n",
      "reward 0.0 reshaped 1.35\n",
      "reward 0.0 reshaped 1.33\n",
      "reward 0.0 reshaped 1.3\n",
      "reward 0.0 reshaped 1.27\n",
      "reward 10.0 reshaped 10.28\n",
      "reward 0.0 reshaped 1.22\n",
      "reward 0.0 reshaped 1.2\n",
      "reward 0.0 reshaped 1.19\n",
      "reward 0.0 reshaped 1.18\n",
      "reward 0.0 reshaped 1.19\n",
      "reward 0.0 reshaped 1.2\n",
      "reward 0.0 reshaped 1.19\n",
      "reward 0.0 reshaped 1.2\n",
      "reward 0.0 reshaped 1.19\n",
      "reward 0.0 reshaped 1.17\n",
      "reward 0.0 reshaped 1.15\n",
      "reward 0.0 reshaped 1.12\n",
      "reward 0.0 reshaped 1.07\n",
      "reward 0.0 reshaped 1.05\n",
      "reward 0.0 reshaped 1.04\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 1.01\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 0.98\n",
      "reward 0.0 reshaped 0.95\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 0.98\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.01\n",
      "reward 0.0 reshaped 1.01\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 0.98\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.99\n",
      "reward 0.0 reshaped 1.0\n",
      "reward 0.0 reshaped 0.98\n",
      "reward 0.0 reshaped 0.95\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.8\n",
      "reward 0.0 reshaped 0.79\n",
      "reward 0.0 reshaped 0.78\n",
      "reward 0.0 reshaped 0.79\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.8\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.96\n",
      "reward 0.0 reshaped 1.02\n",
      "reward 0.0 reshaped 1.06\n",
      "reward 0.0 reshaped 1.08\n",
      "reward 0.0 reshaped 1.1\n",
      "reward 0.0 reshaped 1.12\n",
      "reward 0.0 reshaped 1.13\n",
      "reward 0.0 reshaped 1.13\n",
      "reward 0.0 reshaped 1.11\n",
      "reward 0.0 reshaped 1.12\n",
      "reward 0.0 reshaped 1.13\n",
      "reward 0.0 reshaped 1.14\n",
      "reward 0.0 reshaped 1.16\n",
      "reward 0.0 reshaped 1.19\n",
      "reward 0.0 reshaped 1.2\n",
      "reward 0.0 reshaped 1.21\n",
      "reward 0.0 reshaped 1.21\n",
      "episode ran for 1805 steps\n",
      "agent_rewards -1016.3580012418696\n",
      "adversary_rewards 240.0\n"
     ]
    }
   ],
   "source": [
    "# Demo environment with hardcoded policies, 3 agents of each class, 2 landmarks\n",
    "eps = 0.3\n",
    "\n",
    "def hardcode_policy_1(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def hardcode_policy_2(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            # get agent_0's\n",
    "            x, y = observation[12:14]\n",
    "            if x < -eps: # go left\n",
    "                return 1\n",
    "            elif x > eps: # go right\n",
    "                return 2\n",
    "            elif y < -eps: # go down\n",
    "                return 3\n",
    "            elif y > eps: # go up\n",
    "                return 4\n",
    "            else:\n",
    "                return random.randint(0, 4)\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 0\n",
    "            # return random.randint(0, 4)\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "agent_rewards = 0\n",
    "reshaped_agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "reshaped_adversary_rewards = 0\n",
    "rewardshaper = RewardsShaper(env)\n",
    "normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    reshaped_reward = rewardshaper(agent_name, observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_2(observation, agent_name)\n",
    "        env.step(action)\n",
    "        \n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "        reshaped_adversary_rewards += reshaped_reward\n",
    "    elif \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "        reshaped_agent_rewards += reshaped_reward\n",
    "\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "#         print(\"obs[agent_0 pos]\", np.round(observation[12:14], 2))\n",
    "#         print(\"obs[agent_0 pos]\", np.round(np.linalg.norm(observation[12:14]), 2))\n",
    "        print(\"reward\", reward, \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a696c1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs[ovels] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.]\n",
      "obs[ovels] [-1.15  0.02  0.    0.    0.    0.  ] [-0.89  0.02  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.97 -0.36  0.    0.    0.    0.  ] [-0.75 -0.28  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.73 -0.67  0.    0.    0.    0.  ] [-0.56 -0.52  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.55 -0.9   0.    0.    0.    0.  ] [-0.42 -0.7   0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.41 -1.08  0.    0.    0.    0.  ] [-0.32 -0.83  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.31 -1.21  0.    0.    0.    0.  ] [-0.24 -0.93  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.23 -1.28  0.    0.    0.    0.  ] [-0.17 -0.98  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.16 -1.29  0.    0.    0.    0.  ] [-0.12 -0.99  0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.11 -1.29  0.    0.    0.    0.  ] [-0.09 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.08 -1.3   0.    0.    0.    0.  ] [-0.06 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.06 -1.3   0.    0.    0.    0.  ] [-0.04 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.04 -1.3   0.    0.    0.    0.  ] [-0.03 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.03 -1.3   0.    0.    0.    0.  ] [-0.02 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.02 -1.3   0.    0.    0.    0.  ] [-0.02 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.01 -1.3   0.    0.    0.    0.  ] [-0.01 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.01 -1.3   0.    0.    0.    0.  ] [-0.01 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.01 -1.3   0.    0.    0.    0.  ] [-0.01 -1.    0.    0.    0.    0.  ]\n",
      "obs[ovels] [-0.01 -1.3   0.    0.    0.    0.  ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "obs[ovels] [-0.  -1.3  0.   0.   0.   0. ] [-0. -1.  0.  0.  0.  0.]\n",
      "episode ran for 605 steps\n",
      "agent_rewards -914.9510476125197\n",
      "adversary_rewards 0.0\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_1(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_1\":\n",
    "            return 1\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 3\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "normalize = Normalizer(env)\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_1(observation, agent_name)\n",
    "        env.step(action)\n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "    if \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "    \n",
    "    observation = np.round(observation, 2)\n",
    "    norm_obs = np.round(norm_obs, 2)\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", observation[8:10], norm_obs[8:10])\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2), np.round(norm_obs[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_1\":\n",
    "#         print(observation.shape)\n",
    "        print(\"obs[ovels]\", observation[18:], norm_obs[18:])\n",
    "        pass\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3cd7b",
   "metadata": {},
   "source": [
    "### Test one agent of each class, 1 landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20653c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f74e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=1,\n",
    "    num_adversaries=1,\n",
    "    num_obstacles=1,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21bf611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landmark 0\n",
      "adversary_0 agent_0\n",
      "reward 0.0 reshaped 0.77\n",
      "reward 0.0 reshaped 0.78\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.94\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 0.0 reshaped 0.93\n",
      "reward 10.0 reshaped 10.0\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.92\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.79\n",
      "reward 0.0 reshaped 0.78\n",
      "reward 0.0 reshaped 0.79\n",
      "reward 0.0 reshaped 0.8\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.91\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.9\n",
      "reward 0.0 reshaped 0.89\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.81\n",
      "reward 0.0 reshaped 0.82\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.84\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.83\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.85\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.88\n",
      "reward 0.0 reshaped 0.87\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.86\n",
      "reward 0.0 reshaped 0.85\n",
      "episode ran for 601 steps\n",
      "agent_rewards -552.054051859929\n",
      "adversary_rewards 100.0\n"
     ]
    }
   ],
   "source": [
    "# Demo environment with hardcoded policies, 3 agents of each class, 2 landmarks\n",
    "eps = 0.3\n",
    "\n",
    "def hardcode_policy_2(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            # get agent_0's\n",
    "            x, y = observation[6:8]\n",
    "            if x < -eps: # go left\n",
    "                return 1\n",
    "            elif x > eps: # go right\n",
    "                return 2\n",
    "            elif y < -eps: # go down\n",
    "                return 3\n",
    "            elif y > eps: # go up\n",
    "                return 4\n",
    "            else:\n",
    "                return random.randint(0, 4)\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 0\n",
    "            # return random.randint(0, 4)\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "print(*[landmark.name for landmark in env.world.landmarks])\n",
    "print(*[agent.name for agent in env.world.agents])\n",
    "agent_rewards = 0\n",
    "reshaped_agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "reshaped_adversary_rewards = 0\n",
    "rewardshaper = RewardsShaper(env)\n",
    "normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    reshaped_reward = rewardshaper(agent_name, observation)\n",
    "#     norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_2(observation, agent_name)\n",
    "        env.step(action)\n",
    "        \n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "        reshaped_adversary_rewards += reshaped_reward\n",
    "    elif \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "        reshaped_agent_rewards += reshaped_reward\n",
    "\n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[6:8], 2))\n",
    "#         print(\"obs[landmark2]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_0 pos]\", np.round(observation[8:10], 2))\n",
    "#         print(\"obs[adversary_1 pos]\", np.round(observation[10:12], 2))\n",
    "#         print(\"obs[agents]\", np.round(observation[10:20], 2))\n",
    "#         print(\"reward\", np.round(reward, 2), \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "#         print(\"obs[agent_0 pos]\", np.round(observation[12:14], 2))\n",
    "#         print(\"obs[agent_0 pos]\", np.round(np.linalg.norm(observation[12:14]), 2))\n",
    "        print(\"reward\", reward, \"reshaped\", np.round(reshaped_reward, 2))\n",
    "        pass\n",
    "    \n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d52e8e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs[ovels] [0. 0.] obs[ovels] [0. 0.]\n",
      "obs[ovels] [-0.  -0.4] obs[ovels] [-0.   -0.31]\n",
      "obs[ovels] [-0.  -0.7] obs[ovels] [-0.   -0.54]\n",
      "obs[ovels] [-0.   -0.92] obs[ovels] [-0.   -0.71]\n",
      "obs[ovels] [-0.   -1.09] obs[ovels] [-0.   -0.84]\n",
      "obs[ovels] [-0.   -1.22] obs[ovels] [-0.   -0.94]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "obs[ovels] [-0.  -1.3] obs[ovels] [-0. -1.]\n",
      "episode ran for 601 steps\n",
      "agent_rewards -2932.931589757907\n",
      "adversary_rewards 0.0\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_3(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            pass\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 3\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "normalize = Normalizer(env)\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    # env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_3(observation, agent_name)\n",
    "        env.step(action)\n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "    if \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "    \n",
    "    if agent_name == \"agent_0\":\n",
    "        # printout assumes 2 landmarks, 3 of each agent class\n",
    "#         print(\"obs\", np.round(observation, 2))\n",
    "#         print(\"obs[vel]\", np.round(observation[:2], 2), \"obs[vel]\", np.round(norm_obs[:2], 2))\n",
    "#         print(\"obs[pos]\", np.round(observation[2:4], 2), \"obs[pos]\", np.round(norm_obs[2:4], 2))\n",
    "#         print(\"obs[landmark1]\", np.round(observation[4:6], 2), \"obs[landmark1]\", np.round(norm_obs[4:6], 2))\n",
    "#         print(\"obs[oagents]\", np.round(observation[6:8], 2), \"obs[oagents]\", np.round(norm_obs[6:8], 2))\n",
    "#         print(\"obs[ovels]\", np.round(observation[8:], 2), \"obs[ovels]\", np.round(norm_obs[8:], 2))\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "        print(\"obs[ovels]\", np.round(observation[8:], 2), \"obs[ovels]\", np.round(norm_obs[8:], 2))\n",
    "        pass\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef16af8",
   "metadata": {},
   "source": [
    "### Visualize physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85c89871",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919cadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=1,\n",
    "    num_adversaries=1,\n",
    "    num_obstacles=0,\n",
    "    max_cycles=50,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82887822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 1 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 2 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 3 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 4 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 5 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 6 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 7 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 8 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 9 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 10 obs[pos] [-0.66 -0.9 ] obs[vel] [0. 0.]\n",
      "step 11 obs[pos] [-0.63 -0.9 ] obs[vel] [0.3 0. ]\n",
      "step 12 obs[pos] [-0.57 -0.9 ] obs[vel] [0.52 0.  ]\n",
      "step 13 obs[pos] [-0.5 -0.9] obs[vel] [0.69 0.  ]\n",
      "step 14 obs[pos] [-0.42 -0.9 ] obs[vel] [0.82 0.  ]\n",
      "step 15 obs[pos] [-0.39 -0.9 ] obs[vel] [0.32 0.  ]\n",
      "step 16 obs[pos] [-0.4 -0.9] obs[vel] [-0.06  0.  ]\n",
      "step 17 obs[pos] [-0.43 -0.9 ] obs[vel] [-0.35  0.  ]\n",
      "step 18 obs[pos] [-0.46 -0.9 ] obs[vel] [-0.26  0.  ]\n",
      "step 19 obs[pos] [-0.48 -0.9 ] obs[vel] [-0.2  0. ]\n",
      "step 20 obs[pos] [-0.49 -0.9 ] obs[vel] [-0.15  0.  ]\n",
      "step 21 obs[pos] [-0.5 -0.9] obs[vel] [-0.11  0.  ]\n",
      "step 22 obs[pos] [-0.51 -0.9 ] obs[vel] [-0.08  0.  ]\n",
      "step 23 obs[pos] [-0.52 -0.9 ] obs[vel] [-0.06  0.  ]\n",
      "step 24 obs[pos] [-0.52 -0.9 ] obs[vel] [-0.05  0.  ]\n",
      "step 25 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.03  0.  ]\n",
      "step 26 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.03  0.  ]\n",
      "step 27 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.02  0.  ]\n",
      "step 28 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 29 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 30 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 31 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.01  0.  ]\n",
      "step 32 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 33 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 34 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 35 obs[pos] [-0.53 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 36 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 37 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 38 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 39 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 40 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 41 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 42 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 43 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 44 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 45 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 46 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 47 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 48 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 49 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n",
      "step 50 obs[pos] [-0.54 -0.9 ] obs[vel] [-0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy_4(step_idx, observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    \n",
    "    if agent_name == \"adversary_0\":\n",
    "        if step_idx == 10 or step_idx == 11 or step_idx == 12 or step_idx == 13:\n",
    "            return 2\n",
    "        elif step_idx == 14 or step_idx == 15 or step_idx == 16:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "env.reset()\n",
    "# normalize = Normalizer(env)\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    step_idx = agent_step_idx // 2\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "#     norm_obs = normalize(observation)\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_4(step_idx, observation, agent_name)\n",
    "        env.step(action)\n",
    "    \n",
    "    observation = np.round(observation, 2)\n",
    "    if agent_name == \"agent_0\":\n",
    "        pass\n",
    "    elif agent_name == \"adversary_0\":\n",
    "#         print(observation.shape)\n",
    "        print(\"step\", step_idx, \"obs[pos]\", observation[2:4], \"obs[vel]\", observation[0:2])\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80ed4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af6f10",
   "metadata": {},
   "source": [
    "## Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c06c406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(3), tensor(2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,3,2,0])\n",
    "torch.argmax(a).item(), torch.max(a), a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33d9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 "
     ]
    }
   ],
   "source": [
    "d = {1: 'a', 2: 'b', 3: 'c'}\n",
    "for i in d:\n",
    "    print(i , end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779f4097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = torch.tensor(2, device=device)\n",
    "b = torch.tensor(3)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53211f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 9, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(6)\n",
    "a = torch.tensor([9, 8])\n",
    "\n",
    "idx = 4\n",
    "\n",
    "torch.hstack((v[:idx], a, v[idx + 2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701179fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0,1,2])\n",
    "w.device\n",
    "w.to(device)\n",
    "w.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859c349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
