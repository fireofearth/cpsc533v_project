{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c086ab",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Baseline agent algorithm with experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "045dec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.version.cuda)\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67c97a",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f4e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=1,\n",
    "    num_adversaries=1,\n",
    "    num_obstacles=0,\n",
    "    max_cycles=100,\n",
    "    continuous_actions=False\n",
    ").unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc69b8",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b9761",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "224a00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f5030b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discount': 0.99,\n",
       " 'epsilon': 0.2,\n",
       " 'n_episodes': 100000,\n",
       " 'batch_size': 1,\n",
       " 'update_target_interval': 10,\n",
       " 'report_interval': 32,\n",
       " 'clip_grad_norm': 1.0,\n",
       " 'lr': 0.001,\n",
       " 'reward_scale': 0.2,\n",
       " 'device': device(type='cuda'),\n",
       " 'common': {'hidden_size': 16, 'n_actions': 5},\n",
       " 'adversary': {'n_agents': 1,\n",
       "  'observation_shape': (8,),\n",
       "  'hidden_size': 16,\n",
       "  'n_actions': 5},\n",
       " 'agent': {'n_agents': 1,\n",
       "  'observation_shape': (6,),\n",
       "  'hidden_size': 16,\n",
       "  'n_actions': 5}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.common.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.2,\n",
    "    n_episodes=100000,\n",
    "    batch_size=1,\n",
    "    update_target_interval=10,\n",
    "    report_interval=32,\n",
    "    clip_grad_norm=1.0,\n",
    "    lr=0.001,\n",
    "    reward_scale=0.2,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    common=AttrDict(\n",
    "        hidden_size=16,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa870d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        self.n_allagents = len(env.world.agents)\n",
    "        self.n_good = sum(map(lambda a: not a.adversary, env.world.agents))\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_abs_pos(s):\n",
    "        \"\"\"Clip absolute position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -1.5, 1.5) / 1.5\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_rel_pos(s):\n",
    "        \"\"\"Clip relative position and scale to [-1, 1]\n",
    "        s is a scalar or an ndarray of one dimension.\"\"\"\n",
    "        return np.clip(s, -3, 3) / 3\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # normalize and clip positions\n",
    "        norm_obs = obs.copy()\n",
    "        # normalize velocity of current entity\n",
    "        norm_obs[:2] = norm_obs[:2] / 1.3\n",
    "        # clip/scale abs. position of current entity\n",
    "        norm_obs[2:4] = self.normalize_abs_pos(norm_obs[2:4])\n",
    "        # clip/scale rel. position of other entities\n",
    "        n_range = self.n_landmarks + self.n_allagents - 1\n",
    "        for i in range(n_range):\n",
    "            norm_obs[4 + (2*i):4 + (2*(i + 1))] = self.normalize_rel_pos(\n",
    "                norm_obs[4 + (2*i):4 + (2*(i + 1))]\n",
    "            )\n",
    "        # normalize velocity of other entities\n",
    "        norm_obs[4 + (2*n_range):] = norm_obs[4 + (2*n_range):] / 1.3\n",
    "        return norm_obs\n",
    "    \n",
    "class RewardsShaper(object):\n",
    "    def __init__(self, env):\n",
    "        self.n_landmarks = len(env.world.landmarks)\n",
    "        # self.n_allagents = len(env.world.agents)\n",
    "        self.name_to_idx = {agent.name: i for i, agent in enumerate(env.world.agents)}\n",
    "        self.idx_to_name = {i: agent.name for i, agent in enumerate(env.world.agents)}\n",
    "        self.goodagent_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"agent\")\n",
    "        ]\n",
    "        self.adversary_indices = [\n",
    "            i for i, agent in enumerate(env.world.agents) if agent.name.startswith(\"adversary\")\n",
    "        ]\n",
    "        # rdist - distance between adversary-good agent to start computing rewards.\n",
    "        self.rdist = 2\n",
    "        # collision_dist - distance between adversary-good agent to count collision.\n",
    "        #    Based on PettingZoo numbers. \n",
    "        self.collision_dist = 0.075 + 0.05\n",
    "\n",
    "    @staticmethod\n",
    "    def bound(x):\n",
    "        if x < 0.9:\n",
    "            return 0\n",
    "        if x < 1.0:\n",
    "            return (x - 0.9) * 10\n",
    "        return min(np.exp(2 * x - 2), 10)\n",
    "        \n",
    "    def __call__(self, agent_name, obs):\n",
    "        \"\"\"Compute reshaped rewards from observation for agent given agent name.\n",
    "        Adversary: start gaining small rewards as it nears good agents.\n",
    "        \n",
    "        Good agent: starts gaining small penality as it nears bad agents.\n",
    "        \"\"\"\n",
    "        _obs = obs[4 + (2*self.n_landmarks):]\n",
    "        agent_idx = self.name_to_idx[agent_name]\n",
    "        cum_r = 0.\n",
    "        if agent_name.startswith(\"agent\"):\n",
    "            # penalty across all adversaries\n",
    "            for adversary_idx in self.adversary_indices:\n",
    "                # penalty from distance of adversary; penalty of collision\n",
    "                other_idx = adversary_idx - 1 if agent_idx < adversary_idx else adversary_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d < self.collision_dist:\n",
    "                    cum_r -= 10\n",
    "                else:\n",
    "                    cum_r -= min(max(1 -  (1/self.rdist)*d, 0), 1)\n",
    "                \n",
    "            # penalty from boudary based on PettingZoo\n",
    "            pos = obs[2:4]\n",
    "            cum_r -= self.bound(abs(pos[0]))\n",
    "            cum_r -= self.bound(abs(pos[1]))\n",
    "        \n",
    "        elif agent_name.startswith(\"adversary\"):\n",
    "            # reward across all agents\n",
    "            for goodagent_idx in self.goodagent_indices:\n",
    "                # reward from distance to agent; reward of collision\n",
    "                other_idx = goodagent_idx - 1 if agent_idx < goodagent_idx else goodagent_idx\n",
    "                x, y = _obs[2*other_idx:(2*other_idx) + 2]\n",
    "                d    = math.sqrt(x**2 + y**2)\n",
    "                if d < self.collision_dist:\n",
    "                    cum_r += 10\n",
    "                else:\n",
    "                    cum_r += min(max(1 -  (1/self.rdist)*d, 0), 1)\n",
    "        \n",
    "        return cum_r\n",
    "\n",
    "normalize = Normalizer(env) # norm_obs = normalize(obs)\n",
    "shapereward = RewardsShaper(env) # reward = shapereward(agent_name, obs)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515aa458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.device      = config.device\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.n_actions   = config[agent_type].n_actions\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.output_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.observation_size, self.hidden_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(self.hidden_size, self.hidden_size * 2),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(self.hidden_size, self.n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, observation):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        \"\"\"\n",
    "        observation = torch.tensor(observation, dtype=torch.float, device=self.device)\n",
    "        Q = self.output_mlp(observation)\n",
    "        return Q\n",
    "\n",
    "def choose_action(config, agent_type, Q, is_val=False):\n",
    "    if not is_val and random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6188397c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_episode(config, adversary_net, should_render=False, is_val=False):\n",
    "    \"\"\"Run one episodes.\n",
    "    \n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    AttrDict\n",
    "        Contains episode metrics:\n",
    "        - steps : number of steps. All agents take an action at each step.\n",
    "        - reward : episodic rewards indexed by ('adversary', 'agent').\n",
    "        - step_records : list of quantities produced indiced by step, ('adversary', 'agent'), agent index.\n",
    "          Each step record has:\n",
    "            + observation\n",
    "            + Q\n",
    "            + reward\n",
    "            + done\n",
    "        - loss : contains episodic losses indexed by ('adversary', 'agent'). To be updated by train_agents()\n",
    "    \"\"\"\n",
    "    episode = AttrDict(\n",
    "        steps=0,\n",
    "        reward=AttrDict(adversary=0, agent=0),\n",
    "        step_records=[],\n",
    "        loss=AttrDict(adversary=0, agent=0)\n",
    "    )\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        \n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "\n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        reward = shapereward(agent_name, obs)\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "        \n",
    "        if agent_type == \"agent\":\n",
    "            env.step(0)\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                action=0,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "        else:\n",
    "            # agent_type == \"adversary\"\n",
    "            Q_curr = adversary_net(obs_curr)\n",
    "            action = choose_action(config, agent_type, Q_curr, is_val=is_val)\n",
    "            env.step(action)\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                # inputs to network\n",
    "                observation=obs_curr,\n",
    "                # outputs of network / inputs to environment\n",
    "                action=action,\n",
    "                # output of environment\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "        episode.reward[agent_type] += reward\n",
    "\n",
    "    return episode\n",
    "\n",
    "def train_agents(config, batch, adversary_net, adversary_target_net, adversary_optimizer):\n",
    "    \"\"\"Compute loss of episode and update agent\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float, device=device)\n",
    "    for episode in batch:\n",
    "        for step_idx in reversed(range(episode.steps)):\n",
    "            for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "                curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "                if curr_record.done:\n",
    "                    # agent is done at this step\n",
    "                    continue\n",
    "                    \n",
    "                next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "                r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "                y = None\n",
    "                if next_record.done:\n",
    "                    # agent terminates at next step\n",
    "                    y = r\n",
    "                else:\n",
    "                next_o = next_record.observation\n",
    "#                 print(next_o)\n",
    "#                 break\n",
    "                target_Q = adversary_target_net(next_o)\n",
    "                max_target_Q = torch.max(target_Q)\n",
    "                y = r + (1-next_record.done)*discount*max_target_Q\n",
    "#                 y = r + (1-next_record.done)*discount*max_target_Q\n",
    "\n",
    "                u = curr_record.action\n",
    "                Q_u = curr_record.Q[u]\n",
    "#                 adversary_loss = torch.pow(y - Q_u, 2.)\n",
    "#                 distance_loss = loss_fn(adversary_pos, agent_pos)\n",
    "#                 adversary_loss = loss_fn(y, Q_u) - 2.*(torch.norm(after_rel_distance) < torch.norm(initial_rel_distance))\n",
    "                adversary_loss = loss_fn(y, Q_u)\n",
    "#                 print(distance_loss)\n",
    "        \n",
    "                loss = adversary_loss\n",
    "#                 loss = distance_loss\n",
    "\n",
    "                adversary_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(adversary_net.parameters(), config.clip_grad_norm)\n",
    "                adversary_optimizer.step()\n",
    "                episode.loss.adversary += loss.item()\n",
    "                # print('here')\n",
    "                # del adversary_loss\n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    print(\"Training the agents...\")\n",
    "    os.makedirs(\"models/batched-baseline-test\", exist_ok=True)\n",
    "    t0 = time.time()\n",
    "    device = config.device\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    adversary_target_net.eval()\n",
    "    print(\"Created the agent nets.\")\n",
    "    adversary_optimizer = torch.optim.SGD(adversary_net.parameters(), lr=config.lr)\n",
    "    logger = AttrDict(\n",
    "        episodic_losses=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_rewards=AttrDict(adversary=[], agent=[])\n",
    "    )\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "    print(\"Initial update of target nets\")\n",
    "    update_targets()\n",
    "    \n",
    "    batch = []\n",
    "    print(\"Beginning the episodes...\")\n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        # Run an episode\n",
    "        episode = run_episode(config, adversary_net,\n",
    "                              should_render=episode_idx % 32 == 0 and episode_idx > 1)\n",
    "        batch.append(episode)\n",
    "        \n",
    "        # Train on the episode\n",
    "        if episode_idx % config.batch_size == 0 and episode_idx > 0:\n",
    "            train_agents(config, batch, adversary_net,\n",
    "                         adversary_target_net,\n",
    "                         adversary_optimizer)\n",
    "            batch = []\n",
    "        \n",
    "        # Logging the reward and los\n",
    "        logger.episodic_losses.adversary.append(episode.loss.adversary)\n",
    "        logger.episodic_losses.agent.append(episode.loss.agent)\n",
    "        logger.episodic_rewards.adversary.append(episode.reward.adversary)\n",
    "        logger.episodic_rewards.agent.append(episode.reward.agent)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            print('updating')\n",
    "            # Update double network\n",
    "            update_targets()\n",
    "\n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            # Logging\n",
    "            t1 = time.time()\n",
    "            tdelta = TimeDelta(round(t1 - t0, 0))\n",
    "            print(f\"on episode {episode_idx} (time taken so far: {tdelta})\")\n",
    "            mean_loss_adversary = statistics.fmean(logger.episodic_losses.adversary[-config.report_interval:])\n",
    "            mean_reward_adversary = statistics.fmean(logger.episodic_rewards.adversary[-config.report_interval:])\n",
    "            mean_reward_agent = statistics.fmean(logger.episodic_rewards.agent[-config.report_interval:])\n",
    "            print(f\"     mean loss: adversary {mean_loss_adversary}\")\n",
    "            print(f\"     mean reward: adversary {mean_reward_adversary}, agent {mean_reward_agent}\")\n",
    "\n",
    "        if episode_idx % 1000 == 0 and episode_idx > 0:\n",
    "            print('saving')\n",
    "            torch.save(\n",
    "                adversary_net.state_dict(),\n",
    "                f\"models/batched-baseline-test/adversary-net-{episode_idx}.pth\"\n",
    "            )\n",
    "    \n",
    "    return adversary_net, logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dc99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8966eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agents...\n",
      "Created the agent nets.\n",
      "Initial update of target nets\n",
      "Beginning the episodes...\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 32 (time taken so far: 0-00:00:5.0)\n",
      "     mean loss: adversary 0.35503706675294333\n",
      "     mean reward: adversary 0.125, agent -0.345558202271273\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 64 (time taken so far: 0-00:00:7.0)\n",
      "     mean loss: adversary 0.08816819349732544\n",
      "     mean reward: adversary 0.0, agent -0.16194511640247625\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 96 (time taken so far: 0-00:00:9.0)\n",
      "     mean loss: adversary 0.07033352024338607\n",
      "     mean reward: adversary 0.0, agent -0.22132357117150028\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 128 (time taken so far: 0-00:00:10.0)\n",
      "     mean loss: adversary 0.09101418396481989\n",
      "     mean reward: adversary 0.0, agent -0.08045136824022603\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 160 (time taken so far: 0-00:00:12.0)\n",
      "     mean loss: adversary 0.2021942473111096\n",
      "     mean reward: adversary 0.0, agent -0.23794357258955953\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 192 (time taken so far: 0-00:00:14.0)\n",
      "     mean loss: adversary 0.07018268651018814\n",
      "     mean reward: adversary 0.0, agent -0.1560752868670428\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 224 (time taken so far: 0-00:00:16.0)\n",
      "     mean loss: adversary 0.06728163261697967\n",
      "     mean reward: adversary 0.0, agent -0.12351711063688639\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 256 (time taken so far: 0-00:00:18.0)\n",
      "     mean loss: adversary 0.04658864144745217\n",
      "     mean reward: adversary 0.0, agent -0.14797081290834324\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 288 (time taken so far: 0-00:00:20.0)\n",
      "     mean loss: adversary 0.0907209338100566\n",
      "     mean reward: adversary 0.0, agent -0.13733458930305678\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 320 (time taken so far: 0-00:00:21.0)\n",
      "     mean loss: adversary 0.0883919451394439\n",
      "     mean reward: adversary 0.0, agent -0.33017083512653056\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 352 (time taken so far: 0-00:00:23.0)\n",
      "     mean loss: adversary 0.0861806929756596\n",
      "     mean reward: adversary 0.0, agent -0.09127660027658263\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 384 (time taken so far: 0-00:00:25.0)\n",
      "     mean loss: adversary 0.067262757965032\n",
      "     mean reward: adversary 0.0, agent -0.14370407711581512\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 416 (time taken so far: 0-00:00:27.0)\n",
      "     mean loss: adversary 0.07815269983318096\n",
      "     mean reward: adversary 0.0, agent -0.2746714441841742\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 448 (time taken so far: 0-00:00:29.0)\n",
      "     mean loss: adversary 0.07100757210753972\n",
      "     mean reward: adversary 0.0, agent -0.08712942841902846\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 480 (time taken so far: 0-00:00:30.0)\n",
      "     mean loss: adversary 0.0482394390809468\n",
      "     mean reward: adversary 0.0, agent -0.14584124734582707\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 512 (time taken so far: 0-00:00:32.0)\n",
      "     mean loss: adversary 0.08445751499828592\n",
      "     mean reward: adversary 0.0, agent -0.094553912613284\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 544 (time taken so far: 0-00:00:34.0)\n",
      "     mean loss: adversary 0.053244743830574544\n",
      "     mean reward: adversary 0.0, agent -0.2263891471511111\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 576 (time taken so far: 0-00:00:36.0)\n",
      "     mean loss: adversary 0.06276311196000164\n",
      "     mean reward: adversary 0.0, agent -0.14674518545725823\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 608 (time taken so far: 0-00:00:38.0)\n",
      "     mean loss: adversary 0.09063936948723966\n",
      "     mean reward: adversary 0.0, agent -0.21108669098963753\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 640 (time taken so far: 0-00:00:39.0)\n",
      "     mean loss: adversary 0.3431134903316208\n",
      "     mean reward: adversary 0.0625, agent -0.379046564215879\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 672 (time taken so far: 0-00:00:41.0)\n",
      "     mean loss: adversary 0.09703654608473938\n",
      "     mean reward: adversary 0.0, agent -0.2689622518182248\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 704 (time taken so far: 0-00:00:43.0)\n",
      "     mean loss: adversary 0.0640629800618562\n",
      "     mean reward: adversary 0.0, agent -0.17207757211063532\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 736 (time taken so far: 0-00:00:45.0)\n",
      "     mean loss: adversary 0.05321016305184941\n",
      "     mean reward: adversary 0.0, agent -0.02229336325217946\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 768 (time taken so far: 0-00:00:47.0)\n",
      "     mean loss: adversary 0.0870256508291612\n",
      "     mean reward: adversary 0.0, agent -0.1286417657875649\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 800 (time taken so far: 0-00:00:48.0)\n",
      "     mean loss: adversary 0.6022692816707155\n",
      "     mean reward: adversary 0.25, agent -0.48112404799581554\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 832 (time taken so far: 0-00:00:50.0)\n",
      "     mean loss: adversary 0.18858903667547525\n",
      "     mean reward: adversary 0.0625, agent -0.4184368606976791\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 864 (time taken so far: 0-00:00:52.0)\n",
      "     mean loss: adversary 0.2100101839305327\n",
      "     mean reward: adversary 0.0625, agent -0.43764091608560796\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 896 (time taken so far: 0-00:00:54.0)\n",
      "     mean loss: adversary 0.09572128732857726\n",
      "     mean reward: adversary 0.0, agent -0.07599258542402204\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 928 (time taken so far: 0-00:00:55.0)\n",
      "     mean loss: adversary 0.08022098818356291\n",
      "     mean reward: adversary 0.0, agent -0.09960683162330813\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 960 (time taken so far: 0-00:00:57.0)\n",
      "     mean loss: adversary 0.590808391060353\n",
      "     mean reward: adversary 0.25, agent -0.33175071527024647\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 992 (time taken so far: 0-00:00:59.0)\n",
      "     mean loss: adversary 0.5753546865203345\n",
      "     mean reward: adversary 0.25, agent -0.368468215575239\n",
      "updating\n",
      "saving\n",
      "updating\n",
      "updating\n",
      "on episode 1024 (time taken so far: 0-00:01:1.0)\n",
      "     mean loss: adversary 0.08524423525528439\n",
      "     mean reward: adversary 0.0, agent -0.32299843102533876\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1056 (time taken so far: 0-00:01:3.0)\n",
      "     mean loss: adversary 0.2024806372798971\n",
      "     mean reward: adversary 0.0625, agent -0.20734660852249057\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1088 (time taken so far: 0-00:01:4.0)\n",
      "     mean loss: adversary 0.056837291697430806\n",
      "     mean reward: adversary 0.0, agent -0.16564370701241243\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1120 (time taken so far: 0-00:01:6.0)\n",
      "     mean loss: adversary 0.10460617160083252\n",
      "     mean reward: adversary 0.0, agent -0.04926121962817324\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1152 (time taken so far: 0-00:01:8.0)\n",
      "     mean loss: adversary 0.0795526716018074\n",
      "     mean reward: adversary 0.0, agent -0.35184148045801816\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1184 (time taken so far: 0-00:01:10.0)\n",
      "     mean loss: adversary 0.32339992846211546\n",
      "     mean reward: adversary 0.125, agent -0.27615643595565675\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1216 (time taken so far: 0-00:01:12.0)\n",
      "     mean loss: adversary 0.06666171171492333\n",
      "     mean reward: adversary 0.0, agent -0.14623835983801683\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1248 (time taken so far: 0-00:01:13.0)\n",
      "     mean loss: adversary 0.06975127374990528\n",
      "     mean reward: adversary 0.0, agent -0.20681309981270177\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1280 (time taken so far: 0-00:01:15.0)\n",
      "     mean loss: adversary 0.07774756719249115\n",
      "     mean reward: adversary 0.0, agent -0.1602542239482913\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1312 (time taken so far: 0-00:01:17.0)\n",
      "     mean loss: adversary 0.4666902117638552\n",
      "     mean reward: adversary 0.1875, agent -0.3813036868663227\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1344 (time taken so far: 0-00:01:19.0)\n",
      "     mean loss: adversary 0.3309285000798302\n",
      "     mean reward: adversary 0.125, agent -0.4110548806170642\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1376 (time taken so far: 0-00:01:20.0)\n",
      "     mean loss: adversary 0.06733273431001038\n",
      "     mean reward: adversary 0.0, agent -0.09916491053699467\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1408 (time taken so far: 0-00:01:22.0)\n",
      "     mean loss: adversary 0.06086083330436537\n",
      "     mean reward: adversary 0.0, agent -0.12248887736551453\n",
      "updating\n",
      "updating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating\n",
      "updating\n",
      "on episode 1440 (time taken so far: 0-00:01:24.0)\n",
      "     mean loss: adversary 0.3252071136571662\n",
      "     mean reward: adversary 0.125, agent -0.4069540695104835\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1472 (time taken so far: 0-00:01:26.0)\n",
      "     mean loss: adversary 0.0724004565661279\n",
      "     mean reward: adversary 0.0, agent -0.13857859154041033\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1504 (time taken so far: 0-00:01:28.0)\n",
      "     mean loss: adversary 0.19272578152021508\n",
      "     mean reward: adversary 0.0625, agent -0.1822138042022353\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1536 (time taken so far: 0-00:01:29.0)\n",
      "     mean loss: adversary 0.34596655412492244\n",
      "     mean reward: adversary 0.125, agent -0.241171650043186\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1568 (time taken so far: 0-00:01:31.0)\n",
      "     mean loss: adversary 0.3439967039643785\n",
      "     mean reward: adversary 0.125, agent -0.37214508694397386\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1600 (time taken so far: 0-00:01:33.0)\n",
      "     mean loss: adversary 0.08371205444544091\n",
      "     mean reward: adversary 0.0, agent -0.1558576523030606\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1632 (time taken so far: 0-00:01:35.0)\n",
      "     mean loss: adversary 0.06195188472572724\n",
      "     mean reward: adversary 0.0, agent -0.16321097078470045\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1664 (time taken so far: 0-00:01:36.0)\n",
      "     mean loss: adversary 0.10595102323173021\n",
      "     mean reward: adversary 0.0, agent -0.06903274700845118\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1696 (time taken so far: 0-00:01:38.0)\n",
      "     mean loss: adversary 0.0670317800868504\n",
      "     mean reward: adversary 0.0, agent -0.17859827582700122\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1728 (time taken so far: 0-00:01:40.0)\n",
      "     mean loss: adversary 0.10054258579806417\n",
      "     mean reward: adversary 0.0, agent -0.18827706228411828\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1760 (time taken so far: 0-00:01:42.0)\n",
      "     mean loss: adversary 0.544036446712684\n",
      "     mean reward: adversary 0.0625, agent -0.20448384982632262\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1792 (time taken so far: 0-00:01:43.0)\n",
      "     mean loss: adversary 0.07323624851046345\n",
      "     mean reward: adversary 0.0, agent -0.13228411407023297\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1824 (time taken so far: 0-00:01:45.0)\n",
      "     mean loss: adversary 0.07288087787878259\n",
      "     mean reward: adversary 0.0, agent -0.1795230924349848\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1856 (time taken so far: 0-00:01:47.0)\n",
      "     mean loss: adversary 0.06913296385479757\n",
      "     mean reward: adversary 0.0, agent -0.087892140271602\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1888 (time taken so far: 0-00:01:49.0)\n",
      "     mean loss: adversary 0.3367507340770021\n",
      "     mean reward: adversary 0.125, agent -0.30945041459801637\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1920 (time taken so far: 0-00:01:51.0)\n",
      "     mean loss: adversary 0.6941373716303549\n",
      "     mean reward: adversary 0.25, agent -0.358754671293785\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1952 (time taken so far: 0-00:01:52.0)\n",
      "     mean loss: adversary 0.05321840920943278\n",
      "     mean reward: adversary 0.0, agent -0.16468613006336108\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 1984 (time taken so far: 0-00:01:54.0)\n",
      "     mean loss: adversary 0.07040548085283181\n",
      "     mean reward: adversary 0.0, agent -0.08944004550119053\n",
      "updating\n",
      "updating\n",
      "saving\n",
      "updating\n",
      "on episode 2016 (time taken so far: 0-00:01:56.0)\n",
      "     mean loss: adversary 0.08573723532407725\n",
      "     mean reward: adversary 0.0, agent -0.07315961192420879\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2048 (time taken so far: 0-00:01:58.0)\n",
      "     mean loss: adversary 0.058517202253159\n",
      "     mean reward: adversary 0.0, agent -0.2734437805051354\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2080 (time taken so far: 0-00:02:0.0)\n",
      "     mean loss: adversary 0.05914008167716381\n",
      "     mean reward: adversary 0.0, agent -0.1239450844280951\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2112 (time taken so far: 0-00:02:1.0)\n",
      "     mean loss: adversary 0.0671533602798286\n",
      "     mean reward: adversary 0.0, agent -0.1184596490755912\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2144 (time taken so far: 0-00:02:3.0)\n",
      "     mean loss: adversary 0.6073964729557586\n",
      "     mean reward: adversary 0.25, agent -0.5440869663361622\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2176 (time taken so far: 0-00:02:5.0)\n",
      "     mean loss: adversary 0.683554973807664\n",
      "     mean reward: adversary 0.25, agent -0.5053976647952796\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2208 (time taken so far: 0-00:02:7.0)\n",
      "     mean loss: adversary 0.05885412891460998\n",
      "     mean reward: adversary 0.0, agent -0.16990856112796224\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2240 (time taken so far: 0-00:02:9.0)\n",
      "     mean loss: adversary 0.06460501877974557\n",
      "     mean reward: adversary 0.0, agent -0.16089636226415377\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2272 (time taken so far: 0-00:02:10.0)\n",
      "     mean loss: adversary 0.06762402801842092\n",
      "     mean reward: adversary 0.0, agent -0.29100736590659104\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2304 (time taken so far: 0-00:02:12.0)\n",
      "     mean loss: adversary 0.18966713424198348\n",
      "     mean reward: adversary 0.0625, agent -0.25205964588244856\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2336 (time taken so far: 0-00:02:14.0)\n",
      "     mean loss: adversary 0.08376387127062891\n",
      "     mean reward: adversary 0.0, agent -0.11457117774548925\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2368 (time taken so far: 0-00:02:16.0)\n",
      "     mean loss: adversary 0.0855438756169733\n",
      "     mean reward: adversary 0.0, agent -0.13272878467073432\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2400 (time taken so far: 0-00:02:18.0)\n",
      "     mean loss: adversary 0.3310978626313588\n",
      "     mean reward: adversary 0.125, agent -0.14615301728691044\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2432 (time taken so far: 0-00:02:20.0)\n",
      "     mean loss: adversary 0.09589944465941204\n",
      "     mean reward: adversary 0.0, agent -0.21769755099464153\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2464 (time taken so far: 0-00:02:22.0)\n",
      "     mean loss: adversary 0.21734115882715577\n",
      "     mean reward: adversary 0.0625, agent -0.3015165772080206\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2496 (time taken so far: 0-00:02:24.0)\n",
      "     mean loss: adversary 0.33762085336317893\n",
      "     mean reward: adversary 0.125, agent -0.430326210500043\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2528 (time taken so far: 0-00:02:26.0)\n",
      "     mean loss: adversary 0.3166201785351982\n",
      "     mean reward: adversary 0.125, agent -0.1792344765653169\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2560 (time taken so far: 0-00:02:28.0)\n",
      "     mean loss: adversary 0.20305074355909397\n",
      "     mean reward: adversary 0.0625, agent -0.20510891141744692\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2592 (time taken so far: 0-00:02:29.0)\n",
      "     mean loss: adversary 0.0772727265649196\n",
      "     mean reward: adversary 0.0, agent -0.24744743380754675\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2624 (time taken so far: 0-00:02:31.0)\n",
      "     mean loss: adversary 0.319793561970132\n",
      "     mean reward: adversary 0.125, agent -0.425450162066459\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2656 (time taken so far: 0-00:02:33.0)\n",
      "     mean loss: adversary 0.07464230828350385\n",
      "     mean reward: adversary 0.0, agent -0.1487063090250905\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2688 (time taken so far: 0-00:02:35.0)\n",
      "     mean loss: adversary 0.08507041745344796\n",
      "     mean reward: adversary 0.0, agent -0.04814865802116295\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2720 (time taken so far: 0-00:02:37.0)\n",
      "     mean loss: adversary 0.18818725886626908\n",
      "     mean reward: adversary 0.0625, agent -0.23388064807816922\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2752 (time taken so far: 0-00:02:38.0)\n",
      "     mean loss: adversary 0.08155088454801623\n",
      "     mean reward: adversary 0.0, agent -0.15088324713604734\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2784 (time taken so far: 0-00:02:40.0)\n",
      "     mean loss: adversary 0.06609510961631637\n",
      "     mean reward: adversary 0.0, agent -0.19269383920984895\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2816 (time taken so far: 0-00:02:42.0)\n",
      "     mean loss: adversary 0.0742025815528212\n",
      "     mean reward: adversary 0.0, agent -0.31287885677810995\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2848 (time taken so far: 0-00:02:44.0)\n",
      "     mean loss: adversary 0.08658877567539859\n",
      "     mean reward: adversary 0.0, agent -0.1625471507154404\n",
      "updating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2880 (time taken so far: 0-00:02:46.0)\n",
      "     mean loss: adversary 0.06471905058078939\n",
      "     mean reward: adversary 0.0, agent -0.052639889776692775\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2912 (time taken so far: 0-00:02:47.0)\n",
      "     mean loss: adversary 0.3534287880342908\n",
      "     mean reward: adversary 0.125, agent -0.4092649071662454\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2944 (time taken so far: 0-00:02:49.0)\n",
      "     mean loss: adversary 0.20861172791671626\n",
      "     mean reward: adversary 0.0625, agent -0.14066655887993282\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 2976 (time taken so far: 0-00:02:51.0)\n",
      "     mean loss: adversary 0.38948340528950826\n",
      "     mean reward: adversary 0.125, agent -0.21321449106837026\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "saving\n",
      "on episode 3008 (time taken so far: 0-00:02:53.0)\n",
      "     mean loss: adversary 0.2054678515706982\n",
      "     mean reward: adversary 0.0625, agent -0.26158780586309555\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3040 (time taken so far: 0-00:02:55.0)\n",
      "     mean loss: adversary 0.07880867759069743\n",
      "     mean reward: adversary 0.0, agent -0.24462235735915877\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3072 (time taken so far: 0-00:02:56.0)\n",
      "     mean loss: adversary 0.0762500237600052\n",
      "     mean reward: adversary 0.0, agent -0.2272368501349507\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3104 (time taken so far: 0-00:02:58.0)\n",
      "     mean loss: adversary 0.0745329490349213\n",
      "     mean reward: adversary 0.0, agent -0.23194199227741186\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3136 (time taken so far: 0-00:03:0.0)\n",
      "     mean loss: adversary 0.09826439890824418\n",
      "     mean reward: adversary 0.0, agent -0.11854165173446016\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3168 (time taken so far: 0-00:03:2.0)\n",
      "     mean loss: adversary 0.07560183365874053\n",
      "     mean reward: adversary 0.0, agent -0.20743040751552513\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3200 (time taken so far: 0-00:03:4.0)\n",
      "     mean loss: adversary 0.35978693338387474\n",
      "     mean reward: adversary 0.125, agent -0.308381940876828\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3232 (time taken so far: 0-00:03:6.0)\n",
      "     mean loss: adversary 0.32627640435563676\n",
      "     mean reward: adversary 0.125, agent -0.28152115960471263\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3264 (time taken so far: 0-00:03:8.0)\n",
      "     mean loss: adversary 0.07354661059743592\n",
      "     mean reward: adversary 0.0, agent -0.09541222559012494\n",
      "updating\n",
      "updating\n",
      "updating\n",
      "on episode 3296 (time taken so far: 0-00:03:10.0)\n",
      "     mean loss: adversary 0.09091411863970746\n",
      "     mean reward: adversary 0.0, agent -0.03176394493787123\n",
      "updating\n",
      "updating\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "adversary_net, logger = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654178e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_net = SimpleTagNet(config, \"adversary\").to(config.device)\n",
    "agent_net = SimpleTagNet(config, \"agent\").to(config.device)\n",
    "adversary_net.load_state_dict(torch.load('./models/batched-baseline-test/adversary-net-9000.pth'))\n",
    "# agent_net.load_state_dict(torch.load('./models/batched-baseline-test/agent-net-9000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].plot(logger.episodic_losses.adversary[50:], label=\"adversary\")\n",
    "axes[0].plot(logger.episodic_losses.agent[50:], label=\"good agent\")\n",
    "axes[0].set_title(\"loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(logger.episodic_rewards.adversary[50:], label=\"adversary\")\n",
    "axes[1].plot(logger.episodic_rewards.agent[50:], label=\"good agent\")\n",
    "axes[1].set_title(\"reward\")\n",
    "axes[1].legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ce98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(config, adversary_net):\n",
    "    adversary_net.eval()\n",
    "    with torch.no_grad():\n",
    "        return run_episode(config, adversary_net, should_render=True, is_val=True)\n",
    "\n",
    "episode = visualize(config, adversary_net)\n",
    "print(\"episode steps\", episode.steps)\n",
    "print(\"episode rewards\", *episode.reward.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12305969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90db25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e38bcc7126743dead9439655a5af9f3fb751878ebe55ba955d74bf2be8defae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
