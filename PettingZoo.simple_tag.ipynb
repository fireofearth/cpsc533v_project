{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50130d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "        \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 4\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [-0.84429837  0.7186429 ]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.34800373 -0.42134618]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -3420.714186464787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17103.570932323935"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33232068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode ran for 605 steps\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy(observation, agent):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "#     print(observation.shape)agent_step_idx\n",
    "#     print(agent)\n",
    "    if \"adversary\" in agent:\n",
    "        # adversary\n",
    "        if agent == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "        \n",
    "    if \"agent\" in agent:\n",
    "        # non-adversary\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "for agent_step_idx, agent in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy(observation, agent)\n",
    "        env.step(action)\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c62b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5ad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.all.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.05,\n",
    "    n_episodes=200,\n",
    "    update_target_interval=10,\n",
    "    report_interval=20,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    all=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "        n_rnn_layers=2,\n",
    "        apply_bn=False,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "#         keys = [*self.__message_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__message_d[k]\n",
    "#         keys = [*self.__hidden_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__hidden_d[k]\n",
    "        \n",
    "        for idx in range(config.adversary.n_agents):\n",
    "            self.__message_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.message_size*(config.adversary.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"adversary_{idx}\"]  = torch.zeros(\n",
    "                (config.adversary.n_rnn_layers, 1, self.config.adversary.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        for idx in range(config.agent.n_agents):\n",
    "            self.__message_d[f\"agent_{idx}\"] = torch.zeros(\n",
    "                self.config.agent.message_size*(config.agent.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"agent_{idx}\"]  = torch.zeros(\n",
    "                (config.agent.n_rnn_layers, 1, self.config.agent.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        return self.__message_d[agent_name]\n",
    "\n",
    "    def get_hidden(self, agent_name):\n",
    "        return self.__hidden_d[agent_name]\n",
    "\n",
    "    def update_message(self, agent_name, message):\n",
    "        \"\"\"Update message cache.\n",
    "        \n",
    "        Messages of multiple agents are concatenated together.\n",
    "        For example, if agent 2 receives messages from agents 0, 1, and 3 then\n",
    "        the message is a vector of the form: [ 0's message, 1's message, 3's message ]\n",
    "        \"\"\"\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        for jdx in range(config[agent_type].n_agents):\n",
    "            if jdx < agent_idx:\n",
    "                start_idx = agent_idx - 1\n",
    "            elif jdx == agent_idx:\n",
    "                # do not update message to oneself\n",
    "                continue\n",
    "            else:\n",
    "                # agent_idx < jdx\n",
    "                start_idx = agent_idx\n",
    "            end_idx   = start_idx + self.config[agent_type].message_size\n",
    "            # print(jdx, agent_idx, self.__message_d[f\"{agent_type}_{jdx}\"].shape, start_idx, end_idx)\n",
    "            messages = self.__message_d[f\"{agent_type}_{jdx}\"]\n",
    "            self.__message_d[f\"{agent_type}_{jdx}\"] = \\\n",
    "                    torch.hstack((messages[:start_idx], message, messages[end_idx:]))\n",
    "\n",
    "    def update_hidden(self, agent_name, hidden):\n",
    "        self.__hidden_d[agent_name] = hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.device = config.device\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.send_message_size = config[agent_type].message_size\n",
    "        self.recv_message_size = config[agent_type].message_size*(config[agent_type].n_agents - 1)\n",
    "        self.n_agents = config[agent_type].n_agents\n",
    "        self.n_actions = config[agent_type].n_actions\n",
    "        self.apply_bn = config[agent_type].apply_bn\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.n_rnn_layers = config[agent_type].n_rnn_layers\n",
    "        self.n_output = self.n_actions + self.send_message_size\n",
    "        \n",
    "        self.agent_lookup    = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.action_lookup   = torch.nn.Embedding(self.n_actions, self.hidden_size)\n",
    "        self.observation_mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"linear\", torch.nn.Linear(self.observation_size, self.hidden_size)),\n",
    "            (\"relu\", torch.nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.message_mlp = torch.nn.Sequential()\n",
    "        # if self.apply_bn:\n",
    "        #     # input must have shape (N, C), output has the same shape\n",
    "        #     self.message_mlp.add_module(\"bn\", torch.nn.BatchNorm1d(self.recv_message_size))\n",
    "        self.message_mlp.add_module(\"linear\", torch.nn.Linear(self.recv_message_size, self.hidden_size))\n",
    "        self.message_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        # input must have shape (N, L, H_in)\n",
    "        # output has shape  (N, L, H_out)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.n_rnn_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_mlp = torch.nn.Sequential()\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.output_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.n_output))\n",
    "    \n",
    "    def forward(self, agent_idx, observation, message, hidden):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        agent_idx : int\n",
    "            Index of agent\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        message : torch.Tensor\n",
    "            Messages from the other agents. By default has shape (message_size*(n_agents - 1))\n",
    "            where message_size=4 and n_agents=3\n",
    "        hidden : torch.Tensor\n",
    "            Hidden state of GRU. By default has shape (n_layers=2, N=1, H_out=128).\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        torch.Tensor\n",
    "            The message to pass to other agents.\n",
    "        torch.Tensor\n",
    "            The hidden state used by GRU.\n",
    "        \"\"\"\n",
    "        agent_idx   = torch.tensor(agent_idx, dtype=torch.int, device=self.device)\n",
    "        observation = torch.tensor(observation, dtype=torch.float, device=self.device)\n",
    "        z_a = self.agent_lookup(agent_idx)\n",
    "        z_o = self.observation_mlp(observation)\n",
    "        z_m = self.message_mlp(message)\n",
    "        z = z_a + z_o + z_m\n",
    "        # z has shape (N=1, L=1, H_in=128)\n",
    "        z = z.unsqueeze(0).unsqueeze(0)\n",
    "        # hidden has shape (n_layers=2, N=1, H_out=128) before and after\n",
    "        # out has shape (N=1, L=1, H_out=128)\n",
    "        out, hidden = self.rnn(z, hidden)\n",
    "        out = out.squeeze(0).squeeze(0)\n",
    "        out = self.output_mlp(out)\n",
    "        Q = out[0:self.n_actions]\n",
    "        m = out[self.n_actions:self.n_actions + self.send_message_size]\n",
    "        return Q, m, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f86814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def choose_action(config, agent_type, Q):\n",
    "    if random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "def run_episode(config, container, adversary_net, agent_net, should_render=False):\n",
    "    \"\"\"Run one episodes.\n",
    "    \n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    AttrDict\n",
    "        Contains episode metrics:\n",
    "        - steps : number of steps. All agents take an action at each step.\n",
    "        - reward : episodic rewards indexed by ('adversary', 'agent').\n",
    "        - step_records : list of quantities produced indiced by step, ('adversary', 'agent'), agent index.\n",
    "          Each step record has:\n",
    "            + observation\n",
    "            + message\n",
    "            + hidden\n",
    "            + Q\n",
    "            + reward\n",
    "            + done\n",
    "        - loss : contains episodic losses indexed by ('adversary', 'agent'). To be updated by train_agents()\n",
    "    \"\"\"\n",
    "    episode = AttrDict(\n",
    "        steps=0,\n",
    "        reward=AttrDict(adversary=0, agent=0),\n",
    "        step_records=[],\n",
    "        loss=AttrDict(adversary=0, agent=0)\n",
    "    )\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    \n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "            \n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                message=None,\n",
    "                hidden=None,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "            \n",
    "        m_prev = container.get_message(agent_name)\n",
    "        h_prev = container.get_hidden(agent_name)\n",
    "        if agent_type == \"adversary\":\n",
    "            Q_curr, m_curr, h_curr = adversary_net(agent_idx, obs_curr, m_prev, h_prev)\n",
    "        else:\n",
    "            # agent type is synonymous with good agent\n",
    "            Q_curr, m_curr, h_curr = agent_net(agent_idx, obs_curr, m_prev, h_prev)\n",
    "\n",
    "        action = choose_action(config, agent_type, Q_curr)\n",
    "        env.step(action)\n",
    "        container.update_message(agent_name, m_curr)\n",
    "        container.update_hidden(agent_name, h_curr)\n",
    "        step_record[agent_type][agent_idx] = AttrDict(\n",
    "            # inputs to network\n",
    "            observation=obs_curr,\n",
    "            message=m_prev,\n",
    "            hidden=h_prev,\n",
    "            # outputs of network / inputs to environment\n",
    "            action=action,\n",
    "            Q=Q_curr,\n",
    "            # output of environment\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "        )\n",
    "        episode.reward[agent_type] += reward\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def train_agents(config, episode, adversary_net, agent_net,\n",
    "                 adversary_target_net, agent_target_net,\n",
    "                 adversary_optimizer, agent_optimizer):\n",
    "    \"\"\"Compute loss of episode and update agent weights.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float, device=device)\n",
    "    adversary_loss = torch.tensor(0., device=device)\n",
    "    agent_loss = torch.tensor(0., device=device)\n",
    "    for step_idx in range(episode.steps):\n",
    "        for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "            curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                with torch.no_grad():\n",
    "                    target_Q, _, _ = adversary_target_net(agent_idx, next_o, next_m, next_h)\n",
    "                    max_target_Q = torch.max(target_Q)\n",
    "                    y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            adversary_loss += torch.pow(y - Q_u, 2.)\n",
    "            \n",
    "        for agent_idx in episode.step_records[step_idx].agent.keys():\n",
    "            curr_record = episode.step_records[step_idx].agent[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].agent[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                with torch.no_grad():\n",
    "                    target_Q, _, _ = agent_target_net(agent_idx, next_o, next_m, next_h)\n",
    "                    max_target_Q = torch.max(target_Q)\n",
    "                    y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            agent_loss += torch.pow(y - Q_u, 2.)\n",
    "    \n",
    "    adversary_optimizer.zero_grad()\n",
    "    agent_optimizer.zero_grad()\n",
    "    adversary_loss.backward()\n",
    "    agent_loss.backward()\n",
    "    adversary_optimizer.step()\n",
    "    agent_optimizer.step()\n",
    "    episode.loss = AttrDict(adversary=adversary_loss.item(), agent=agent_loss.item())\n",
    "    \n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    print(\"Training the agents...\")\n",
    "    t0 = time.time()\n",
    "    device = config.device\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_net = SimpleTagNet(config, \"agent\")\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_target_net = SimpleTagNet(config, \"agent\")\n",
    "    adversary_net        = adversary_net.to(device)\n",
    "    agent_net            = agent_net.to(device)\n",
    "    adversary_target_net = adversary_target_net.to(device)\n",
    "    agent_target_net     = agent_target_net.to(device)\n",
    "    adversary_target_net.eval()\n",
    "    agent_target_net.eval()\n",
    "    print(\"Created the agent nets.\")\n",
    "    adversary_optimizer = torch.optim.RMSprop(adversary_net.parameters())\n",
    "    agent_optimizer = torch.optim.RMSprop(agent_net.parameters())\n",
    "    container = Container(config)\n",
    "    logger = AttrDict(\n",
    "        episodic_losses=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_rewards=AttrDict(adversary=[], agent=[])\n",
    "    )\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "        agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "    \n",
    "    print(\"Beginning the episodes...\")\n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        episode = run_episode(config, container, adversary_net, agent_net,\n",
    "                              should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n",
    "        train_agents(config, episode, adversary_net, agent_net, adversary_target_net, agent_target_net,\n",
    "                     adversary_optimizer, agent_optimizer)\n",
    "        logger.episodic_losses.adversary.append(episode.loss.adversary)\n",
    "        logger.episodic_losses.agent.append(episode.loss.agent)\n",
    "        logger.episodic_rewards.adversary.append(episode.reward.adversary)\n",
    "        logger.episodic_rewards.agent.append(episode.reward.agent)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            update_targets()\n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            t1 = time.time()\n",
    "            tdelta = TimeDelta(round(t1 - t0, 0))\n",
    "            print(f\"on episode {episode_idx} (time taken so far: {tdelta})\")\n",
    "            print(f\"     loss: adversary {episode.loss.adversary}, agent {episode.loss.agent}\")\n",
    "            print(f\"     reward: adversary {episode.reward.adversary}, agent {episode.reward.agent}\")\n",
    "        container.reset()\n",
    "    \n",
    "    return adversary_net, agent_net, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agents...\n",
      "Created the agent nets.\n",
      "Beginning the episodes...\n",
      "on episode 20 (time taken so far: 0-00:01:5.0)\n",
      "     loss: adversary 3000.000244140625, agent 1763.371826171875\n",
      "     reward: adversary 300.0, agent -480.8840834994114\n",
      "on episode 40 (time taken so far: 0-00:02:9.0)\n",
      "     loss: adversary 0.0018904113676398993, agent 303.2289123535156\n",
      "     reward: adversary 0.0, agent -324.47898797691613\n",
      "on episode 60 (time taken so far: 0-00:03:13.0)\n",
      "     loss: adversary 5400.00146484375, agent 1800.0\n",
      "     reward: adversary 540.0, agent -180.0\n",
      "on episode 80 (time taken so far: 0-00:04:16.0)\n",
      "     loss: adversary 1800.0, agent 1155.83935546875\n",
      "     reward: adversary 180.0, agent -519.5679302341233\n",
      "on episode 100 (time taken so far: 0-00:05:20.0)\n",
      "     loss: adversary 300.00445556640625, agent 100.0\n",
      "     reward: adversary 30.0, agent -10.0\n",
      "on episode 120 (time taken so far: 0-00:06:25.0)\n",
      "     loss: adversary 600.0015258789062, agent 349.0075988769531\n",
      "     reward: adversary 60.0, agent -168.94278996888582\n",
      "on episode 140 (time taken so far: 0-00:07:30.0)\n",
      "     loss: adversary 0.000518498825840652, agent 1257.9364013671875\n",
      "     reward: adversary 0.0, agent -627.8383287414833\n"
     ]
    }
   ],
   "source": [
    "adversary_net, agent_net, logger = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8cf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0af6f10",
   "metadata": {},
   "source": [
    "## Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c06c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(3), tensor(2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,3,2,0])\n",
    "torch.argmax(a).item(), torch.max(a), a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33d9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 "
     ]
    }
   ],
   "source": [
    "d = {1: 'a', 2: 'b', 3: 'c'}\n",
    "for i in d:\n",
    "    print(i , end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779f4097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = torch.tensor(2, device=device)\n",
    "b = torch.tensor(3)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53211f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 9, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(6)\n",
    "a = torch.tensor([9, 8])\n",
    "\n",
    "idx = 4\n",
    "\n",
    "torch.hstack((v[:idx], a, v[idx + 2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701179fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0,1,2])\n",
    "w.device\n",
    "w.to(device)\n",
    "w.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3dcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
