{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59cd7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29838cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633dd43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 2\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [ 0.70805589 -0.5818902 ]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.27165083 -0.45720992]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d40f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 2\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [-0.45257476  0.98974294]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.46874369 -0.89777674]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed8ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -3285.9782898487856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-16429.89144924393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=False, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148c75ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode ran for 1805 steps\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy(observation, agent):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "#     print(observation.shape)agent_step_idx\n",
    "#     print(agent)\n",
    "    if \"adversary\" in agent:\n",
    "        # adversary\n",
    "        if agent == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "        \n",
    "    if \"agent\" in agent:\n",
    "        # non-adversary\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "for agent_step_idx, agent in enumerate(env.agent_iter()):\n",
    "#     env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy(observation, agent)\n",
    "        env.step(action)\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab8e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.all.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.05,\n",
    "    n_episodes=200,\n",
    "    update_target_interval=10,\n",
    "    report_interval=20,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    all=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "        n_rnn_layers=2,\n",
    "        apply_bn=False,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "#         keys = [*self.__message_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__message_d[k]\n",
    "#         keys = [*self.__hidden_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__hidden_d[k]\n",
    "        \n",
    "        for idx in range(config.adversary.n_agents):\n",
    "            self.__message_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.message_size*(config.adversary.n_agents - 1),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "            self.__hidden_d[f\"adversary_{idx}\"]  = torch.zeros(\n",
    "                (config.adversary.n_rnn_layers, 1, self.config.adversary.hidden_size,),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        for idx in range(config.agent.n_agents):\n",
    "            self.__message_d[f\"agent_{idx}\"] = torch.zeros(\n",
    "                self.config.agent.message_size*(config.agent.n_agents - 1),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "            self.__hidden_d[f\"agent_{idx}\"]  = torch.zeros(\n",
    "                (config.agent.n_rnn_layers, 1, self.config.agent.hidden_size,),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        return self.__message_d[agent_name]\n",
    "\n",
    "    def get_hidden(self, agent_name):\n",
    "        return self.__hidden_d[agent_name]\n",
    "\n",
    "    def update_message(self, agent_name, message):\n",
    "        \"\"\"Update message cache.\n",
    "        \n",
    "        Messages of multiple agents are concatenated together.\n",
    "        For example, if agent 2 receives messages from agents 0, 1, and 3 then\n",
    "        the message is a vector of the form: [ 0's message, 1's message, 3's message ]\n",
    "        \"\"\"\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        for jdx in range(config[agent_type].n_agents):\n",
    "            if jdx < agent_idx:\n",
    "                start_idx = agent_idx - 1\n",
    "            elif jdx == agent_idx: # down update message to oneself\n",
    "                continue\n",
    "            else: # agent_idx < jdx\n",
    "                start_idx = agent_idx\n",
    "            end_idx   = start_idx + self.config[agent_type].message_size\n",
    "            # print(jdx, agent_idx, self.__message_d[f\"{agent_type}_{jdx}\"].shape, start_idx, end_idx)\n",
    "            messages = self.__message_d[f\"{agent_type}_{jdx}\"]\n",
    "            self.__message_d[f\"{agent_type}_{jdx}\"] = \\\n",
    "                    torch.hstack((messages[:start_idx], message, messages[end_idx:]))\n",
    "\n",
    "    def update_hidden(self, agent_name, hidden):\n",
    "        self.__hidden_d[agent_name] = hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c5ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.send_message_size = config[agent_type].message_size\n",
    "        self.recv_message_size = config[agent_type].message_size*(config[agent_type].n_agents - 1)\n",
    "        self.n_agents = config[agent_type].n_agents\n",
    "        self.n_actions = config[agent_type].n_actions\n",
    "        self.apply_bn = config[agent_type].apply_bn\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.n_rnn_layers = config[agent_type].n_rnn_layers\n",
    "        self.n_output = self.n_actions + self.send_message_size\n",
    "        \n",
    "        self.agent_lookup    = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.action_lookup   = torch.nn.Embedding(self.n_actions, self.hidden_size)\n",
    "        self.observation_mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"linear\", torch.nn.Linear(self.observation_size, self.hidden_size)),\n",
    "            (\"relu\", torch.nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.message_mlp = torch.nn.Sequential()\n",
    "        # if self.apply_bn:\n",
    "        #     # input must have shape (N, C), output has the same shape\n",
    "        #     self.message_mlp.add_module(\"bn\", torch.nn.BatchNorm1d(self.recv_message_size))\n",
    "        self.message_mlp.add_module(\"linear\", torch.nn.Linear(self.recv_message_size, self.hidden_size))\n",
    "        self.message_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        # input must have shape (N, L, H_in)\n",
    "        # output has shape  (N, L, H_out)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.n_rnn_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_mlp = torch.nn.Sequential()\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.output_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.n_output))\n",
    "    \n",
    "    def forward(self, agent_idx, observation, message, hidden):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        agent_idx : int\n",
    "            Index of agent\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        message : torch.Tensor\n",
    "            Messages from the other agents. By default has shape (message_size*(n_agents - 1))\n",
    "            where message_size=4 and n_agents=3\n",
    "        hidden : torch.Tensor\n",
    "            Hidden state of GRU. By default has shape (n_layers=2, N=1, H_out=128).\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        torch.Tensor\n",
    "            The message to pass to other agents.\n",
    "        torch.Tensor\n",
    "            The hidden state used by GRU.\n",
    "        \"\"\"\n",
    "        agent_idx   = torch.tensor(agent_idx, dtype=torch.int)\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        z_a = self.agent_lookup(agent_idx)\n",
    "        z_o = self.observation_mlp(observation)\n",
    "        z_m = self.message_mlp(message)\n",
    "        z = z_a + z_o + z_m\n",
    "        # z has shape (N=1, L=1, H_in=128)\n",
    "        z = z.unsqueeze(0).unsqueeze(0)\n",
    "        # hidden has shape (n_layers=2, N=1, H_out=128) before and after\n",
    "        # out has shape (N=1, L=1, H_out=128)\n",
    "        out, hidden = self.rnn(z, hidden)\n",
    "        out = out.squeeze(0).squeeze(0)\n",
    "        out = self.output_mlp(out)\n",
    "        Q = out[0:self.n_actions]\n",
    "        m = out[self.n_actions:self.n_actions + self.send_message_size]\n",
    "        return Q, m, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d0098a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello?\n",
      "Is cuda? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/leamin.27667584.0/ipykernel_148019/2962163876.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  agent_idx   = torch.tensor(agent_idx, dtype=torch.int)\n",
      "/localscratch/leamin.27667584.0/ipykernel_148019/2962163876.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  observation = torch.tensor(observation, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 20\n",
      "     loss: adversary 300.0, agent 5264.982421875\n",
      "     reward: adversary 30.0, agent -1430.9673374470208\n",
      "on episode 40\n",
      "     loss: adversary 300.0, agent 393.92047119140625\n",
      "     reward: adversary 30.0, agent -266.0359323966399\n",
      "on episode 60\n",
      "     loss: adversary 1500.0, agent 728.2037353515625\n",
      "     reward: adversary 150.0, agent -264.1979001515051\n",
      "on episode 80\n",
      "     loss: adversary 0.0, agent 950.7318725585938\n",
      "     reward: adversary 0.0, agent -512.0540965474062\n",
      "on episode 100\n",
      "     loss: adversary 300.0, agent 906.8951416015625\n",
      "     reward: adversary 30.0, agent -570.8632940518224\n",
      "on episode 120\n",
      "     loss: adversary 0.0, agent 0.0\n",
      "     reward: adversary 0.0, agent 0.0\n",
      "on episode 140\n",
      "     loss: adversary 600.0, agent 1315.36279296875\n",
      "     reward: adversary 60.0, agent -532.1831796500637\n",
      "on episode 160\n",
      "     loss: adversary 0.0, agent 448.57940673828125\n",
      "     reward: adversary 0.0, agent -317.907411321856\n",
      "on episode 180\n",
      "     loss: adversary 0.0, agent 113.54822540283203\n",
      "     reward: adversary 0.0, agent -109.92454111249508\n"
     ]
    }
   ],
   "source": [
    "def choose_action(config, agent_type, Q):\n",
    "    if random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "def run_episode(config, container, adversary_net, agent_net, should_render=False, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \"\"\"\n",
    "    episode = AttrDict(steps=0, reward=AttrDict(adversary=0, agent=0), step_records=[])\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    cpu_device = torch.device('cpu')\n",
    "    \n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "            \n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                message=None,\n",
    "                hidden=None,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "            \n",
    "        m_prev = container.get_message(agent_name)\n",
    "        h_prev = container.get_hidden(agent_name)\n",
    "        if agent_type == \"adversary\":\n",
    "            Q_curr, m_curr, h_curr = adversary_net(\n",
    "                torch.as_tensor(agent_idx, dtype=torch.int).to(device), \n",
    "                torch.as_tensor(obs_curr, dtype=torch.float32).to(device), \n",
    "                torch.as_tensor(m_prev, dtype=torch.float32).to(device), \n",
    "                torch.as_tensor(h_prev, dtype=torch.float32).to(device)\n",
    "            )\n",
    "        else: # good agent\n",
    "            Q_curr, m_curr, h_curr = agent_net(\n",
    "                torch.as_tensor(agent_idx, dtype=torch.int).to(device), \n",
    "                torch.as_tensor(obs_curr, dtype=torch.float32).to(device), \n",
    "                torch.as_tensor(m_prev, dtype=torch.float32).to(device), \n",
    "                torch.as_tensor(h_prev, dtype=torch.float32).to(device)\n",
    "            )\n",
    "\n",
    "        action = choose_action(config, agent_type, Q_curr)\n",
    "        env.step(action)\n",
    "        container.update_message(agent_name, m_curr.to(cpu_device))\n",
    "        container.update_hidden(agent_name, h_curr.to(cpu_device))\n",
    "        step_record[agent_type][agent_idx] = AttrDict(\n",
    "            # inputs to network\n",
    "            observation=obs_curr,\n",
    "            message=m_prev,\n",
    "            hidden=h_prev,\n",
    "            # outputs of network / inputs to environment\n",
    "            action=action,\n",
    "            Q=Q_curr,\n",
    "            # output of environment\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "        )\n",
    "        episode.reward[agent_type] += reward\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def train_agents(config, episode, adversary_net, agent_net,\n",
    "                 adversary_target_net, agent_target_net,\n",
    "                 adversary_optimizer, agent_optimizer, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float)\n",
    "    \n",
    "    adversary_loss = torch.tensor(0.).to(device)\n",
    "    agent_loss = torch.tensor(0.).to(device)\n",
    "    for step_idx in range(episode.steps):\n",
    "        \n",
    "        for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "            curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                target_Q, _, _ = adversary_target_net(\n",
    "                    torch.as_tensor(agent_idx, dtype=torch.int).to(device), \n",
    "                    torch.as_tensor(next_o).to(device), \n",
    "                    torch.as_tensor(next_m).to(device), \n",
    "                    torch.as_tensor(next_h).to(device)\n",
    "                )\n",
    "                max_target_Q = torch.max(target_Q.detach())\n",
    "                y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            adversary_loss += torch.pow(y - Q_u, 2.)\n",
    "            \n",
    "        for agent_idx in episode.step_records[step_idx].agent.keys():\n",
    "            curr_record = episode.step_records[step_idx].agent[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].agent[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                target_Q, _, _ = agent_target_net(\n",
    "                    torch.as_tensor(agent_idx, dtype=torch.int).to(device), \n",
    "                    torch.as_tensor(next_o).to(device), \n",
    "                    torch.as_tensor(next_m).to(device), \n",
    "                    torch.as_tensor(next_h).to(device)\n",
    "                )\n",
    "                max_target_Q = torch.max(target_Q.detach())\n",
    "                y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            agent_loss += torch.pow(y - Q_u, 2.)\n",
    "    \n",
    "    adversary_optimizer.zero_grad()\n",
    "    agent_optimizer.zero_grad()\n",
    "    adversary_loss.backward()\n",
    "    agent_loss.backward()\n",
    "    adversary_optimizer.step()\n",
    "    agent_optimizer.step()\n",
    "    episode.loss = AttrDict(adversary=adversary_loss.item(), agent=agent_loss.item())\n",
    "    \n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     device = torch.device('cpu')\n",
    "\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_net = SimpleTagNet(config, \"agent\")\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_target_net = SimpleTagNet(config, \"agent\")\n",
    "    \n",
    "    print('hello?')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            adversary_net = torch.nn.DataParallel(adversary_net)\n",
    "            agent_net = torch.nn.DataParallel(agent_net)\n",
    "            adversary_target_net = torch.nn.DataParallel(adversary_target_net)\n",
    "            agent_target_net = torch.nn.DataParallel(agent_target_net)\n",
    "        adversary_net.to(device)\n",
    "        agent_net.to(device)\n",
    "        adversary_target_net.to(device)\n",
    "        agent_target_net.to(device)\n",
    "\n",
    "    print('Is cuda?', next(agent_target_net.parameters()).is_cuda)\n",
    "    \n",
    "    adversary_target_net.eval()\n",
    "    agent_target_net.eval()\n",
    "    adversary_optimizer = torch.optim.RMSprop(adversary_net.parameters())\n",
    "    agent_optimizer = torch.optim.RMSprop(agent_net.parameters())\n",
    "    container = Container(config)\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "        agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "    \n",
    "    for episode_idx in range(config.n_episodes):\n",
    "#         episode = run_episode(config, container, adversary_net, agent_net,\n",
    "#                               should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n",
    "        episode = run_episode(config, container, adversary_net, agent_net, should_render=False, device=device)\n",
    "        train_agents(config, episode, adversary_net, agent_net, adversary_target_net, agent_target_net,\n",
    "                     adversary_optimizer, agent_optimizer, device=device)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            update_targets()\n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            print(f\"on episode {episode_idx}\")\n",
    "            print(f\"     loss: adversary {episode.loss.adversary}, agent {episode.loss.agent}\")\n",
    "            print(f\"     reward: adversary {episode.reward.adversary}, agent {episode.reward.agent}\")\n",
    "        container.reset()\n",
    "    \n",
    "    return adversary_net, agent_net\n",
    "\n",
    "adversary_net, agent_net = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b6af2fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/localscratch/leamin.27667584.0/ipykernel_148019/2323937617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"on episode {episode_idx}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"     loss: adversary {episode.loss.adversary}, agent {episode.loss.agent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"     reward: adversary {episode.reward.adversary}, agent {episode.reward.agent}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episode_idx' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"on episode {episode_idx}\")\n",
    "print(f\"     loss: adversary {episode.loss.adversary}, agent {episode.loss.agent}\")\n",
    "print(f\"     reward: adversary {episode.reward.adversary}, agent {episode.reward.agent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0984301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
