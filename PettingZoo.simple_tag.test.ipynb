{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa011482",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Testing some hardcoded algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cef3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "        \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 4\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [-0.84429837  0.7186429 ]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.34800373 -0.42134618]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -3542.3399623641044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17711.699811820523"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33232068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode ran for 1805 steps\n",
      "agent_rewards -543.1800593395708\n",
      "adversary_rewards 60.0\n"
     ]
    }
   ],
   "source": [
    "eps = 0.3\n",
    "\n",
    "def hardcode_policy_1(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent_name : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def hardcode_policy_2(observation, agent_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    observation : ndarray\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "    if \"adversary\" in agent_name:\n",
    "        # adversary\n",
    "        if agent_name == \"adversary_0\":\n",
    "            # get agent_0's\n",
    "            x, y = observation[12:14]\n",
    "            if x < -eps: # go left\n",
    "                return 1\n",
    "            elif x > eps: # go right\n",
    "                return 2\n",
    "            elif y < -eps: # go down\n",
    "                return 3\n",
    "            elif y > eps: # go up\n",
    "                return 4\n",
    "            else:\n",
    "                return random.randint(0, 4)\n",
    "            # return np.random.binomial(2, 0.3) + 3\n",
    "    elif \"agent\" in agent_name:\n",
    "        # non-adversary\n",
    "        if agent_name == \"agent_0\":\n",
    "            return 0\n",
    "            # return random.randint(0, 4)\n",
    "            # return np.random.binomial(2, 0.3) + 3\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "agent_rewards = 0\n",
    "adversary_rewards = 0\n",
    "for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy_1(observation, agent_name)\n",
    "        env.step(action)\n",
    "    if \"adversary\" in agent_name:\n",
    "        adversary_rewards += reward\n",
    "    if \"agent\" in agent_name:\n",
    "        agent_rewards += reward\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")\n",
    "print(\"agent_rewards\", agent_rewards)\n",
    "print(\"adversary_rewards\", adversary_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af6f10",
   "metadata": {},
   "source": [
    "## Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c06c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(3), tensor(2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,3,2,0])\n",
    "torch.argmax(a).item(), torch.max(a), a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33d9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 "
     ]
    }
   ],
   "source": [
    "d = {1: 'a', 2: 'b', 3: 'c'}\n",
    "for i in d:\n",
    "    print(i , end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779f4097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = torch.tensor(2, device=device)\n",
    "b = torch.tensor(3)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53211f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 9, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(6)\n",
    "a = torch.tensor([9, 8])\n",
    "\n",
    "idx = 4\n",
    "\n",
    "torch.hstack((v[:idx], a, v[idx + 2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701179fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([0,1,2])\n",
    "w.device\n",
    "w.to(device)\n",
    "w.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859c349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
