{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a622b81",
   "metadata": {},
   "source": [
    "Simple Tag\n",
    "https://www.pettingzoo.ml/mpe/simple_tag\n",
    "\n",
    "> This is a predator-prey environment. Good agents (green) are faster and receive a negative reward for being hit by adversaries (red) (-10 for each collision). Adversaries are slower and are rewarded for hitting good agents (+10 for each collision). Obstacles (large black circles) block the way. By default, there is 1 good agent, 3 adversaries and 2 obstacles.\n",
    "\n",
    "Baseline agent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f7b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import statistics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class TimeDelta(object):\n",
    "    def __init__(self, delta_time):\n",
    "        \"\"\"Convert time difference in seconds to days, hours, minutes, seconds.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        delta_time : float\n",
    "            Time difference in seconds.\n",
    "        \"\"\"\n",
    "        self.fractional, seconds = math.modf(delta_time)\n",
    "        seconds = int(seconds)\n",
    "        minutes, self.seconds = divmod(seconds, 60)\n",
    "        hours, self.minutes = divmod(minutes, 60)\n",
    "        self.days, self.hours = divmod(hours, 24)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.days}-{self.hours:02}:{self.minutes:02}:{self.seconds + self.fractional:02}\"\n",
    "        \n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c62b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5ad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.all.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.05,\n",
    "    n_episodes=200,\n",
    "    update_target_interval=10,\n",
    "    report_interval=20,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    all=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "        n_rnn_layers=2,\n",
    "        apply_bn=False,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        for idx in range(config.adversary.n_agents):\n",
    "            self.__message_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.message_size*(config.adversary.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"adversary_{idx}\"]  = torch.zeros(\n",
    "                (config.adversary.n_rnn_layers, 1, self.config.adversary.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        for idx in range(config.agent.n_agents):\n",
    "            self.__message_d[f\"agent_{idx}\"] = torch.zeros(\n",
    "                self.config.agent.message_size*(config.agent.n_agents - 1),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "            self.__hidden_d[f\"agent_{idx}\"]  = torch.zeros(\n",
    "                (config.agent.n_rnn_layers, 1, self.config.agent.hidden_size,),\n",
    "                dtype=torch.float, device=self.device\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = self.config.device\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        return self.__message_d[agent_name]\n",
    "\n",
    "    def get_hidden(self, agent_name):\n",
    "        return self.__hidden_d[agent_name]\n",
    "\n",
    "    def update_message(self, agent_name, message):\n",
    "        \"\"\"Update message cache.\n",
    "        \n",
    "        Messages of multiple agents are concatenated together.\n",
    "        For example, if agent 2 receives messages from agents 0, 1, and 3 then\n",
    "        the message is a vector of the form: [ 0's message, 1's message, 3's message ]\n",
    "        \"\"\"\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        message_size = self.config[agent_type].message_size\n",
    "        for jdx in range(config[agent_type].n_agents):\n",
    "            if jdx < agent_idx:\n",
    "                start_idx = message_size * (agent_idx - 1)\n",
    "            elif jdx == agent_idx:\n",
    "                # do not update message to oneself\n",
    "                continue\n",
    "            else:\n",
    "                # agent_idx < jdx\n",
    "                start_idx = message_size * agent_idx\n",
    "            end_idx   = start_idx + self.config[agent_type].message_size\n",
    "            # print(jdx, agent_idx, self.__message_d[f\"{agent_type}_{jdx}\"].shape, start_idx, end_idx)\n",
    "            messages = self.__message_d[f\"{agent_type}_{jdx}\"]\n",
    "            self.__message_d[f\"{agent_type}_{jdx}\"] = \\\n",
    "                    torch.hstack((messages[:start_idx], message, messages[end_idx:]))\n",
    "\n",
    "    def update_hidden(self, agent_name, hidden):\n",
    "        self.__hidden_d[agent_name] = hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.device      = config.device\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.n_agents    = config[agent_type].n_agents\n",
    "        self.n_actions   = config[agent_type].n_actions\n",
    "        self.apply_bn    = config[agent_type].apply_bn\n",
    "        self.n_output    = self.n_actions\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        \n",
    "        self.agent_lookup    = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.observation_mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"linear\", torch.nn.Linear(self.observation_size, self.hidden_size)),\n",
    "            (\"relu\", torch.nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.output_mlp = torch.nn.Sequential()\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.output_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.n_output))\n",
    "    \n",
    "    def forward(self, agent_idx, observation):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        agent_idx : int\n",
    "            Index of agent\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        \"\"\"\n",
    "        agent_idx   = torch.tensor(agent_idx, dtype=torch.int, device=self.device)\n",
    "        observation = torch.tensor(observation, dtype=torch.float, device=self.device)\n",
    "        z_a = self.agent_lookup(agent_idx)\n",
    "        z_o = self.observation_mlp(observation)\n",
    "        # z has shape (N=1, L=1, H_in=128)\n",
    "        z = z_a + z_o\n",
    "        Q = self.output_mlp(z)\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f86814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def choose_action(config, agent_type, Q, is_val=False):\n",
    "    if not is_val and random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "def run_episode(config, adversary_net, agent_net, should_render=False, is_val=False):\n",
    "    \"\"\"Run one episodes.\n",
    "    \n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    AttrDict\n",
    "        Contains episode metrics:\n",
    "        - steps : number of steps. All agents take an action at each step.\n",
    "        - reward : episodic rewards indexed by ('adversary', 'agent').\n",
    "        - step_records : list of quantities produced indiced by step, ('adversary', 'agent'), agent index.\n",
    "          Each step record has:\n",
    "            + observation\n",
    "            + Q\n",
    "            + reward\n",
    "            + done\n",
    "        - loss : contains episodic losses indexed by ('adversary', 'agent'). To be updated by train_agents()\n",
    "    \"\"\"\n",
    "    episode = AttrDict(\n",
    "        steps=0,\n",
    "        reward=AttrDict(adversary=0, agent=0),\n",
    "        step_records=[],\n",
    "        loss=AttrDict(adversary=0, agent=0)\n",
    "    )\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    \n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "            \n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "\n",
    "        if agent_type == \"adversary\":\n",
    "            Q_curr = adversary_net(agent_idx, obs_curr)\n",
    "        else:\n",
    "            # agent type is synonymous with good agent\n",
    "            Q_curr = agent_net(agent_idx, obs_curr)\n",
    "\n",
    "        action = choose_action(config, agent_type, Q_curr, is_val=is_val)\n",
    "        env.step(action)\n",
    "        step_record[agent_type][agent_idx] = AttrDict(\n",
    "            # inputs to network\n",
    "            observation=obs_curr,\n",
    "            # outputs of network / inputs to environment\n",
    "            action=action,\n",
    "            Q=Q_curr,\n",
    "            # output of environment\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "        )\n",
    "        episode.reward[agent_type] += reward\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def train_agents(config, episode, adversary_net, agent_net,\n",
    "                 adversary_target_net, agent_target_net,\n",
    "                 adversary_optimizer, agent_optimizer):\n",
    "    \"\"\"Compute loss of episode and update agent weights.\n",
    "    \"\"\"\n",
    "    device = config.device\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float, device=device)\n",
    "    adversary_loss = torch.tensor(0., device=device)\n",
    "    agent_loss = torch.tensor(0., device=device)\n",
    "    for step_idx in range(episode.steps):\n",
    "        for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "            curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                with torch.no_grad():\n",
    "                    target_Q = adversary_target_net(agent_idx, next_o)\n",
    "                    max_target_Q = torch.max(target_Q)\n",
    "                    y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            adversary_loss += torch.pow(y - Q_u, 2.)\n",
    "            \n",
    "        for agent_idx in episode.step_records[step_idx].agent.keys():\n",
    "            curr_record = episode.step_records[step_idx].agent[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].agent[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float, device=device)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                with torch.no_grad():\n",
    "                    target_Q = agent_target_net(agent_idx, next_o)\n",
    "                    max_target_Q = torch.max(target_Q)\n",
    "                    y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            agent_loss += torch.pow(y - Q_u, 2.)\n",
    "    \n",
    "    adversary_optimizer.zero_grad()\n",
    "    agent_optimizer.zero_grad()\n",
    "    adversary_loss.backward()\n",
    "    agent_loss.backward()\n",
    "    adversary_optimizer.step()\n",
    "    agent_optimizer.step()\n",
    "    episode.loss = AttrDict(adversary=adversary_loss.item(), agent=agent_loss.item())\n",
    "    \n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    print(\"Training the agents...\")\n",
    "    t0 = time.time()\n",
    "    device = config.device\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    agent_net = SimpleTagNet(config, \"agent\").to(device)\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\").to(device)\n",
    "    agent_target_net = SimpleTagNet(config, \"agent\").to(device)\n",
    "    adversary_target_net.eval()\n",
    "    agent_target_net.eval()\n",
    "    print(\"Created the agent nets.\")\n",
    "    adversary_optimizer = torch.optim.RMSprop(adversary_net.parameters())\n",
    "    agent_optimizer = torch.optim.RMSprop(agent_net.parameters())\n",
    "    logger = AttrDict(\n",
    "        episodic_losses=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_rewards=AttrDict(adversary=[], agent=[]),\n",
    "        episodic_message_maxsum=AttrDict(adversary=[], agent=[])\n",
    "    )\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "        agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "    print(\"Initial update of target nets\")\n",
    "    update_targets()\n",
    "    \n",
    "    print(\"Beginning the episodes...\")\n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        # Run an episode\n",
    "        episode = run_episode(config, adversary_net, agent_net,\n",
    "                              should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n",
    "        \n",
    "        # Train on the episode\n",
    "        train_agents(config, episode, adversary_net, agent_net, adversary_target_net, agent_target_net,\n",
    "                     adversary_optimizer, agent_optimizer)\n",
    "        \n",
    "        # Logging the reward and los\n",
    "        logger.episodic_losses.adversary.append(episode.loss.adversary)\n",
    "        logger.episodic_losses.agent.append(episode.loss.agent)\n",
    "        logger.episodic_rewards.adversary.append(episode.reward.adversary)\n",
    "        logger.episodic_rewards.agent.append(episode.reward.agent)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            # Update double network\n",
    "            update_targets()\n",
    "        \n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            # Logging\n",
    "            t1 = time.time()\n",
    "            tdelta = TimeDelta(round(t1 - t0, 0))\n",
    "            print(f\"on episode {episode_idx} (time taken so far: {tdelta})\")\n",
    "            mean_loss_adversary = statistics.fmean(logger.episodic_losses.adversary[-config.report_interval:])\n",
    "            mean_loss_agent = statistics.fmean(logger.episodic_losses.agent[-config.report_interval:])\n",
    "            mean_reward_adversary = statistics.fmean(logger.episodic_rewards.adversary[-config.report_interval:])\n",
    "            mean_reward_agent = statistics.fmean(logger.episodic_rewards.agent[-config.report_interval:])\n",
    "            print(f\"     mean loss: adversary {mean_loss_adversary}, agent {mean_loss_agent}\")\n",
    "            print(f\"     mean reward: adversary {mean_reward_adversary}, agent {mean_reward_agent}\")\n",
    "    \n",
    "    return adversary_net, agent_net, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0210fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agents...\n",
      "Created the agent nets.\n",
      "Initial update of target nets\n",
      "Beginning the episodes...\n",
      "on episode 20 (time taken so far: 0-00:00:32.0)\n",
      "     mean loss: adversary 135184.68412590027, agent 5129.7748748779295\n",
      "     mean reward: adversary 67.5, agent -685.724525668559\n",
      "on episode 40 (time taken so far: 0-00:01:2.0)\n",
      "     mean loss: adversary 715.449042224884, agent 722.892974384129\n",
      "     mean reward: adversary 64.5, agent -318.00463705550834\n",
      "on episode 60 (time taken so far: 0-00:01:32.0)\n",
      "     mean loss: adversary 957.8769523620606, agent 1665.9887882232665\n",
      "     mean reward: adversary 82.5, agent -446.3706295279882\n",
      "on episode 80 (time taken so far: 0-00:02:1.0)\n",
      "     mean loss: adversary 969.5068424224853, agent 1279.3180610656739\n",
      "     mean reward: adversary 76.5, agent -509.23451175643913\n",
      "on episode 100 (time taken so far: 0-00:02:31.0)\n",
      "     mean loss: adversary 976.1876487731934, agent 1038.273481464386\n",
      "     mean reward: adversary 57.0, agent -372.07702913954233\n",
      "on episode 120 (time taken so far: 0-00:03:0.0)\n",
      "     mean loss: adversary 731.1128952026368, agent 816.670743560791\n",
      "     mean reward: adversary 39.0, agent -379.9419573985014\n",
      "on episode 140 (time taken so far: 0-00:03:30.0)\n",
      "     mean loss: adversary 1113.2355110168457, agent 1023.9817997455597\n",
      "     mean reward: adversary 46.5, agent -383.1452797707141\n",
      "on episode 160 (time taken so far: 0-00:03:59.0)\n",
      "     mean loss: adversary 9180.103869628907, agent 1546.318896484375\n",
      "     mean reward: adversary 79.5, agent -502.4507262248062\n",
      "on episode 180 (time taken so far: 0-00:04:28.0)\n",
      "     mean loss: adversary 2343.110675048828, agent 810.7154356002808\n",
      "     mean reward: adversary 61.5, agent -324.71438114797775\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "adversary_net, agent_net, logger = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae038102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary_net = SimpleTagNet(config, \"adversary\").to(config.device)\n",
    "agent_net = SimpleTagNet(config, \"agent\").to(config.device)\n",
    "adversary_net.load_state_dict(torch.load('./models/batched-baseline/adversary-net-9984.pth'))\n",
    "agent_net.load_state_dict(torch.load('./models/batched-baseline/agent-net-9984.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "654746eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4104/1807972940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodic_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madversary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adversary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodic_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"good agent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].plot(logger.episodic_losses.adversary[50:], label=\"adversary\")\n",
    "axes[0].plot(logger.episodic_losses.agent[50:], label=\"good agent\")\n",
    "axes[0].set_title(\"loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(logger.episodic_rewards.adversary[50:], label=\"adversary\")\n",
    "axes[1].plot(logger.episodic_rewards.agent[50:], label=\"good agent\")\n",
    "axes[1].set_title(\"reward\")\n",
    "axes[1].legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f65c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode steps 301\n",
      "episode rewards ('adversary', 60.0) ('agent', -29.778725897663204)\n"
     ]
    }
   ],
   "source": [
    "def visualize(config, adversary_net, agent_net):\n",
    "    container = Container(config)\n",
    "    adversary_net.eval()\n",
    "    agent_net.eval()\n",
    "    with torch.no_grad():\n",
    "        return run_episode(config, adversary_net, agent_net, should_render=True, is_val=True)\n",
    "\n",
    "episode = visualize(config, adversary_net, agent_net)\n",
    "print(\"episode steps\", episode.steps)\n",
    "print(\"episode rewards\", *episode.reward.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53717a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
